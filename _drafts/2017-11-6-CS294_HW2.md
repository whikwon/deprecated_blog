---
title:  "CS294_HW2"
date: 2017-11-6 00:00:00
comments: true
---

- Berkeley 대학의 2017년도 가을학기 강의인 CS294: Deep Reinforcement Learning 의 HW2를 정리한 내용이다.

### 코드 내용
4, 5강에서 배운 Policy gradient, Actor-critic 알고리즘을 구현한다. 내용은 대부분 Actor-critic 마지막 쪽 설명하는 내용이며 아마도 가장 최신 로직을 구현한 듯하다. (*앞 내용은 뒤를 설명하기 위해서 발전의 흐름대로 나열한 듯함.*)

수업 소제목 중 코드에 포함된 내용은 아래와 같다.

- Architecture design: $Q_{\phi}^{\pi}(s_t, a_t), \pi_{\theta}(a \lvert s)$를 각각 NN으로 학습시킨다.
- Reward-to-go: $\hat Q_{i, t}$를 구해서 baseline으로 사용한다.
- Discount factors: reward-to-go를 구할 때 $\gamma$ 사용한다.
- Control variates: action-dependent baselines: $\hat A^\pi(s_t, a_t)$를 구하는 식에서 baseline으로 $V_{\phi}^{\pi}(s_t)$ 대신 $Q_{\phi}^{\pi}(s_t, a_t)$를 사용한다.
- Eligibility traces & n-step return: 아래 GAE를 구현하기 위해 사용한다.  
- Generalized advantage estimation: $\hat A_{GAE}^{\pi}(s_t, a_t)$를 구현한다.

### 학습 순서
아래 알고리즘에 따라 학습을 진행하였다.
![algorithm](https://whikwon.github.io/images/CS294_actor_critic.png)
<center> <i> &lt;사용된 알고리즘(Batch Actor-critic)&gt;</i> </center> <br>

1. 샘플 얻기(batch)
  - env를 불러오고 reset시켜서 초기 obs값을 얻는다.
  - obs를 NN 모델(policy)에 통과시켜 action을 구한다. <br> *(discrete/continuous 각각에 따라 sample 구하는 방법이 달라진다.)*
  - action을 env에 통과시켜서 next_obs, reward를 얻는다.
  - 위 과정을 반복해서 원하는 길이의 trajectory를 얻는다.
2. $Q_{\phi}^{\pi}(s_t, a_t)$를 fit시킨다.
  - $V_{\phi}^{\pi}(s_t)$ 대신 $Q_{\phi}^{\pi}(s_t, a_t)$를 fitting 한다.
  - input: 1에서 샘플링으로 얻은 observation
  - target: 1의 샘플로부터 얻은 reward-to-go 값
3. $\hat A_{GAE}^{\pi}(s_t, a_t)$를 구한다.
4. cost의 theta에 대한 gradient를 구한다.
  - gradient를 NN 모델로 정의된 policy와 3에서 구한 advantage 값을 곱한 값으로 정의하고 수식으로 나타낸다.
5. theta를 update 해준다.

### TODO:
1. advantage normalization을 왜 사용하지?
stability 목적으로 사용한다. backpropagation 시에 gradient가 값들에 대해서 영향을 받기 때문에 normalization을 통해
일정한 범위 내에 값이 존재하도록 해주는 것이다. batch_normalization과 유사한 목적으로 사용한다고 보면 되겠다.

출처: https://datascience.stackexchange.com/questions/20098/why-do-we-normalize-the-discounted-rewards-when-doing-policy-gradient-reinforcem
