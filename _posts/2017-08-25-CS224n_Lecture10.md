---
title:  "CS224n_Lecture10: Neural Machine Translation and Models with Attention"
date: 2017-08-25 00:00:00
layout: post
excerpt: "Stanford 대학의 2017년도 겨울학기 강의인 CS224n: Natural Language Processing with Deep Learning의 10강을 정리한 내용이다."
categories: [NLP, Lecture]
comments: true
---
## 강의 내용
9강 내용과 일부 겹치는데 크게 Machine Translation in Deep Learning에 대해서 중점적으로 다루고
attention mechanism을 활용해서 성능을 향상 시킨 사례를 소개하고 있다.

## NMT
Google, Facebook, eBay 등 많은 전세계 대기업들이 Machine Translation에 투자하고
성능을 향상시키고 있다. 전세계적으로 교류가 점점 더 많아짐에 따른 당연한 방향성으로 볼 수 있고
지금까지 NLP수업을 들으면서 가장 MT분야가 뜨거운 듯하다.(*돈이 된다. 비전의 자율주행처럼.*)

**Neural MT(NMT)란?** <br>
Neural Machine Translation is the approach of modeling the **entire** MT process
via one big artificial neural network.

NMT를 위와 같이 정의하고 있는데 기존의 MT의 일부분만 Neural network로 사용하는 것이 아닌 전체 process를
end-to-end로 학습시키는 점이 중요하다. 이를 통해 기존의 한계점들을 극복하고 더 나은 모델을
만들수 있게 된다.

NMT의 기본 구조는 Input text - Encoder - Decoder - Translated text 로 이루어진다.

시간 순으로 NMT에 사용된 모델을 살펴보겠다.
먼저, Google에서 14년에 사용한 **Deep RNN** 모델이다. <br>
encoder의 마지막 hidden state가 앞선 정보들을 다 가지고 있고 이를 decoder의 첫 hidden state로 평행하게
전달한다. start token을 받아 decoding이 시작되며 output을 input으로 받다가 end token이 나오면
decoding이 끝난다.
![Deep RNN](https://whikwon.github.io/images/deep_rnn.PNG)

다음은 **Conditional Recurrent Language** 모델이다. 모델명에서 기존 모델에
조건부로 무언가 추가되었음을 예상해볼 수 있겠다. <br>
조건부가 추가된 곳은 decoder이며 내용은 바로 Encoder의 마지막 hidden state에서 나온 값이다.
즉, 기존 RNN에서 매번 단어를 생성할 때 각각 encoding한 결과를 고려하게 되었으며
이전 hidden을 지날 때마다 전달이 약해지는 점을 고려했을 때 encoder의 값을 전달해줬을 때
조금 더 input의 의미가 뒤 쪽 sequence까지 전달이 잘 될 것이라고 볼 수 있겠다. <br>
아래는 Conditional RNN 모델이며 $$Y=h_7$$이 decoder에 모두 연결되어 있음을 볼 수 있다. <br>
![Conditional RNN](https://whikwon.github.io/images/conditional_RNN.PNG)

NMT가 어느 정도 성능을 발휘하는지 기존 모델과 비교해서 보자. <br>
NMT는 2015년부터 사용되기 시작했고 1년 만에 다른 모델들보다 나은 성능을 보이는 것이 입증되었다.
그리고 여기서 주의 깊게 봐야할 것은 기존 모델들의 성장 곡선을 봤을 때 어느 정도 한계점에 도달해
정체되어 있는 것을 알 수 있다. 그러므로 NMT의 어떤 특징이 기존 모델들이 가지던 한계를 넘을 수 있도록
했고, 이제 시작이므로 앞으로 더 큰 발전을 기대해도 될 듯하다.
![MT Trend](https://whikwon.github.io/images/MT_trend.PNG)

이렇듯 좋은 성능을 내는 NMT이 몇 가지 특징이 있는데 아래와 같다.

1. **End-to-end training** : 기존 MT과 차별되는 매우 중요한 점이다. 이 특징으로 인해 딥러닝 분야가 이렇게 빠르게
발전하지 않나 싶다. <br>
2. **Distributed represetations share strength** : word2vec에서도 distributed representation의 중요성을 많이 언급했지만
단어 혹은 절 사이의 유사성을 잘 잡아내어 언어학적인 규칙이나 패턴을 나타낼 수 있다. <br>
3. **Better exploitation of context** : 더 큰 문맥에 대해 정확하게 번역이 가능하다. <br>
4. **More fluent text generation** : 자연스러운 표현이 많고 번역의 질이 높다. <br>

아직 해결하지 못한 부분은 내부가 Black box라서 어떤 식으로 진행되는 지 알기 어렵다는 점,
Explicit use of syntactic/semantic/discourse structure 한다는 점인데 이해가 잘 되지 않아서 유추하면
사람들이 쓰는 문장 구조나 의미들을 있는 그대로 사용한다는 게 아닐까 싶다. 애매한 표현이나 중복된 단어같은 경우
구별하기가 힘들다는 뜻이지 않을까

Manning 교수님이 중국어 한 문장을 몇 년간 번역한 내용이 나오는데 NMT로 바꾸고 과거에 비해 번역의 품질이 엄청나게 좋아진 것을
보여주고 Microsoft, Google 등 많은 기업들에서 NMT에 많은 투자를 하고 있다고 한다.
아마도 NLP쪽에서 MT분야가 가장 돈이 많이 되고 먼저 선점했을 때의 이점을 누리려고 그러는 듯하다.

## Attention
기존 seq2seq의 문제점은 문장이 짧을 때의 성능은 괜찮으나 길어질 수록 성능이 저하되는 것을 볼 수 있다.
이는 encoder의 마지막 hidden_state가 모든 정보를 담고 있기 때문에 encoder를 지나면서 정보의 손실이 많아져서
그럴 것이라고 생각할 수 있다. <br>
그래서 이를 해결하고자 나온 것이 **attention mechanism** 의 도입이다. <br>
핵심은 decoder의 hidden_state가 encoder의 hidden_state의 정보들에 접근할 수 있도록 해서 필요한 정보들을
가중치를 줘서 가져올 수 있게 하는 것이다.(*아래 그림*) 이럴 경우 encoder내 정보 손실을 막을 수 있고 decoder 입장에서도
실제 필요한 정보만 뽑아서 쓰는 것이므로 win-win이다. <br>
![attention](https://whikwon.github.io/images/attention.PNG)

그리고 attention을 사용했을 때 alignment라는 한가지 더 이점을 얻을 수 있다. decoder의 경우 output을
생성할 때 encoder의 hidden_state에 가중치를 준 합으로 나타내므로 가중치를 확인하면 encoder의 어떤 단어와 가까운지
statistical하게 나타낼 수 있다.

![alignment](https://whikwon.github.io/images/alignment.PNG)

그럼 기본적인 attention의 구성에 대해서 설명하도록 하겠다. 4단계로 구성되며
각각에 대한 설명은 아래와 같다.

1. **Scoring** : encoder의 $$\bar{h}_s$$와 decoder의 $$\bar{h}_{t-1}$$으로부터 score를 구한다.
2. **Normalization** : encoder의 각각의 t에 대해 위에서 얻은 score를 바탕으로 $$a_t(s) = {e^{score(s)} \above 0.5pt \Sigma_{s'} e^{score(s')}}$$ 를 계산한다.
이 값은 각각 encoder hidden의 가중치가 된다.
3. **Context** : 2에서 얻은 값을 가중치로 사용해서 context vector를 구한다. $$c_t = \Sigma_s a_t(s) \bar{h}_s$$
4. **Hidden state** : context vector를 $$h_t$$를 계산할 때 전달해서 $$h_t$$를 구한다.

위 내용을 간단하게 도식화해놓은 것이 아래 그림이며, score를 구하는 방법은 여러가지가 사용된다.
조경현 교수님이 발표하신 논문에 나오는 내용인 $$h_{t}^T \bar{h}_s$$는 단순히 dot product로 유사성을 보는 것이고
Manning 교수님이 참여한 논문에서 약간 수정한 $$h_{t}^T W_a \bar{h}_s$$는 $$W_a$$가 가운데 껴서 나머지 항들이 interaction할 수 만들어준다고 한다.
마지막 식은 Montreal Univ.에서 발표한 것으로 NN의 형태를 가지고 있다고 한다. (*자세히 안 봄*)
![attention mechanism](https://whikwon.github.io/images/attention_2.PNG)

attention의 mechanism을 위에서 봤을 때, 원하는 encoder의 정보만 decoder로 전달할 수 있다는 것은 아마도 긴 문장을 다룰 때
정보의 손실이 적어져서 큰 이득일 것이라고 생각할 수 있다. <br>
하지만, score를 모든 encoder에 대해 구하고 normalization하고 등등의 연산이 추가되므로 학습속도는 느려질 것이다.
따라서 ***추가되는 연산을 어떻게 최소화 할 수 있을까*** 에 대한 고민을 하였고 Global, Local attention 두 가지 방법으로 접근하게 되었다.
Global이 앞에서 본 내용이고 Local은 우리가 필요한 ***몇 개의 encoder*** 만 뽑아서 context vector를 만들어주는 것이다.
추가로 볼 attention 관련 논문에 hard, soft라는 이름으로 나오게 되는데 그 때 자세한 내용은 정리하도록 하겠다.
아래 그림에서 Global과 Local의 접근 방식을 잘 설명해주고 있다.
![global vs local](https://whikwon.github.io/images/global vs local.PNG)

attention 적용했을 때 성능을 비교해보면 짧은 문장에서도 성능이 향상됨을 볼 수 있지만 기존 모델과 비교했을 때
문장 길이가 30이 넘어갔을 때에도 성능이 줄어들지 않는 것을 확인할 수 있다.
![performance](https://whikwon.github.io/images/performance.PNG)

뒤에 Caption generation이나 Doubly attention의 내용이 간략하게 나오는데 해당 논문을 정리할 예정이므로 그 때 얘기하도록 하겠다.

## Decoding

마지막 내용으로 학습이 끝난 후에 입력 값에 대한 Decoding할 때 어떤 방법을 택하는 지 소개하도록 하겠다.
학습이 완료되었을 때 decoder가 생성해야 할 output은 확률 분포에서 가장 적합한 값을 내놓는다고 보면 되겠다.
생성은 end token인 &lt;eos&gt;를 $$\hat{x}_t$$가 가지면 끝난다.

1. Exhaustive Search : 모든 경우의 수를 다 고려해서 score를 구하고 가장 확률이 높은 것을 뽑아낸다. (*절대 안 쓴다.*)
2. Ancestral Sampling : 각각 단어 생성할 때 이전 조건에 대해서 다 고려해서 생성한다. <br> (*학습할 때 decoding이랑 같은듯*)
장점은 효율적이고 unbiased한데 단점은 variance가 높고 비효율적이다.(?)
![ancestral](https://whikwon.github.io/images/ancestral_sampling.PNG)
3. Greedy Search : 가장 높은 확률의 단어를 계속 생성하고 타고 들어가서 문장을 생성한다.
엄청 빠르다. 근데 단어 하나만 보고 넘어가니까 suboptimal하다.
![greedy](https://whikwon.github.io/images/greedy_search.PNG)
4. Beam Search : 확인할 개수인 k를 정해서 각 단어 별로 k개 만큼 확인한 다음에 이를 k개로 다시 간추리고
또 k개 보고.. 반복해서 최종적으로 얻은 결과 문장 중 가장 확률이 높은 값을 output으로 갖는다.
속도도 괜찮고 성능도 k에 따라 조절이 가능해서 가장 많이 쓰는 방법이다. 특히나 NN에서는 k값을 작게 두고
학습을 시켜도 잘 되서 많이 쓴다. <br>
(*$$k=1$$인 beam search가 Greedy Search이다.*)
![beam](https://whikwon.github.io/images/beam_search.PNG)


Reference: <br>
[Stanford CS224n lecture10 slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf) <br>
Image from [Sennrich 2016](http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf) <br>
Dzmitry Bahdanau, KyungHuyn Cho, and Yoshua Bengio. [Neural Machine Translation by Jointly Learning to Translate and Align.](https://arxiv.org/pdf/1409.0473) 2014.
