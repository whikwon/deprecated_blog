---
title:  "RL_Lecture3: Planning by Dynamic Programming"
date: 2017-11-28 00:00:00
comments: true
---

- David Silver의 Reinforcement Learning 3강을 정리한 내용이다.

***
## Dynamic programming(DP)
### 1) 정의
  - Dynamic: sequential, temporal component에 대한 문제를 푸는 것.
  - Programming: 수학적인 문제(policy)를 푸는 것.

### 2) 요구 사항
  - Optimal substructure: subproblem들로 쪼개서 solution을 구하면 전체 optimal solution을 구할 수 있다. (*Principle of optimality*)
  - Overlapping subproblems: subproblem들이 반복되므로 저장하고 다시 사용할 수 있다.
  - Markov Decision Process(MDP)는 두 가지 특징 모두 만족한다.
    - Bellman equation을 통해 recursive decomposition 가능하다.
    - Value function으로 solution 저장하고 다시 사용할 수 있다.

### 3) Planning
  - Dynamic programming은 MDP의 full knowledge를 알고 있다고 가정한다. 모델(혹은 env)을 알고 학습하는 과정이기 때문에 *planning* 이라고 한다.
  - Prediction: MDP(MRP), policy가 주어졌을 때 value function($$v_{\pi}$$)을 구하는 것. policy를 평가할 수 있다.
  - Control: MDP가 주어졌을 때 optimal value function($$v_{\ast}$$)과 optimal policy($$\pi_{\ast}$$)를 구하는 것. 최종 목적인 optimal solution을 구할 수 있다.

### 4) Other applications
  - DP는 Scheduling, String, Graph algorithms, Graphical models, Bioinformatics등에 쓰이며 보다시피 sequential 혹은 temporal한 문제를 잘 푼다고 보면 된다.

***
## Policy Evaluation
### 1) Iterative Policy Evaluation
policy $$\pi$$에 대해 평가를 하기 위해서 value function을 구한다. 방법은 *iterative application of Bellman expectation backup*
을 사용한다. 아래 그림을 보면 쉬운데 Bellman equation에 따라서 순서대로 one-step lookahead 방식으로 value function을 구하는 방법이다. ($$v_1 \rightarrow v_2 \rightarrow ... \rightarrow v_{\pi}$$)
순서대로 처리하므로 *synchronous* backup의 형태로 진행이 되는데 나중에 *asynchronous* backup에 대해서도 살펴본다고 한다. (*수렴에 대한 내용은 뒤에서 살펴본다고 한다.*)

![iterative policy evaluation](https://whikwon.github.io/images/david_silver/iterative_policy_evaluation.png)

### 2) 예시: Small Gridworld
policy evaluation을 통해서 value function의 참 값을 구할 수 있는지 살펴보는 예제이다. $$4 \times 4$$ 바둑판에서
총 16개의 state와 4개의 action(좌,우,상,하)로 이루어져 있다. 이 때 왼쪽 위 모서리, 오른쪽 아래 모서리로 나가는 문제를 풀며
매 움직임 당 reward를 -1로 준다. policy는 uniform random으로 주어진다.

이 때, 주어진 조건에서 Bellman equation $$v_{k+1}(s) = \displaystyle \sum_{a \in A} \pi(a \lvert s) \big (\mathcal{R}_s^a + \gamma \sum_{s' \in S} \mathcal{P}_{ss'}^a v_k(s') \big )$$
을 풀면 아래와 같이 value function의 참 값을 구할 수 있게 된다. 그리고 이를 통해서 optimal policy도 구할 수 있게 된다.

![gridworld](https://whikwon.github.io/images/david_silver/gridworld.png)
![gridworld](https://whikwon.github.io/images/david_silver/gridworld2.png)
다
근데, 여기서 의문이 드는 점은 굳이 value function을 끝까지 iteration하지 않더라도 우리의 목적인 optimal policy를 구할 수 있지 않을까라는 점이다.


***
## Policy iteration
### 1) Policy iteration
위에서는 policy를 평가했는데 이를 통해서 policy를 개선하자 라는 아이디어에서 출발한다. 평가는 위와 마찬가지로 value function을 통해서 진행하고
개선은 value function에서 action을 greedy하게 택하는 방식을 취한다. 그림으로 나타내면 아래와 같은데 어떤 $$v, \pi$$에서 시작하든 iteration을 통해서
optimal policy, value function에 도달할 수 있다. (*수렴에 대한 내용은 뒤에서 살펴본다고 한다.*)

![policy iteration](https://whikwon.github.io/images/david_silver/policy_iteration.png)

### 2) 예시: Jack's Car Rental
렌트 업체 2군데에 20대의 차가 각각 위치할 수 있으며 현재 차의 대수를 state로 갖는다. action을 통해 최대 5대의 차까지 업체 간에 이동시킬 수 있으며
렌트된 차 한 대당 $10를 reward로 받는다. 차가 반납되거나 요청되는 것은 무작위로 일어나며 Poisson 분포를 따른다고 한다.

policy iteration을 통해 policy, value function을 구하면 iteration에 따라 다음과 같이 변하는 것을 볼 수 있다.  

![jack's car rental](https://whikwon.github.io/images/david_silver/jacks_car.png)

### 3) Policy improvement
policy 개선에 대해 조금 더 상세하게 설명해보자. policy를 deterministic하다고 가정하고 이전에 배운 것처럼 action-value fuction으로부터
policy를 개선할 수 있다. 그럼 개선한 policy가 이 전 step보다 낫다는 걸 보일 수 있어야 하는데 이는 value function의 비교를 통해 보일 수 있다.
(*2강에서 배운 Theorem*) 그럼 결국 policy가 변해도 value function이 변하지 않을 경우에 개선이 멈췄다고 볼 수 있고 이 때 value function, policy가 optimal하다고 볼 수 있다.

![policy improvement](https://whikwon.github.io/images/david_silver/policy_improvement.png)
![policy improvement](https://whikwon.github.io/images/david_silver/policy_improvement2.png)

### 4) Modified Policy iteration
policy iteration에서 value function과 policy를 번갈아가면서 update를 해준다. 여기서 차라리 그냥 policy evaluation을 어느 정도 한 뒤에 구한 value function을
바탕으로 policy를 구할 수 있지 않을까? 라는 생각을 할 수 있다. (*small gridworld의 경우 k=3일 때 policy를 구하면 최적의 policy를 얻을 수 있었다.*)
그렇게 해서 나온 것이 *value iteration* 이다.


***
## Value iteration
### 1) Principle of Optimality
value iteration 알고리즘은 principle of optimality theorem으로부터 출발한다. Dynamic programming을 떠올려보면 optimal policy는 2개의 component
$$A_{\ast}$$(optimal first action)와 $$S'$$(successor state)로 나눌 수 있다. 그럼 policy가 만약 optimal이라면 $$s'$$에서도 $$v_{\pi}(s') = v_{\ast}(s')$$를
만족해야 한다. 이런 식으로 step 별로 쪼개서 생각을 해보면 결국 one-step lookahead가 되는데 $$v_{\ast}(s) \leftarrow \underset {a \in A} {max} \mathcal{R}_s^a + \gamma \displaystyle \sum_{s' \in S} \mathcal{P}_{ss'}^a v_{\ast}(s')$$를 통해서 optimal value function을 구할 수 있다. value iteration은 이 과정을 iteration하면 된다.

![optimality principle](https://whikwon.github.io/images/david_silver/optimality_principle.png)

### 2) 예시: shortest path
아래는 value iteration을 통해서 value function 값을 구하는 예시이다. 위에서 본 Small Gridworld와 같은 action, state, policy를 가지며 출구가 하나로 줄었다.
iteration에 따라서 value function 값이 구해지고 7번 진행 후 optimal policy를 구할 수 있는 것을 보여준다.

![shortest path](https://whikwon.github.io/images/david_silver/shortest_path.png)

### 3) Value iteration
optimal policy $$\pi$$를 구하는 것을 목적으로 *iterative application of Bellman optimality backup* 을 사용한다. policy evaluation과 마찬가지로
value function을 synchronous하게 구해서 optimal value function $$v_{\ast}$$를 얻는다. policy iteration과 차이점은 value에 대해서만 iteration하고
마지막에 한 번만 policy를 결정하므로 과정 중간에 explicit policy는 없다는 점과 수렴되지 않은 중간 value function을 통해서 policy를 구할 수 없다는 점이 있다.

![value iteration](https://whikwon.github.io/images/david_silver/value_iteration.png)

***
## Dynamic Programming Algorithms
지금까지 배운 알고리즘들을 정리하면 아래와 같다. 모두 MDP가 주어졌을 때 planning을 하는 문제이고 Prediction, Control을 하며
각각 다른 Bellman Equation을 solution으로 사용한다. complexity는 state-value function의 경우 $$n$$ state에 대해 $$n$$개의 다음 state와
$$m$$개의 action을 고려해야 하니 $$O(mn^2)$$, action-value function의 경우 state, action에 대해 모두 고려해야 하니 $$O(m^2n^2)$$를 갖는다.

![DP algorithms](https://whikwon.github.io/images/david_silver/DP_algorithms.png)

***
## Extension to Dynamic Programming
synchronous하게 진행하는 것은 비효율적이므로 이를 해결하기 위해 Asynchronous하게 각각의 state에 대해 backup하는 것을 생각할 수 있다.
이 때에도 state가 계속해서 선택된다면 수렴을 보장할 수 있다. 사용하는 여러 가지 방법 중 3가지를 살펴보도록 하자.

### 1) In-place dynamic Programming
아래 식과 같이 기존에는 $$v_{\text{new}}(s), v_{\text{old}}(s)$$의 변수를 함께 사용했는데 이를 $$v(s)$$ 하나만 놓고 사용해서 좀 더 효율적으로 연산이 가능하게 한다.

![in-place DP](https://whikwon.github.io/images/david_silver/inplace_DP.png)

### 2) Prioritized sweeping
학습이 시작되는 처음을 생각해봤을 때, reward를 얻는 마지막 goal 지점을 제외하고는 모두 Value function의 값이 0을 갖게 된다. 그럼 모든 지점에 대해서 update하는게
아무런 의미가 없어지는데 이는 매우 비효율적이다. 그래서 어느 정도 update할 부분을 focus해서 search하는 것이 필요해서 나온 것이 prioritized sweeping이다.
방법은 focus할 부분을 정할 때 Bellman error를 사용하는 것이다. Bellman error는 현재 state에서 update하고자 하는 방향을 나타내주는데 이 값이 매우 작은 경우에는
학습 초기 단계이거나 거의 수렴한 경우이므로 제외하고 update해도 무방하다. 아래 알고리즘처럼 queue를 이용해서 이전 state에서의 error를 기억한 뒤 선택적으로 update해주면 된다.

![prioritized sweeping](https://whikwon.github.io/images/david_silver/prioritized_sweeping.png)

### 3) Real-time dynamic programming
real-time DP는 실제 agent의 경험을 바탕으로 관련이 있는 state만 sampling해서 업데이트를 하는 방식이다. 시작 지점으로부터 optimal policy에서 아예 도달하지 않을
state의 경우에는 배제하고 관련이 있는 state만 update하는 것이다.

![real-time DP](https://whikwon.github.io/images/david_silver/realtime_DP.png)

### 4) Full-Width backups
지금까지 DP는 *full-width* backup으로 진행되었다. 현재 상태의 value를 update하기 위해서 미래 전체 state에 대한 value 정보를 얻는 방식인데 이는 크기가 작은
tabular한 경우에는 괜찮지만 크기가 커지면 연산량이 너무 많아져서 비효율적이고 해결할 수 없게 된다. 그래서 이를 해결하기 위해서 *sample* backup 방법이 나오게 되고
이는 다음 장에서 살펴 볼 내용이다. 추가적으로 DP에서는 MDP가 완전히 주어졌을 때에 대한 문제를 푸는데 이 또한 현실에서는 거의 불가능한 일이므로 MDP가 주어지지 않았을 때,
*Model-free* 방법으로 푸는 것도 다음 장에서 살펴 볼 예정이다.


Reference: <br>
[David Silver RL Lecture3 slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf)
