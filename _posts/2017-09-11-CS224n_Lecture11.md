---
title:  "CS224n_Lecture11"
date: 2017-09-11 00:00:00
comments: true
---

- CS224n Lecture11, Lecture-note6에 대한 내용을 정리하였다.

먼저 11강의 내용의 흐름을 정리하면 먼저 seq2seq 모델이 NMT 분야에서 거쳐온 흐름을 설명한다. 그리고 MT의 evaluation 방법을
간단하게 소개하고 마지막으로 NMT가 가졌던 문제점들과 이를 해결하기 위한 접근 방법들을 차례대로 소개하고 있다.

seq2seq 모델은 NER label 구별하는 것과 같은 single output이 아닌 문장을 번역하거나 요약하거나 하는 일을 하기 위해 고안되었다.
기본적으로 *encoder-decoder* 구조를 가지며 주로 LSTM과 GRU를 사용한다. 초창기 모델에서는 input의 정보가 단방향으로 encoder의 마지막 hidden state에
decoder로 넘겨주었다. 이 때, 어순이 다른 언어 간 번역일 경우 역방향도 고려하는 게 유리해서 bidirectional RNN을 사용하게 되었고
encoder의 마지막 hidden state에서만 정보를 넘겨주었을 때 정보 손실이 많이 발생해서 모든 hidden state를 고려하기 위해서 attention mechanism이 도입되었다.

## 1) Seq2seq의 decoder

여기서 잠깐 학습이 끝나고나서 decoder의 output 생성 방법을 살펴보자. 크게 3가지로 구분된다.
- **Ancestral sampling** : $$x_t \sim \mathbb{P}(x_t \vert x_1, ..., x_n)$$ <br>
수식 그대로 $$t$$에서 이전 상태를 고려해서 $$x_t$$의 조건부확률을 구하는 방식이다. 연산량이 크고 성능이 낮은 편에 속하며 variance가 높아서 많이 사용하지 않는다.
- **Greedy search** : $$x_t = argmax_{\tilde x_t} \mathbb{P}(\tilde x_t \vert x_1, ..., x_n)$$ <br>
위 식에서 약간 변형이 되었는데 이전 상태를 고려한 $$x_t$$의 조건부확률이 가장 높은 값을 $$x_t$$ 놓고 진행한다. 연산량이 대폭 감소되었으나
중간에 하나라도 틀리게되면 전체 output이 이상해질 수 있다는 단점이 존재한다.
- **Beam search** : $$\mathcal{H}_t = {(x_1^1, ..., x_t^1), ..., (x_1^K, ..., x_t^K)}$$ <br>
위 식처럼 확률이 가장 높은 $$K$$개 후보를 뽑아놓는 형태로 진행이 된다. <br>
$$\tilde {\mathcal{H}}_{t+1} = \bigcup_{k=1}^K \mathcal{H}_{t+1}^{\tilde k}$$ where
$$\mathcal{H}_{t+1}^{\tilde k} = \{(x_1^k,...,x_t^k, v_1),...,(x_1^k,...,x_t^k,v_{\vert V \vert})\}$$ <br>
그리고 $$t+1$$ 상태로 넘어갈 때는 $$K$$개를 뽑고, 기존의 것들과 비교($$K*K$$)해서 다시 $$K$$개의 후보로 줄인다.
이런 식으로 최종적으로 확률이 가장 높은 $$K$$개의 값을 얻을 수 있다. $$K$$를 증가시키면 precision이 올라가고 asymptotically exact해진다.
하지만 연산량이 증가해서 적정한 값을 지정하는게 중요하다. NMT에서 가장 많이 쓰이는 방법이다.

## 2) MT 평가 방법
정리 예정이다.



## 3) NMT 문제점과 극복 방안

NMT의 장점은 domain 지식이 크게 필요하지 않고 end-to-end로 학습이 가능하다는 점이며 단점은 training의 complexity로 인해 **target words의 수가 제한적이다.**
주로 target vocabulary를 가장 빈번한 k개의 shortlist를 선정해서 사용하는데 보통 30K-80K 사이이며 이 외 단어들은 *unknown* 으로 처리한다. <br>
unknown word는 적을 때는 문제가 없지만 증가할수록 성능을 크게 떨어트려서 실제 학습 시에 문제가 된다. 이 문제를 해결하기 위한 모델들이 계속해서 발표되고 있고
아직도 해결할 부분이 많다고 한다.

대표적인 논문 몇 가지를 살펴볼 예정이며 아래에 간단하게 정리해놓은 내용을 보고 더 자세하게 살펴보도록 하자.

- **Importance sampling** : 연산량을 위해 target vocabulary 크기를 제한하고, context는 건들지 않는다. 빈번하지 않은 단어는 *unknown* 로 학습이 진행된다. <br>
- **Pointer** : 위와 마찬가지이다. 그렇게 진행되어야 context word가 그대로 복사되어 *unknown* 를 대체할 것이기 때문이다. <br>
- **Byte pair encoding** : *subword* 를 생성해서 target vocabulary에 넣고 같이 학습을 시킨다. *subword* 가 포함된 단어들은 쪼개져서 각각 단어들처럼 학습이 되고
나머지 빈번하지 않은 단어는 *unknown* 을 target으로 학습이 진행된다. <br>
- **Hybrid** : Hybrid에서는 context, target vocabulary의 크기를 모두 제한한 후에 빈번하지 않은 단어에 대해서 char-based는 input-output
그리고 마찬가지로 word는 *unknown* 을 target으로 학습, char는 해당 단어를 target으로 학습이 진행된다.

## 논문 1) On Using Very Large Target Vocabulary for Neural Machine Translation

14년 논문으로 NMT가 나온지 얼마되지 않았을 때이다. <br>
위에서 설명한 *unknown/rare word* 문제를 해결하기 위해서 target vocabulary의 수를 증가시키는 방법으로 접근하였고 **importance sampling** 기반의 근사 방법을 제시한다.
(*학습 속도는 크게 증가시키지 않으면서 target vocabulary의 수를 증가시키는 것이 목적이다.*)

방법은 총 vocabulary($$V$$) 수를 크게 증가시키고 그 중에서 subset($$V'$$)을 뽑은 뒤에 학습시키는 방법이다.
주로 $$|V| = 500K,\ |V'| = 30K/50K$$ 크기로 사용한다. subset은 순차적으로 예제를 선택한다.
(*word2vec에서 살펴본 NCE와 유사하다.*)
![segment data](https://whikwon.github.io/images/NMT_segment_data.png)
그리고 test시에는 $$K$$개의 빈번한 shortlist를 뽑고, 그 중 적합한 $$K'$$개의 후보를 선정하는 방식을 취한 다음
가장 높은 확률의 문장을 선택한다. (*shortlist 만드는 기존 방식과 beam search의 방식을 쓴 듯한데 특별한게 있나 싶다*)
주로 $$K' = 10/20,\ K = 15K/30K/50K$$의 크기로 사용한다.
![candidate word](https://whikwon.github.io/images/NMT_candidate_word.png)

## 논문 2) Pointing the Unknown words

16년 논문으로 위에서 살펴본 것처럼 *unknown/rare word* 문제를 해결하기 위한 새로운 접근 방법을 제시하였다. <br>
핵심적인 내용은 **pointer가 어딜 언제 가리킬지 배우는 것** 이다. **pointer** 은 unknown word가 발생했을 때
본문의 내용을 번역 내용의 적절한 위치에 그대로 복사해서 붙여넣는 역할을 한다. 주로 고유명사의 경우 빈번하지 않아
unknown에 포함이 많이 되고 어떤 요약을 할 때에도 본문의 내용을 그대로 붙여넣는 경우가 있어 직관적으로 효과가 있을 것이라고
생각할 수 있다. 아래 그림처럼 본문의 내용을 번역할 때 그대로 복사해서 붙여넣는다. <br>
![pointer](https://whikwon.github.io/images/NMT_pointer.png) <br>

아래 그림은 모델의 구조를 나타내고 있다.
![structure](https://whikwon.github.io/images/NMT_pointer_structure.png) <br>
언제 pointer를 써야할지 알기 위해서(*unknown 인지 알기 위해서*) shortlist에 포함되는 지 안 되는지
판단이 필요하다. 이를 판단해주기 위해서 **switching network($$z_t$$)** 가 사용된다. multilayer perceptron을
사용하며 0, 1을 반환한다. 1일 경우 **shortlist softmax($${y^w}_t$$)** 이 진행되고 0일 경우엔 **location softmax($${y^l}_t$$)** 이 진행된다.

shortlist softmax은 앞선 모델들에서 한 target vocabulary에서 단어들의 확률을 구하는 방법이고 location softmax는
attention mechanism과 관련이 있는데 번역을 해야할 단어와 연관이 있는 본문의 단어의 확률을 나타낸다. ($$l_t$$) <br>
즉, 본문의 단어를 그대로 복사해서 사용할건데 가장 영향력이 큰 단어를 쓰겠다는 얘기이다.

학습은 입력 값 $$x$$에 대해서 어떤 softmax를 쓸 건지의 확률인 $$z_t$$ 을 최대로하고 각각의 softmax인 $$y_w, y_l$$이 최대가 되는 방향으로
진행하면 되겠다. <br>
위 내용에 따라서 Objective function은 $${argmax}_{\theta} {\frac 1 N} \Sigma_{n=1}^N log p_{\theta}(y_n, z_n \vert x_n)$$ 이다.

## 논문 3) Neural Machine Translation of Rare Words with Subword Units

16년에 나온 논문으로 위와 같은 문제를 해결하기 위해서 핵심 내용으로 **Byte pair encoding** 방법을 제시한다. <br>
논문의 목표는 *back-off model* 없이 rare word를 **byte pair encoding(BPE)** 방법으로 **subword unit** 으로 encoding하고 성능이 좋게 나오는 것을 보이는 것이다.

먼저, subword translation의 가능성에 대해서 얘기를 하고 있다. 3가지 경우를 예로 들면서 *transparent translation(이거 무슨 뜻이지..)*
이 가능하다고 주장한다. ***name entity, cognates and loanwords, morphologically complex words*** 가 그것들이다.
원리는 언어 간의 번역이 진행될 때 subword 단위의 규칙을 찾아내는 것이다. 예를 들면, 외래어를 한글로 번역할 때 고유 명사에 해당되는 규칙이 있고
***er*** 같은 비교 표현이 들어갔을 때는 형태소를 쪼개서 어떤 뜻을 가지는 지 규칙을 찾아낼 수 있을 것이다. 이렇게 빈번하게 사용되는 규칙을 모아서
shortlist에 함께 학습시키면 *unknown word* 일지라도 규칙을 조합해서 번역할 수 있겠다는 내용이다.

subword translation을 위한 방법으로 **BPE** 을 제시한다. 단어를 문자로 나누고 n-gram을 사용해서 자주 나오는 문자 조합을 얻는다.
아래 예시를 보면 ***es, est, lo*** 등 빈번하게 나오는 순으로 subword를 얻을 수 있다.
![BPE](https://whikwon.github.io/images/NMT_BPE.png)

## 논문 4) Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models

16년에 나온 논문으로 word-based, char-based RNN을 함께 사용한 Hybrid 모델이다. <br>
기존의 다른 접근 방법, 특히 pointer가 간과하는 점에 대해서 지적한다. 첫째는, 언어 내에 *monolingually* 한 특징이 있어서 형태학적으로
서로 단어들이 연관이 있는데 이를 독립적으로 처리해서 유사한 단어가 나와도 빈도수가 적으면 학습하기 힘들다는 점이다.
둘째는, 언어 간 *crosslingually* 한 특징이 있어서 같은 고유명사라도 다른 표기법을 갖고 이를 전부다 학습하기가 어렵다는 점이다.
(*이건 pointer의 단점이 아닌듯한데 어쨋든*)

전체적인 구조는 아래 그림을 보면 알 수 있다. 아래 예시에서는 "a &lt;unk&gt; cat" 문장이 입력되고 여기에 'c','u','t','e'를 문자열로
쪼개서 따로 입력해준다. 이 때, "un &lt;unk&gt; chat"이 생성되었다. 모델은 &lt;unk&gt;를 인식하고 미리 학습시켜놓은 RNN을 통과시켜서
'j','o','l','i' 라는 문자열을 만들어낸다. 입력할 때는 &lt;unk&gt;을 char-based 모델을 통과시켜서 정보를 전달해주고 출력할 때는
&lt;unk&gt;를 인식하면 받은 정보를 바탕으로 char-based 모델을 통과시키는 전략이라고 할 수 있다. <br>
![structure](https://whikwon.github.io/images/NMT_hybrid_structure.png) <br>

조금 더 상세하게 보자. 2가지 모델(*word-based, char-based*)를 함께 학습시켜야 하므로 objective function은
$$J = J_w + \alpha J_c$$가 되겠다. $$\alpha$$는 word-based, char-based 학습에 대한 가중치이다.
decoder의 hidden state에서 &lt;unk&gt;가 출력되었을 때 char-based 모델을 실행시켜야 하므로 이 때 initialize할 정보가 필요하다.
(*source 정보에 dependent하다*) 이 때 attentional vector $${\tilde h}_t = tanh(W[c_t;h_t])$$를 사용한다.
아래 그림에 attentional mechanism이 잘 설명되어 있다. <br>
![attention](https://whikwon.github.io/images/NMT_hybrid_attention.png) <br>
근데 attentional vector를 그대로 사용하면 $${\tilde h}_t$$가 &lt;unk&gt;, char생성 둘 다 하는 *same-path* 로 문제가 생긴다고 한다.
그래서 *separate-path* 로 만들어주기 위해서 counter vector $${\check h}_t = tanh(\check W[c_t;h_t])$$ 를 따로 학습시켜서 사용한다.

이렇게 학습이 완료되면 decoding은 2단계로 진행이 된다. word-level beam search가 진행되고 &lt;unk&gt;에 대해서 char-level beam search가 진행된다.


Reference: <br>
[Stanford CS224n lecture11 slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture11.pdf) <br>
[Stanford CS224n lecture-note](http://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-notes6.pdf) <br>
Jean et al. [2015, On Using Very Large Target Vocabulary for Neural Machine Translation](https://arxiv.org/pdf/1412.2007) <br>
Gulcehre et al. 2016, [Pointing the Unknown Words](https://arxiv.org/pdf/1603.08148) <br>
Sennrich et al. 2016, [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909) <br>
