---
title:  "CS224n_Lecture11"
date: 2017-09-11 00:00:00
comments: true
---

- CS224n Lecture11에 대한 내용을 정리하였다.

먼저 11강의 내용의 흐름을 정리하면 전 시간에 배운 GRU, LSTM을 정리해서 다시 설명하고 MT의 evaluation 방법을
간단하게 소개한다. 그 후에 NMT가 가졌던 문제점들과 이를 해결하기 위한 접근 방법을 논문을 통해서 소개하고 있다.

NMT의 장점은 domain 지식이 크게 필요하지 않고 end-to-end로 학습이 가능하다는 점이며 단점은 training의 complexity로 인해 target words의 수가 제한적이다.
주로 target vocabulary를 가장 빈번한 k개의 shortlist를 선정해서 사용하는데 보통 30K-80K 사이이며 이 외 단어들은 *unknown* 으로 처리한다. <br>
unknown word는 적을 때는 문제가 없지만 증가할수록 성능을 크게 떨어트려서 실제 학습 시에 문제가 된다.  

보다보니까 조금씩 shortlist의 맥락이 다른 느낌이다.

처음에는 softmax의 연산량으로 target vocabulary만 제한하고, context는 건들지 않는다. 그럼 빈번하지 않은 단어는 학습 시에 *unknown* 로 학습이 진행되어야 한다.
Pointer에서도 마찬가지이다. 그래야 context word가 그대로 복사되어 *unknown* 를 대체할 수 있기 때문이다.
BPE에서는 약간 다른데, *subword* 를 생성해서 target vocabulary에 넣고 같이 학습을 시킨다. *subword* 가 포함된 단어들은 쪼개져서 각각 확률이 존재하게 되고
여기에서도 선택받지 못한 값들은 *unknown* 을 target으로 학습이 진행되겠다.
Hybrid에서는 context, target vocabulary를 모두 제한한 후에 *unknown* 를 char-based training을 시킨다. (*특이하다*)
그리고 마찬가지로 word는 *unknown* 을 target으로 학습, char는 해당 단어를 target으로 학습이 진행된다.

1. On Using Very Large Target Vocabulary for Neural Machine Translation

14년 논문으로 NMT가 나온지 얼마되지 않았을 때이다.
위에서 설명한 *unknown/rare word* 문제를 해결하기 위해서 target vocabulary의 수를 증가시키는 방법으로 접근하였고 **importance sampling** 기반의 근사 방법을 제시한다.
(*학습 속도는 크게 증가시키지 않으면서 target vocabulary의 수를 증가시키는 것이 목적이다.*)

방법은 총 vocabulary($$V$$) 수를 크게 증가시키고 그 중에서 subset($$V'$$)을 뽑은 뒤에 학습시키는 방법이다.
주로 $$|V| = 500K,\ |V'| = 30K/50K$$ 크기로 사용한다. subset은 순차적으로 예제를 선택한다.
(*word2vec에서 살펴본 NCE와 유사하다.*)
![segment data](https://whikwon.github.io/images/NMT_segment_data.png)
그리고 test시에는 $$K$$개의 빈번한 shortlist를 뽑고, 그 중 적합한 $$K'$$개의 후보를 선정하는 방식을 취한 다음
가장 높은 확률의 문장을 선택한다. (*shortlist 만드는 기존 방식과 beam search의 방식을 쓴 듯한데 특별한게 있나 싶다*)
주로 $$K' = 10/20,\ K = 15K/30K/50K$$의 크기로 사용한다.
![candidate word](https://whikwon.github.io/images/NMT_candidate_word.png)

2. Pointing the Unknown words

16년 논문으로 위에서 살펴본 것처럼 *unknown/rare word* 문제를 해결하기 위한 새로운 접근 방법을 제시하였다.
핵심적인 내용은 **pointer가 어딜 언제 가리킬지 배우는 것** 이다. **pointer** 은 unknown word가 발생했을 때
본문의 내용을 번역 내용의 적절한 위치에 그대로 복사해서 붙여넣는 역할을 한다. 주로 고유명사의 경우 빈번하지 않아
unknown에 포함이 많이 되고 어떤 요약을 할 때에도 본문의 내용을 그대로 붙여넣는 경우가 있어 직관적으로 효과가 있을 것이라고
생각할 수 있다. 아래 그림처럼 본문의 내용을 번역할 때 그대로 복사해서 붙여넣는다. <br>
![pointer](https://whikwon.github.io/images/NMT_pointer.png) <br>

아래 그림은 모델의 구조를 나타내고 있다.
![structure](https://whikwon.github.io/images/NMT_pointer_structure.png) <br>
언제 pointer를 써야할지 알기 위해서(*unknown 인지 알기 위해서*) shortlist에 포함되는 지 안 되는지
판단이 필요하다. 이를 판단해주기 위해서 **switching network($$z_t$$)** 가 사용된다. multilayer perceptron을
사용하며 0, 1을 반환한다. 1일 경우 **shortlist softmax($${y^w}_t$$)** 이 진행되고 0일 경우엔 **location softmax($${y^l}_t$$)** 이 진행된다.

shortlist softmax은 앞선 모델들에서 한 target vocabulary에서 단어들의 확률을 구하는 방법이고 location softmax는
attention mechanism과 관련이 있는데 번역을 해야할 단어와 연관이 있는 본문의 단어의 확률을 나타낸다. ($$l_t$$) <br>
즉, 본문의 단어를 그대로 복사해서 사용할건데 가장 영향력이 큰 단어를 쓰겠다는 얘기이다.

학습은 입력 값 $$x$$에 대해서 어떤 softmax를 쓸 건지의 확률인 $$z_t$$ 을 최대로하고 각각의 softmax인 $$y_w, y_l$$이 최대가 되는 방향으로
진행하면 되겠다. <br>
위 내용에 따라서 Objective function은 $${argmax}_{\theta} {\frac 1 N} \Sigma_{n=1}^N log p_{\theta}(y_n, z_n \vert x_n)$$ 이다.

3. Neural Machine Translation of Rare Words with Subword Units

16년에 나온 논문으로 위와 같은 문제를 해결하기 위해서 핵심 내용으로 **Byte pair encoding** 방법을 제시한다.
논문의 목표는 *back-off model* 없이 rare word를 **byte pair encoding(BPE)** 방법으로 **subword unit** 으로 encoding하고 성능이 좋게 나오는 것을 보이는 것이다.

먼저, subword translation의 가능성에 대해서 얘기를 하고 있다. 3가지 경우를 예로 들면서 *transparent translation(이거 무슨 뜻이지..)*
이 가능하다고 주장한다. ***name entity, cognates and loanwords, morphologically complex words*** 가 그것들이다.
원리는 언어 간의 번역이 진행될 때 subword 단위의 규칙을 찾아내는 것이다. 예를 들면, 외래어를 한글로 번역할 때 고유 명사에 해당되는 규칙이 있고
***er*** 같은 비교 표현이 들어갔을 때는 형태소를 쪼개서 어떤 뜻을 가지는 지 규칙을 찾아낼 수 있을 것이다. 이렇게 빈번하게 사용되는 규칙을 모아서
shortlist에 함께 학습시키면 *unknown word* 일지라도 규칙을 조합해서 번역할 수 있겠다는 내용이다.

subword translation을 위한 방법으로 **BPE** 을 제시한다. 단어를 문자로 나누고 n-gram을 사용해서 자주 나오는 문자 조합을 얻는다.
아래 예시를 보면 ***es, est, lo*** 등 빈번하게 나오는 순으로 subword를 얻을 수 있다.
![BPE](https://whikwon.github.io/images/NMT_BPE.png)

4. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models

16년에 나온 논문으로 word-based, char-based RNN을 함께 사용한 Hybrid 모델이다.









Reference: <br>
[Stanford CS224n lecture11 slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture11.pdf) <br>
Jean et al. [2015, On Using Very Large Target Vocabulary for Neural Machine Translation](https://arxiv.org/pdf/1412.2007) <br>
Gulcehre et al. 2016, [Pointing the Unknown Words](https://arxiv.org/pdf/1603.08148) <br>
Sennrich et al. 2016, [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909) <br>
