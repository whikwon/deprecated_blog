---
title:  "CS224n_Lecture15"
date: 2017-09-22 00:00:00
comments: true
---

- CS224n Lecture15에 대한 내용을 정리하였다.

15강에서는 coreference resolution에 대한 개념을 설명하고 기존 방법과 딥러닝이 적용된 방법을
차례로 소개한다.

### Coreference resolution 이란?
*Identify all noun phrases* **(mentions)** *that refer to the same real world entity* 이다. <br>
해석해보자면 같은 실재를 나타내는 모든 명사구들을 찾아내는 작업이라고 보면 되겠다. 아래 예시를 보면 ***Barack Obama, his, He*** 가 같은
사람을 나타내는 것을 알 수 있고 ***Hillary Rodham Clinton, her, she, First Lady*** 가 같은 사람을 나타내는 것을 알 수 있다.
![corefernce example1](https://whikwon.github.io/images/NLP_coreference_example1.png)
<center> <i> &lt;Coreference resolution 예시1&gt;</i> </center> <br>
아래는 명사구끼리 pair를 이루는 것을 다양하게 보여준 예시이다. ***his pay*** 는  ***$1.3 million*** 와 같은 의미를 나타내는데 그 안에
***his*** 는 별개로 ***Jonh Smith, CFO of Prime Corp, the 57-year-old*** 와 같은 의미를 나타내는 것을 볼 수 있다.
![corefernce example2](https://whikwon.github.io/images/NLP_coreference_example2.png)
<center> <i> &lt;Coreference resolution 예시2&gt;</i> </center> <br>

Coreference resolution은 NLP의 여러 분야에서 필요로 한다. 아래 목록을 보자.

- 텍스트의 완전한 이해
- 기계 번역 : 일부 언어의 경우 gender를 하나로 사용해서 이를 분리하기 위해서 번역하기 위해서는 문맥 상 누구를 가리키는 지 알 수 있어야 한다.
- 텍스트 요약, 인용
- 질의 응답 : 질문의 대상이 어떤 것을 가리키는 지 정확하게 알아야 적절한 대답을 내놓을 수 있다.   

### Coreference Evaluation(평가 방법)
$$B^3$$(B cubed) evaluation metric을 주로 사용한다. 방법은 아래 그림에 자세히 나와있다. <br>
같은 실재를 나타내는 단어들로 cluster로 만든다. 그 다음 precision(*왼쪽*)과 recall(*오른쪽*)을 그림과 같이 구한 뒤에
cluster 크기에 따라서 가중치를 줘서 전체 precision, recall의 평균을 구하는 방법이다. chain-chain pairing 최적화하는게
computation NP-hard sense로 어려워서 Greedy matching을 많이 쓴다고 한다. <br>
이 외에도 다른 방법들이 있는데 대부분 cluster algorithm을 많이 사용한다고 한다.
![corefernce evaluation](https://whikwon.github.io/images/NLP_coreference_evaluation.png)
<center> <i> &lt;Coreference resolution 평가방법&gt;</i> </center> <br>

### Reference 종류
여러가지 종류의 reference가 있다. 아래를 살펴보자.
- Referring expressions : 직접적으로 나타내는 방법. (*John Smith*)
- Free variables : 다른 명사구와 접촉해서 나타내는 방법. (Smith saw *his pay* increase)
- Bound variables : 다른 명사구와 연결해주는 방법. (The dancer hurt *herself*)
주로 Free, Bound variables이 문법적인 제약이 있어서 Referring expression보다 학습하기 쉽다고 한다. <br>
***한가지 추가 사항으로 모든 명사구는 referring한다고 생각할 수도 있는데 그렇지 않다.*** <br>
예시 문구로 ***Every(No) dancer*** twisted ***her knee*** 가 있다. 여기에서 ***Every(No) dancer*** 는 non-referential하며
그러므로 다른 명사구들도 마찬가지로 non-referential하게 된다.

### anaphors, cataphors
- anaphors : 사전적으로 **반복을 피하기 위한 대명사** 라는 의미를 가진다. 어떤 term이 다른 term(antecedent라고 한다.)을 나타낼 때
이를 anaphor라고 하며 주로 antecedent는 anaphor보다 앞에 온다고 한다. 어떻게 보면 coreference와 동일하게 생각할 수도 있는데 그렇지 않으며
coreference는 동일한 실재를 나타내는 두 명사구라고 하면, anaphor는 한 명사구가 한 명사구에 의존적이기만(***dependent on***) 해도 된다. <br>
예시로, We went to see a ***concert*** last night. ***The tickets*** were really expensive. 에서 ***concert*** 와 ***The tickets*** 와의 관계이다.
이를 *bridging anaphora* 라고 하는데, 실제 이를 해석하려는 노력은 거의 없다고 한다.
- cataphors : anaphor의 특별한 경우(?)인데 antecedent가 anaphor보다 뒤에 오는 경우라고 보면 되겠다.(*forward anaphora*)
실제 NLP에서 이런 식으로 표현하는 경우는 거의 없다고 한다. (*he가 먼저 나오고 Mike가 나오면 누군지 어떻게 알아*)
anaphor와 coreference 사이에 차이가 있지만 실제 모델에서는 다르게 다루지 않는다고 해서 크게 신경쓰지 않아도 되겠다.

### Hobbs' naive algorithm
coreference resolution의 대표적인 알고리즘으로 Hobbs' 알고리즘이 있다. 2010년대 초반까지도 사용되었다고 하며 정해진 규칙에 따라서
찾아가는 방법이라고 보면 되겠다. 좋은 방법이긴 한데 명확한 한계가 있다. 아래와 같은 경우인데, coreference를 알기 위해서 문맥상 의미를 해석해야 할
필요가 있는 경우에 어려움을 겪는다. 위 아래 문장이 분명 동일한 구조이긴 하지만 ***they*** 가 corefer하는 단어가 각각 ***The city counsil, the women*** 으로
다름을 알 수 있다. 이러한 문제를 해결하기 위해서 *knowledge-based* 모델이 생겨나기 시작했고 Hobbs 역시 naive algorithm을 knowledge-based을 비교할
baseline으로 만들었다고 한다.
![hobbs algorithm limitation](https://whikwon.github.io/images/NLP_hobbs_limitation.png)
<center> <i> &lt;Hobbs' Algorithm의 한계 예시&gt;</i> </center> <br>

### Coreference model 종류
- Mention Pair models : coreference chain들을 단어 간에 연결된 pair를 모아놓은 것이라고 생각한다.
각각의 pairwise decision에 대해서 독립적으로 결정한다. binary classification이랑 같은 방식이라고 생각해도 되겠다. (*하나 하나 다 훑어봐야 되니까 연산이 조금 오래걸리지 않을까 싶다.*)
- Mention ranking models : mention에 대해서 모든 antecedent 후보를 모은 다음에 ranking을 매기는 방법이다.
- Entity-Mention models <br>
Coreference resolution을 할 때, feature가 필요한 경우가 있는데 주로 애매한 상황에 대해서 특정한 규칙을 만들어준다고 생각하면 된다.
예시로, ***John*** went with ***Jack*** to a movie, ***Joe*** went with ***him*** to a bar. 라는 문장이 있을 때 마지막 ***him*** 이
나타내는 사람이 누구인지 되게 애매하다. 이런 경우 recency effect를 사용해서 가까운 사람을 매칭시킨다.

### Neural Coreference 모델
딥러닝에서 coreference resolution이 어떻게 발전했는지 설명하기 시작한다. 적용된지 얼마되지 않은 분야라 역사가 짧다.
어떤 모델들을 주로 사용했는지 정도를 살펴보면 될 듯하다.
![Neural coreference model](https://whikwon.github.io/images/NLP_neural_coref_history.png)
<center> <i> &lt;Neural Coreference 모델 종류&gt;</i> </center> <br>
위 목록 중 Clark 이라는 분과 Manning 교수님이 함께 연구한 모델을 대표로 살펴보도록 하자. 해당 모델은 기존의 Mention Pair 모델에 강화 학습을 적용시켜서
성능을 끌어올린 모델이다. 이 모델을 보면서 neural net이 적용되지 않은 기존 모델들과 비교를 해보고, 어떤 문제가 있기에 강화 학습이 필요하게 되었는지도 살펴보도록 하자.

위 설명에서 나타나듯이 기본적인 Mention Pair 모델은 각각의 mention을 pair(corefence)로 했을 때 score를 계산하고 이를 바탕으로 pair 여부를 결정하는 방식이다.
아래 그림에 잘 나타나있다. ***antecedent 후보의 embedding, feature 값, mention의 embedding, feature 값, 추가적인 기타 feature 값*** 들을 입력하고
***feed forward neural net*** 을 거쳐서 ***score*** 가 계산되는 방식이다. 여기에서 기타 feature는 앞에서 잠깐 설명한 recency와 같은
문법적인 내용을 가리킨다.
![Neural mention pair](https://whikwon.github.io/images/NLP_neural_mention_pair.png) <br>
<center> <i> &lt;Neural Mention-Pair 모델 구조&gt;</i> </center> <br>
Neural net이 적용되기 전의 모델들과 비교를 해보면 다른 많은 분야에서 그렇듯 사람이 작업해야하는 feature 수가 현저하게 줄었음을 알 수 있다.
그리고 그럼에도 불구하고 여전히 13개나 hand-crafted feature를 사용되며 중요한 역할을 하고 있다.
![Neural mention pair features](https://whikwon.github.io/images/NLP_mention_pair_features.png) <br>
<center> <i> &lt;Neural Mention-Pair 모델 features&gt;</i> </center> <br>
이제 강화 학습이 적용된 배경을 살펴보자. 아래 예제는 Local decision이 잘못되었을 때 어떤 결과가 발생하는지 나타낸다.
위 예제의 경우 가운데의 ***Clinton*** 이 coreference를 이루게 되면 ***Bill Clinton*** 과 ***Hilary*** 를 가리키는 모든 단어들이
전부 같은 실재를 나타내어 ***Bill Clinton*** 과 ***Hilary*** 이 같아지는 엄청 나쁜 결과를 초래한다. 반면에 아래 예제를 보면 ***it*** 이 아무것도 나타내지 않기 때문에
***car*** 가 coreference해도 연쇄적으로 여러 단어들이 같은 실재를 나타내거나 하는 큰 영향력이 없기 때문에 위 예제에 비해선 괜찮은 편이다. <br>
에러마다 영향력이라는게 존재하므로 이 영향력의 가중치에 대한 학습 방법이 필요하고 이를 영향력에 대한 reward 적용을 생각할 수 있으므로
**강화 학습** 이 적용되었다. 여기까지가 적용 배경이다.   
![Neural mention pair challenge](https://whikwon.github.io/images/NLP_mention_pair_challenge.png) <br>
<center> <i> &lt;Neural Mention-Pair의 문제점&gt;</i> </center> <br>
그럼 이제 기준을 세워야 한다. 에러의 type은 False New, False Anaphoric, Wrong Link가 존재하는데 type에 따라서 치명적인 정도의 다름을 어느 정도는
나타낼 수 있다고 보았다.(*아래 그림*) 예전엔 에러에 대한 가중치에 대한 학습 방법이 없어서 보통 heuristic하게 search해서 사용했다. 하지만 이 방법은
너무 오래걸리고 데이터셋/언어/평가 방법이 바뀔 때마다 바뀌어야 한다는 큰 단점이 있다.
![Error type](https://whikwon.github.io/images/NLP_error_type.png) <br>
<center> <i> &lt;Coreference 에러 type&gt;</i> </center> <br>
![heuristic loss function](https://whikwon.github.io/images/NLP_heuristic_loss_function.png) <br>
<center> <i> &lt;Coreference heuristic cost function&gt;</i> </center> <br>
학습이 필요한 대상이 정해졌고 이제 강화 학습을 시킬 때 reward의 기준이 되는 값이 필요하다. reward의 기준 값은 $$B^3$$ evaluation metric 을 주로 사용한다.
아래는 학습 방법인데, action에 대해서 reward를 계산해야 하는데 가지수가 너무 많아서 sampling을 한 뒤에 expectation을 구하는 방법을 취한다.
![reinforce action](https://whikwon.github.io/images/NLP_reinforce_action.png) <br>
<center> <i> &lt;강화 학습 방법&gt;</i> </center> <br>
위 모델의 학습 결과를 heuristic loss와 비교하였고 성능이 나아지지 않고 조금 낮게 나옴을 확인했다. 문제를 확인해보니 score를 가장 높게 내는 action만 필요한데
expectation 해서 낮은 값들도 고려가 된다는 점이 있었다. 그래서 이 부분을 다른 방법으로 접근을 한다. 각각의 action이 독립적이므로 action을 바꿔주면서
reward와 regret을 확인하도록 하는 것이다.
![action1](https://whikwon.github.io/images/NLP_action1.png) <br>
![action2](https://whikwon.github.io/images/NLP_action2.png) <br>
<center> <i> &lt;강화 학습 action에 따른 reward 변화&gt;</i> </center> <br>
위 아이디어를 적용해서 Reward-Rescaling 방법으로 아래 식을 사용하여 학습을 진행한다. 이 경우에 heuristic loss function보다 성능이 조금 좋게
나왔다.
![reward rescaling](https://whikwon.github.io/images/NLP_reward_rescaling.png) <br>
<center> <i> &lt;Reward-Rescaling 설명&gt;</i> </center> <br>
모델의 학습이 잘 되었는지 결과물을 통해서 확인해보자. 아래 예제를 보면 기존의 모델에서 학습하기 어려운 단어 pair를 학습한 것을 볼 수 있다.
모델이 좀 더 general하게 embedding을 한다고 볼 수 있겠다.
![result example](https://whikwon.github.io/images/NLP_neural_scoring_example.png) <br>
<center> <i> &lt;학습 결과로 기존 대비 잘 나타내는 예시들&gt;</i> </center> <br>
다음은 치명적인 에러가 실제로 줄었는 지 확인해보자. 아래 결과를 보면 상대적으로 치명적이지 않은 False New, False Anaphoric은 수가 증가했지만
치명적인 Wrong Link는 수가 감소하여 치명적인 에러를 감소시켰음을 알 수 있다.
![error breakdown](https://whikwon.github.io/images/NLP_error_breakdown.png) <br>
<center> <i> &lt;모델의 에러 type 비율 확인&gt;</i> </center> <br>


Reference: <br>
[Stanford CS224n lecture15 slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture15.pdf) <br>
