---
title:  "CS224n_Lecture3"
date: 2017-08-07 00:00:00
comments: true
---

- CS224n Lecture3에 대한 내용을 정리하였다.

지난 시간에는 word2vec을 주로 소개했다. skip-gram과 negative sampling에 대해서 상세하게
학습했고 이 부분은 assignment1에서 코딩으로 구현할 예정이다.
이번 시간에는 GloVe라는 또 다른 모델과 word vector evaluation 방법을 배울 에정이다.

먼저 GloVe라는 모델이 왜 나오게 되었는 지 살펴보도록 하자.
window-based 모델인 skip-gram, CBOW가 등장하기 전에 주로 matrix factorization을 하는
count-based인 LSA, HAL 등이 많이 사용되었다. 이들 모델은 효율적으로 ***global statistical
information*** 을 나타낸다는 장점이 있지만 words간 유사성, 규칙을 모델 대비
잘 찾지 못한다는 단점이 존재한다.

window-based 모델의 경우엔 이와 정반대이다. words간 유사성, 규칙을 window-based 모델
대비 잘 학습하나 학습 과정이 word를 하나씩 보고 업데이트하므로
global statistical information을 효율적으로 나타내지 못한다는 단점이 존재한다.

이러한 두가지 장점을 모두 가진 모델을 찾다보니 GloVe라는 모델이 탄생하게 되었다.

그럼 먼저, GloVe를 소개하기 전에 SVD로 co-occurrence를 나타내는 것을 보고 넘어가겠다.
전체 document를 대상으로 혹은 window를 정해서 주변 word를 count해서 matrix로 나타낼 수 있겠다.
window로 예시를 들었을 때 co-occurence matrix를 아래처럼 나타낼 수 있다.

***
Window : 1개 <br>
Corpus : <br>
1. I like deep Learning
2. I like NLP
3. I enjoy flying <br>
위 정보로 주변 단어 개수를 세었을 때 아래 matrix와 같이 표현되고 이를 토대로 문장 내 단어끼리 관계를 조금은 알 수 있다.
![table](https://whikwon.github.io/images/Lecture3_table.PNG)

***
위와 같은 방법의 문제는 vocabulary 크기가 커지면 차원이 크게 증가한다는 데에 있다.
그래서 중요한 정보를 거의 남긴채로 차원을 줄이기 위해 SVD를 활용한다.
(*SVD에 대한 상세한 내용은 [여기](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/)를 참조하도록 하자.*)
결론적으로 SVD를 활용했을 때 어느 정도 syntactic, semantic patter를 확인할 수 있었다.
하지만 연산량의 문제랑($$O(mn_2)$$), hard to incorporate new words or documents,
different learning regime than other DL models 한 점들 때문에 사용하지 않는다고 한다.

Count-based와 Direct-prediction이라고 크게 나누어 설명하고 있다. 살펴보고 넘어가겠다. <br>

***
![table](https://whikwon.github.io/images/Lecture3_model_description.PNG)

Count-based 방법을 살펴보았으니 이제 GloVe 모델을 소개하도록 하겠다.
[논문](http://nlp.stanford.edu/pubs/glove.pdf)에서 먼저 유도 과정을 보인 후에 skip-gram 모델로부터 유도할 수 있음을 보이는데
Lecture note에서 뒤의 과정만 보여주므로 여기서도 그것만 언급하도록 하겠다.

먼저 Notation을 정리하자.
>
- $$X$$ : word-word co-occurrence matrix <br>
- $$X_{ij}$$ : number of times word occur in the context of word i <br>
- $$X_{i} = \sum_k X_{ik}$$ : the number of times any word k appears in the context of word i <br>
- $$P_{ij} = P(w_j\lvert w_i) = {X_{ij} \above 1pt X_i}$$ : the probability of j appearing in the context of word i <br>

Notation을 정의했으니 skip-gram 식으로부터 유도해보자. context word i일 때 j word가 나타날 확률은 아래와 같다.
>
$$Q_{ij} = {exp(\vec u_j^T \vec v_i) \above 0.5pt \sum_{w=1}^W exp(\vec u_j^T \vec v_i)}$$

그럼 cost function을 cross-entropy loss로 정의할 수 있다.
>
$$J = - \sum_{i \in corpus} \sum_{j \in context(i)} log\ Q_{ij}$$

위 식에서 똑같은 words i랑 j가 corpus에서 여러번 나올 수 있다고 하고 i, j를 같게 놓는다.
>
$$J = - \sum_{i=1}^W \sum_{j=1}^W X_{ij} log\ Q_{ij}$$

여기에서 $$Q$$를 normalize하는게 연산량이 많아 식을 변형하여 least square objective fuction을
만든다. (*이해 잘 안 됨.*)
>
$$\hat J= -\sum_{i=1}^W \sum_{j=1}^W X_{ij}{(\hat P_{ij} - \hat Q_{ij})}^2$$ <br>
$$(\hat P_{ij} = X_{ij},\ \hat Q_{ij} = exp(\vec u_j^T \vec v_i)$$

그리고 또, $$X_{ij}$$가 매우 큰 값을 가져서 optimize하기 어려워 log을 씌우고 minimize하는 전략으로 바꿔준다.
>
$$\hat J = \sum_{i=1}^W \sum_{j=1}^W X_i{({log(\hat P)}_{ij} - log(\hat Q_{ij})}^2
         = \sum_{i=1}^W \sum_{j=1}^W X_i{(\vec u_j^T \vec v_i - log\ X_{ij})}^2$$

마지막으로 빈도 수가 높은 단어에 의한 영향을 줄여주기 위해 general weighting function을 넣어주면
유도가 마무리된다.
>
$$\hat J = \sum_{i=1}^W \sum_{j=1}^W f(X_{ij}){(\vec u_j^T \vec v_i - log\ X_{ij})}^2$$

GloVe 모델을 소개한 [논문](http://nlp.stanford.edu/pubs/glove.pdf)을 보면 기존의 방법에 비해 성능이
좋다고 얘기하고 있는데 Levy의 [논문](http://www.aclweb.org/anthology/Q15-1016)에서 모델들 간에 Hyper-parameter를
제대로 조정하지 않아서 그런 것이라는 주장이 있어서 성능 관련해서는 이 후에 다시 정리하도록 하겠다.


그럼 이제 word vector evaluation에 대해 소개하도록 하겠다.
일반적으로 intrinsic/extrinsic 방법 두가지로 구분되어 사용된다.
lecture slide에 나온 내용을 넣도록 하겠다. <br>

***
Intrinsic :
- Evaluation on a specific/intermediate subtask
- Fast to compute
- Helps to understand that system
- Not clear if really helpful unless correlation to real task is established
Extrinsic :
- Evaluation on a real task
- Can take a long time to compute accuracy
- Unclear if the subsystem is the problem or its interaction or other subsystems
- If replacing exactly one subsystem with another improves accuracy → Winning!

***
예시로 이해를 하자면 intrinsic은 word2vec을 통한 성능 평가, extrinsic은 word2vec의 향상을
Machine Translation에 적용했을 때의 성능 평가 정도가 되겠다. intrinsic은 subtask이므로
평가가 원활하게 잘 되지만, extrinsic의 경우엔 다른 조건들을 통제한 상태에서 평가가 이루어져야 하므로
어렵고, 모델의 커서 시간도 오래 걸린다.

intrinsic evaluation에 대해서 소개하겠다.
먼저 word vector analogies로 단어들 간의 syntactic, semantic analogy를 확인하는 평가로
주로 cosine distance를 활용한다. 식은 아래와 같다. <br>
> $$d = argmax_i {(x_b - x_a + x_c)^T x_i} \above 0.5pt \| x_b-x_a+x_c\|$$ <br>

위 식으로부터 아래와 같은 semantic word vector analogies를 얻을 수 있다. <br>
![table2](https://whikwon.github.io/images/Lecture3_word_analogies.PNG)

이 외에도 동사 형태, 수도명 등 다양한 방법으로 syntactic, semantic analogies를 평가할 수 있다.
intrinsic evaluation 시에 모델을 어떤 ***hyper-parameter*** 로 학습시키냐에 따라서 결과가 크게 좌지우지된다.
hyper-parameter 종류와 평가 결과는 아래와 같다.

***
- Dimension of word vectors
- Corpus size
- Corpus souce/Type
- Context window size
- Context symmetry

***
![GloVe_eval1](https://whikwon.github.io/images/Lecture3_evaluation_result1.PNG)<br>
<Figure 3: Performance improving with data size> <br>
![GloVe_eval2](https://whikwon.github.io/images/Lecture3_evaluation_result2.PNG)<br>
<Figure 4: Accuracies vary with vector D and context windows size>

***
주로 성능은 모델의 종류에 크게 의존하며 corpus 크기가 커질 수록 성능이 좋아진다.
또한, word vector의 차원이 작으면 작을수록 words간 의미를 잡아내지 못해서 성능이 안 좋아진다.

추가적으로 correlation evaluation을 소개하고 있다. 기본 컨셉은 사람이 word간 similarity를 매기고
모델을 통해 구한 결과와 상관관계를 비교하는 것이다.
ambiguity를 어떻게 할 지에 대해서도 간략하게 나오는데 넘어가도록 하겠다.

extrinsic word vector evaluation은 뒤 쪽의 내용에서 설명한다고 하므로 다음 시간에 소개하도록 하고
정리하자면 Lecture3에서는 첫번째로 기존 모델들의 한계를 극복하기 위한 GloVe 모델에 대해 학습하였고
두번째로 intrinsic evaluation을 통한 모델 평가 방법, hyper-parameter 조정을 통한 모델 성능 향상
등에 대해서 학습하였다.


Reference: <br>
[Stanford CS224n lecture3 slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture3.pdf)
[Stanford CS224n lecture-note](http://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-notes3.pdf)
