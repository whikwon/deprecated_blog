---
title:  "CS224n_Lecture3"
date: 2017-08-07 00:00:00
comments: true
---

- CS224n Lecture3에 대한 내용을 정리하였다.

지난 시간에는 word2vec을 주로 소개했다. skip-gram과 negative sampling에 대해서 상세하게
학습했고 이 부분은 assignment1에서 코딩으로 구현할 예정이다.
이번 시간에는 GloVe라는 또 다른 모델과 word vector evaluation 방법을 배울 에정이다.

먼저 GloVe라는 모델이 왜 나오게 되었는 지 살펴보도록 하자.
window-based 모델인 skip-gram, CBOW가 등장하기 전에 주로 matrix factorization을 하는
count-based인 LSA, HAL 등이 많이 사용되었다. 이들 모델은 효율적으로 ***global statistical
information*** 을 나타낸다는 장점이 있지만 words간 유사성, 규칙을 모델 대비
잘 찾지 못한다는 단점이 존재한다.

window-based 모델의 경우엔 이와 정반대이다. words간 유사성, 규칙을 window-based 모델
대비 잘 학습하나 학습 과정이 word를 하나씩 보고 업데이트하므로
global statistical information을 효율적으로 나타내지 못한다는 단점이 존재한다.

이러한 두가지 장점을 모두 가진 모델을 찾다보니 GloVe라는 모델이 탄생하게 되었다.

그럼 먼저, GloVe를 소개하기 전에 SVD로 co-occurrence를 나타내는 것을 보고 넘어가겠다.
전체 document를 대상으로 혹은 window를 정해서 주변 word를 count해서 matrix로 나타낼 수 있겠다.
window로 예시를 들었을 때 co-occurence matrix를 아래처럼 나타낼 수 있다.
> Window : 1
Corpus :
I like deep Learning
I like NLP
I enjoy flying
![table](https://kymkh0902.github.io/images/Lecture3_table.PNG)

위와 같은 방법의 문제는 vocabulary 크기가 커지면 차원이 크게 증가한다는 데에 있다.
그래서 중요한 정보를 거의 남긴채로 차원을 줄이기 위해 SVD를 활용한다.
(*SVD에 대한 상세한 내용은 [여기](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/)를 참조하도록 하자.*)
결론적으로 SVD를 활용했을 때 어느 정도 syntactic, semantic patter를 확인할 수 있었다.
하지만 연산량의 문제랑($$O(mn_2)$$), hard to incorporate new words or documents,
different learning regime than other DL models 한 점들 때문에 사용하지 않는다고 한다.

Count-based와 Direct-prediction이라고 크게 나누어 설명하고 있다. 살펴보고 넘어가겠다.
![table](https://kymkh0902.github.io/images/Lecture3_model_description.PNG)

Count-based 방법을 살펴보았으니 이제 GloVe 모델을 소개하도록 하겠다.
[논문](http://nlp.stanford.edu/pubs/glove.pdf)에서 먼저 유도 과정을 보인 후에 skip-gram 모델로부터 유도할 수 있음을 보이는데
Lecture note에서 뒤의 과정만 보여주므로 여기서도 그것만 언급하도록 하겠다.

먼저 Notation을 정리하자.
>
- $$X$$ : word-word co-occurrence matrix <br>
- $$X_{ij}$$ : number of times word occur in the context of word i <br>
- $$X_{i} = \sum_k X_{ik}$$ : the number of times any word k appears in the context of word i <br>
- $$P_{ij} = P(w_j\lvert w_i) = {X_{ij} \above 1pt X_i}$$ : the probability of j appearing in the context of word i <br>

Notation을 정의했으니 skip-gram 식으로부터 유도해보자. context word i일 때 j word가 나타날 확률은 아래와 같다.
>
$$Q_{ij} = {exp(\vec u_j^T \vec v_i) \above 0.5pt \sum_{w=1}^W exp(\vec u_j^T \vec v_i)}$$

그럼 cost function을 cross-entropy loss로 정의할 수 있다.
>
$$J = - \sum_{i \in corpus} \sum_{j \in context(i)} log\ Q_{ij}$$

위 식에서 똑같은 words i랑 j가 corpus에서 여러번 나올 수 있다고 하고 i, j를 같게 놓는다.
>
$$J = - \sum_{i=1}^W \sum_{j=1}^W X_{ij} log\ Q_{ij}$$

여기에서 이해가 잘 안 되는데 $$Q$$를 normalize하는게 연산량이 많아 least square objective fuction을
만든다. 


Reference: <br>
[Stanford CS224n lecture3 slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture3.pdf)
