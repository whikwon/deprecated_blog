---
title:  "RL_Lecture8: Integrating Learning and Planning"
date: 2017-12-23 00:00:00
comments: true
---

- David Silver의 Reinforcement Learning 8강을 정리한 내용이다.

MDP에 대한 model을 학습한 뒤 실제 environment가 아닌 model으로부터 policy/value function을 학습하는 법을 배운다.
model로부터 학습하기 위해서는 environment와 같이 episode를 생성(simulation)할 수 있어야 한다. 이를 위한 simulation search를 배우고
생성한 episode에 대해서 기존 model-free RL의 evaluation 방법을 통해 학습하는 법을 배운다.

그리고 Dyna, Dyna-2를 통해서 real experience와 simulated experience를 함께 활용해서 좀 더 나은 성능을 내는 것을 살펴본다.

***
## Model-Based Reinforcement Learning
7강에서는 experience로부터 직접 policy를 배우는 법을 살펴보았고 그 이전 강의에서는 experience로부터 직접 value function을 배우는
법을 살펴보았다. 이 방법들을 따로 MDP에 대한 model이 없으므로 **Model-Free RL** 이라고 칭하고 policy/value function을 배우는 과정을 **learning** 이라고 한다.

이번 8강에서는 experience로부터 **model** 을 배우고 이를 이용해서 value function이나 policy를 배우는 법을 살펴본다.
이 방법은 MDP에 대한 model을 배우므로 **Model-based RL** 이라고 칭하고 policy/value function을 배우는 과정을 **planning** 이라고 한다.

그리고 learning과 planning의 개념을 합쳐서 하나의 구조로 만든 dyna를 살펴본다. 아래 그림에선 Model-based RL의 흐름을 나타냈다.

단순히 생각하면 우리가 환경으로부터 나오는 reward, action, state를 가지고 supervised learning을 해서 하나의 model을 만든 뒤 이걸 기존 환경을 대신해서
우리의 agent를 학습시키는 것이다. 장점으로는 agent가 환경으로부터 바로바로 피드백을 얻기가 어려운 경우(*비용, 시간이 많이 드는 경우*)에 이를 대신 활용하면
쉽게 시뮬레이션을 할 수 있다. 또, 환경에 대해서 model을 통해서 어느 정도 설명할 수 있다. 단점으로는 결국엔 우리는 어느 정도 model을 근사하기 때문에 error가 생길 수 있다는 점이다.

![model based RL](https://whikwon.github.io/images/david_silver/model_based_RL.png)

***
## Model Learning

### 1) Model?
model은 MDP의 $$\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}$$을 parameter $$\eta$$에 대해 나타낸 것이다.
그래서 특정 state($$S_t$$), action($$A_t$$)를 주면 $$S_{t+1}, R_{t+1}$$를 구할 수 있고 이 둘 사이는 독립적이라고 가정한다.
(*둘은 항상 같이 구해지기는 하나 서로에 대한 영향을 끼치는 요인이 없으니 독립적이다.*)

![model](https://whikwon.github.io/images/david_silver/model.png)

그리고 state, reward에 대해서 각각 supervised learning으로 학습할 수 있고 model, loss 선정은 원하는대로 해주면 된다. MDP가 간단한 경우에는
table lookup도 괜찮지만 크게 복잡해지면 neural network 같은 general approximator를 사용하는게 MDP를 잘 표현해줄 수 있다.

![model learniing](https://whikwon.github.io/images/david_silver/model_learning.png)

### 2) Table Lookup Model
model이 explicit MDP라고 가정하면 episode가 진행됨에 따라 state와 action pair에 대한 **방문 횟수** 를 기록한 뒤 이를 토대로
next_state, reward가 나올 확률을 계산한다. 이 확률을 토대로 sampling을 해서 planning을 할 수 있다.

또는, 매 방문에 대한 state, action, next_state, reward를 model에 저장한 뒤 state, action에 대해서 random sampling으로 뽑으면
next_state, reward를 얻을 수 있고 이를 토대로 planning을 할 수 있다.

![table lookup model](https://whikwon.github.io/images/david_silver/table_lookup_model.png)

***
## Planning with a Model
위에서 model을 세웠다고 가정하면 planning을 할 수 있다. 앞 강의에서 배운 value iteration, policy iteration 등등 다양한 방법을 사용할 수 있다.

### 1) Sample-Based Planning
지금까지 다뤘던 TD, MC, Sarsa 등의 model-free RL에서는 MDP로부터 experience를 sampling한 뒤 value function을 학습했다. 위에서 학습시킨 model도 동일한 방법으로 사용할 수 있다. model이
MDP를 대체한다고 생각하고 experience를 sampling한 뒤 똑같이 해주면 된다.

![sample based planning](https://whikwon.github.io/images/david_silver/sample_based_planning.png)

### 2) 예시: AB Example
앞 강의에서 봤던 예시이다. 실제 experience로부터 가운데 도식도와 같이 transition 확률을 고려해서 model을 만들 수 있고 얻은 확률로 부터
sampling을 하면 오른쪽과 같이 우리가 학습에 사용할 수 있는 experience를 얻을 수 있다. MC에 적용하게 되면 $$V(A) = 1, V(B) = 0.75$$를 얻을 수 있다.

![AB example](https://whikwon.github.io/images/david_silver/AB_example.png)

### 3) Planning with an Inaccurate Model
model이 제대로 학습되지 않으면 결국 성능은 좋게 나올 수가 없다. 이 때는 어렵지 않은 경우에 Model-free RL을 하고 어떤 점 때문에 그런 일이 발생하는지 확인하는 것이 좋다.

![inaccurate model](https://whikwon.github.io/images/david_silver/inaccurate_model.png)

***
## Integrated Architectures
Model-free RL과 Model-based RL 두 가지 개념을 위에서 설명했는데, 둘을 합치는 방법에 대해서 생각해 볼 수 있다. real experience와 simulated experience 모두를
가지고 학습하면 더 빨리 학습시킬 수 있지 않을까? 또, real experience는 근사된 값들이 아니므로 얻을 수 있다면 이를 활용하는 것이 학습에 좀 더 도움이 되지 않을까?

### 1) Dyna
아래 그림은 Dyna의 구조를 나타낸다. env로부터 얻은 experience로 2가지 행동을 수행한다. 첫번째는 직접 policy/value function을 학습시키고(**Model-free RL**)
두번째는 model을 학습해서 MDP를 근사시켜 나타낸 후 planning을 통해 policy/value function을 학습시킨다.(**Model-based RL**)

![dyna](https://whikwon.github.io/images/david_silver/dyna.png)

아래는 Dyna에서 action-value function을 사용한 Dyna-Q 알고리즘이다. (a)~(d)의 내용은 model-free Q-learning에 해당되는 내용이고 (e)~(f)는 model을
학습해서 model based Q-learning으로 planning하는 내용이다. 여기서 학습할 모델의 종류는 선택하면 된다. (*tabular, linear, neural network...*)

![dynaq](https://whikwon.github.io/images/david_silver/dynaq.png)

### 2) 예시: Simple maze
단순한 미로 탈출 예시에서 학습시킨 model의 planning step이 policy 수렴을 위한 필요 env step 수에 얼마나 영향을 미치는 지 살펴보자.
Dyna가 조금 더 효과적으로 작동하기 위해서는 우리가 학습시킨 model로부터의 planning이 policy/value function 학습에 큰 도움이 되도록 해서
env와의 상호작용을 최소한으로 줄여야 한다. 실제로 바깥 세상의 env로부터 feedback을 얻는데에는 돈과 시간이 많이 들기 때문에 이를 줄여야하기 때문이다.

아래 결과 그래프를 보면 planning step의 수를 늘릴 수록 빠르게 policy가 수렴하여 Dyna-Q가 매우 효과적이라는 것을 확인할 수 있다.

![simple maze](https://whikwon.github.io/images/david_silver/simple_maze.png)

그럼 미로를 약간 변경해서 optimal하게 학습된 policy가 잘 변하는지 살펴보자. 루트를 완전 틀어서 반대로 지나가도록 만들면 계속해서 기존 optimal policy대로
움직이려고 하기 때문에 학습하는데 오랜 시간이 걸린다. $$\epsilon$$-greedy하게 움직이므로 아주 낮은 확률로 도착점에 한 번 도달하면 그 후로는 학습이 수월하게 된다.

![dynaq inaccurate model](https://whikwon.github.io/images/david_silver/dynaq_inaccurate_model.png)

다음은 미로에 좀 더 효율적인 루트를 만들었을 때의 경우이다. 이 때, Dyna-Q는 좋은 루트가 새겼음에도 계속 가던 길을 가고 Dyna-Q+는 이를 알아차리고
가장 빠른 길로 policy를 변경한다. (**둘의 차이는?**)

![dynaq inaccurate model](https://whikwon.github.io/images/david_silver/dynaq_inaccurate_model2.png)

***
## Simulation-Based Search

### 1) Forward Search
현재 state로부터 **lookahead** 를 통해서 다음 state, action들을 고려한 뒤 best action을 선택하는 방법이다. 전체 MDP에 대해서 고려할 필요가 없고
여기에서의 MDP는 우리가 학습시킨 model로부터 얻을 수 있다.

### 2) Simulation-Based Search
위에서 본 forward search에서 조금 더 좁혀서 생각하면 현재 state로부터 시작되는 모든 MDP를 고려하려면 어려울테니 몇몇 episode만 sampling해서 만들어내는 방법을
생각해 볼 수 있다. 이 것이 simulation-based search로 개념은 model로부터 episode를 생성한 뒤에 이를 토대로 policy/value function을 학습하자는 것이다.

앞으로 살펴 볼 알고리즘들은 **simulation을 한 뒤 어떻게 evaluation을 할 지** 에 대한 내용을 다룬다.

![simulation based search](https://whikwon.github.io/images/david_silver/simulation_search.png)

### 2) Monte-Carlo Search
simulation policy는 고정되어 있다. 즉, 일정 확률로 episodes들이 생성된다고 보면 된다. 그리고 evaluation을 하는데 Model-free RL에서의 Monte-Carlo evaluation의
내용과 동일하게 진행된다.

![MC search](https://whikwon.github.io/images/david_silver/MC_search.png)

### 3) Monte-Carlo Tree Search
MCTS의 simulation policy는 MC와는 다르게 학습에 따라 변하게 된다. 즉, episode를 만드는 과정이 점차적으로 변한다는 것인데 어떤 식으로 진행되는지 살펴보자.
policy를 우리가 확인한 tree 내 유무에 따라서 적용될 policy를 tree/default로 나눈다. tree policy는 $$Q(S,A)$$를 최대로 하도록 설정하고 default는 random policy로 설정한다.

시행착오를 통해서 tree를 탐색할 것이다. 먼저 tree policy를 따라서 leaf 중에 **탐색 지점** 을 선정한다. terminal이 아닌 경우 탐색 지점으로부터 **default policy로 simulation** 을 한 뒤
탐색 지점에 대한 $$Q(S,A)$$를 구한다. 이 탐색 지점은 이제 tree 내에 편입된다.

이를 계속해서 반복하면 tree를 계속해서 탐색해나가면서 $$Q(S,A)$$를 최대로 하는 길 $$q_{\ast}(S,A)$$을 찾게 될 것이다.

MCTS는 방법은 MDP의 경우의 수가 아주 많은 경우(*바둑*)에 사용하기 좋다. 전체 MDP를 알기는 거의 불가능하고 sampling으로 episode를 뽑는데에도 한계가 있기 때문에
tree를 늘려가면서 접근하는 방식이 효과적이다.

![MCTS](https://whikwon.github.io/images/david_silver/MCTS2.png)

위 내용에서 evaluation은 위에서 본 MC evaluation과 같은 내용이다.

![MCTS](https://whikwon.github.io/images/david_silver/MCTS.png)

MCTS의 장점 아주 복잡한 문제를 효과적으로 접근할 수 있다는 것이다.

![MCTS](https://whikwon.github.io/images/david_silver/MCTS_advantage.png)

### 4) 예시: 바둑
바둑은 흰/검은 돌이 싸우는 게임으로 19x19, 13x13, 9x9 판에서 진행이 된다. 같은 색 돌로 둘러쌓게 되면 제거되고 많은 땅을 차지하면 이기는 게임이다.
reward는 승리/패배가 되고 value function은 현재 state에서의 이길 확률이 된다. 최종적으로 구할 optimal value function은 내가 이기고 상대가 지는 것을
최대로 하는 value function이 된다.

![go](https://whikwon.github.io/images/david_silver/go.png)

아래는 바둑을 Monte-Carlo evaluation으로 접근한 모습이다. 너무 많은 경우의 수가 생기기 때문에 이대로 진행하는 것이 사실상 불가능하다.

![mc go](https://whikwon.github.io/images/david_silver/MC_go.png)

MCTS로 접근한 경우는 아래 그림과 같이 진행된다.

![MCTS](https://whikwon.github.io/images/david_silver/MCTS3.png)
![MCTS](https://whikwon.github.io/images/david_silver/MCTS4.png)

### 5) Temporal-Difference Search
Model-free RL에서는 TD를 MC 대신 사용했을 때 얻는 여러가지 장점이 있었다.(아래 자료) 이를 simulation-based search에서도 똑같이 적용할 수 있다.

![MC vs TD search](https://whikwon.github.io/images/david_silver/MC_TD_search.png)

TD search는 model로부터 simulation을 해서 episode를 얻고 각 step에 대해서 bootstrapping으로 $$Q(S,A)$$를 update해주면 된다.

![TD search](https://whikwon.github.io/images/david_silver/TD_search.png)

### 6) Dyna-2
Dyna에서와 같이 simulation search로부터 얻은 simulated experience와 real experience를 함께 학습에 사용할 수 있다. (*자세한 내용은 강의에서 생략한다.*)

![dyna-2](https://whikwon.github.io/images/david_silver/dyna_2.png)

이 때, simulation 수도 줄어들고 성능도 좋아진다고 한다. 아래 예시 그래프를 보면 전체 space에 대해 TD Learning을 했을 때에는 실질적으로 학습이 거의 안 되는 것을 볼 수 있고
TD search는 MCTS보다 성능이 낫고 TD Learning + TD Search는 그 보다 더 좋은 것을 확인할 수 있다.

![dyna-2 example](https://whikwon.github.io/images/david_silver/dyna_2_example.png)

Reference: <br>
[David Silver RL Lecture8 slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf) <br>
