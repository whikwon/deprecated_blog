---
title:  "CS224n_Lecture2"
date: 2017-08-03 00:00:00
comments: true
---

- CS224n Lecture2에 대한 내용을 정리하였다.

기존의 word를 표현하는 방법에 대한 문제점과 이를 해결하기 위한 word2vec을 어떻게 하는지에 대한
내용을 주로 다루고 있다.

먼저, word를 중점적으로 다룰 것이므로 word가 표현하는 ***meaning*** 의 정의에 대해서 알아보겠다.
이 강의에서는 meaning을  ***denotation(표시)*** 이라고 정의하고 있다. 그러니까 word가 어떤 무언가에 대한
표시를 하는거라고 이해하면 될 듯하다.

***discrete representation*** 에 대한 내용이 나오는데, 단어의 의미에 대해서 딱 잘라서 *이거다* 라고 label을 달아줬을 때
뉘앙스라던지, 새로운 단어라던지, 주관적인 상황에 대해 처리하기가 어려워진다.
또, 그런 label을 달려면 돈도 많이 들고 시간도 많이 들고.. <br>

이렇게 기존에는 ***one-hot representation*** 으로 단어를 나타냈는데 이는 단어의 수가 많아서 dimension도 커지고
서로 간에 orthogonal하여 유사성을 나타내지 못해서 성능의 한계가 명확하게 존재했다.

그래서 이를 보완하기 위해서 discreted에서 ***distributed representation*** 으로 넘어간다.
그럼 어떻게 distributed하게 나타낼 수 있을까에 대해 고민했을 때 *특정 단어를 알고 싶을 때 그 주변 단어들을 통해
그에 대한 정보를 알 수 있겠다* 라는 생각에서 출발해서 ***distributional similarity*** 라는 개념이 등장한다.
이는 이 후에 배울 내용들의 근간이 되는 중요한 내용이다.

***distributional similarity*** 의 내용을 수식으로 써보자. center word $$w_t$$가 있고, context word를 예측하고 싶다고 하자.
확률을 $$p(context|w_t) = ...$$ 로 나타낼 수 있고 이에 대한 loss function은 $$J = 1-p(w_{-t}\lvert w_t)$$ 나타낼 수 있다.
loss function을 구했으니 target값으로 minimize하는 최적점을 찾으면 word를 vector로 나타낼 수 있게 된 것이다.

word2vec에 대해서 2가지 ***Skip-gram, CBOW*** 알고리즘을 배우게 될 것이고, training 방법으로는 ***hierarchical softmax, Negative sampling*** 을 배울
예정이다.

Skip-gram에서 사용하는 objective function, loss function, gradient 를 수식을 통해 설명한다.
- Objective function : $$J'(\theta) = \prod_{t=1}^T \prod_{-m \leqq j \leqq m(j\neq0)}p(w_{t+j}\lvert w_t;\theta)$$
- Loss function : $$J(\theta) = -{1 \above 1pt T}\sum_{t=1}^T \sum_{-m\leqq j \leqq m(j\neq 0)} log\ p(w_{t+j}\lvert w_t)$$
- Probabilities : $$p(o|c) = {exp(u_o^T) \above 1pt \sum_{w=1}^v exp(u_w^Tv_c)}$$
- Gradient of loss w.r.t center word : $${\sigma \above 1pt \sigma v_c} log(p(o|c)) = u_o - \sum_{x=1}^v p(x\lvert c)u_x$$
위 식에서 $$m : window size, \theta : parameter$$.
context word에 대한 gradient도 구해야되지만 나도 마찬가지로 일단 skip한다.
위 식을 통해서 결국 원하는건 $$v_c, u_o$$로 이루어진 $$W$$라는 matrix를 만드는 것이다.
![skipgram](https://kymkh0902.github.io/images/Lecture2_skipgram.PNG) <br>
$$W$$를 구한 뒤에 위 그림처럼 $$w_t$$라는 word가 들어왔을 때 $$W$$를 통과해서 특정한 vector를
갖게 되고 이 값을 모든 단어들의 vector와 dot product 연산을 통해 similarity를 비교한 뒤에
가장 가까운 값을 예측하는 게 가능해질 것이다.

학습 진행할 때에는 무조건 SGD를 사용한다. Batch는 느리기도 하고 SGD의 noise가 학습에 도움이 된다고 한다.

Reference: <br>
[Stanford CS224n lecture2 slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture2.pdf)
