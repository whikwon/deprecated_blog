---
title:  "CS294_lecture3"
date: 2017-10-26 00:00:00
comments: true
---

- Berkeley 대학의 2017년도 가을학기 강의인 CS294: Deep Reinforcement Learning 의 3강을 정리한 내용이다.

## 살펴볼 내용
1. Markov decision process의 정의
2. Reinforcement learning problem의 정의
3. 강화 학습 알고리즘의 해부
4. 전반적인 강화 학습 알고리즘 type에 대해 살펴보기
- 목표:
  - definition, notation 이해하기
  - 강화 학습의 목적에 대해 이해하기
  - 가능한 알고리즘에 대한 요약 내용 공부하기

## 강화 학습에 대해서
아래 그림을 보면서 강화학습의 기본적인 구조와 용어에 대해서 살펴보도록 하자. 호랑이라는 물체를 현재 state($$s_t$$)에서 observation($$o_t$$)하고
이를 바탕으로 action($$a_t$$)를 policy($$\pi_{\theta}(a_t|o_t)$$)한다. action($$a_t$$)은 다음 state($$s_{t+1}$$)에
영향을 미치게 되고 다시 현재 state($$s_{t+1}$$)로부터 observation($$o_{t+1}$$)을 하고... 반복된다.
이 때, Markov property가 적용되는데 이 뜻은 시간 $$t$$에서 $$t-1$$에 의한 영향을 제외한 이전 과거는 현재 상태에 아무런 영향을
주지 않는다는 것이다.
![RL_intros](https://whikwon.github.io/images/CS294_rl_intro.png) <br>
<center> <i> &lt;강화학습의 구조 및 기본적인 용어&gt;</i> </center> <br>

## Reward 함수
강화 학습 시에 각각 action에 대해 어떤 평가를 해주어야 잘 했는지, 혹은 못 했는지를 판단하고 올바른 방향으로 학습을 할 수 있다.
이런 각 action에 대한 평가를 위한 함수를 reward function이라고 하며 이를 다 더한 값을 나중에 목적 함수로 사용해서 학습을 하게 된다.
표기는 $$r(s, a)$$로 나타낸다.

## Markov Decision Process(MDP)
먼저 MDP가 기반이 되는 Markov chain부터 살펴보도록 하자.
Markov chain($$\mathcal{M}$$)는 state space($$\mathcal{S}$$)와 transition operator($$\mathcal{T}$$)로 이루어진다.
$$\mathcal{T}$$ 는 현재 state($$p(s_t)$$)에서 다음 상태로($$p(s_{t+1})$$) 전이 시켜주는 *operator* 역할을 하며 각 확률을
vector로 놓았을 때 $$\mathcal{T}$$을 곱해주면 된다. 아래 그래프 노드에서 볼 수 있듯이 $$s_3$$은 $$s_1$$와 independent한 성질을
갖는데 이를 ***Markov property*** 라고 일컫는다. <br><br>
![markov chain](https://whikwon.github.io/images/CS294_markov_chain.png) <br>
<center> <i> &lt;Markov chain 정의&gt;</i> </center> <br>

MDP로 넘어가면, 기존 항들 외에 action($$\mathcal{A}$$), reward($$\mathcal{r}$$)에 해당하는 항이 추가된다. 좀 더 일반화해서
partially observed MDP를 나타내면 추가적으로 observation($$\mathcal{O}$$), emission($$\mathcal{E}$$)에 해당되는 항들이 추가된다.
이런 MDP 성질이 어떻게 쓰이는 지는 뒤에서 살펴보도록 하자. <br><br>
![partially observed MDP](https://whikwon.github.io/images/CS294_partially_observed_MDP.png) <br>
<center> <i> &lt;partially observed MDP 정의&gt;</i> </center> <br>

## 강화 학습의 목표
강화 학습의 목표는 간단하다 각각의 state(*혹은 partially observed*)에서 policy를 통해 action을 했을 때
전체 reward가 큰 방향으로 학습하면 된다. 이 내용을 표현한 식이 첫번째 그림에 나타난 최적의 $$\theta^{\star}$$를 구하는 식이다.
아래 식에 대한 추가 설명을 덧붙이면 모든 state, action에 대한 확률을 앞으론 $$\pi_{\theta}(\tau)$$로 나타낼 것이고
이 때, 오른쪽 항에는 처음 state($$p(s_1)$$)을 주어지고 나머지는 markov chain을 통과한 형태의 식이다.
그래프와 노드로 표현 하면 2번째 그림처럼 나타낼 수 있고(*$$s_2$$에서 $$a_2$$로 갈 때, policy는 $$\pi_{\theta}(a_2|s_2)$$가 맞다.*)
식을 약간 변형하면 3번째 그림처럼 그래프를 나타낼 수 있다. 식을 변형해준 이유는 Markov chain의 stationary한 특징을 사용하기 위해서이다. <br><br>
![the goal of RL](https://whikwon.github.io/images/CS294_goal_of_RL.png) <br>
![the goal of RL2](https://whikwon.github.io/images/CS294_goal_of_RL2.png) <br>
![the goal of RL3](https://whikwon.github.io/images/CS294_goal_of_RL4.png) <br>
<center> <i> &lt;강화 학습의 목표&gt;</i> </center> <br>

$$\theta^{\star}$$를 나타 냄에 있어서 2가지 경우로 나눈다. finite/infinite horizon case가 각각이며 시간 $$t$$가 유한하다고 가정하면
finite, 무한하다고 가정하면 infinite가 되며 infinite일 때 위에서 얘기한 stationary한 성질을 이용할 수 있다. <br>

finite horizon에서는 $$\theta^{\star}$$의 기존 식에서 $$\tau ~ p_{\theta}(\tau)$$를 동일하지만 표기만 다른 $$(s_t, a_t) ~ p_{\theta}(s_t, a_t)$$
로 나타내어주고 $$\sum$$의 위치를 바꿔준다. 이러면 식이 state-action에 대해 marginal을 구하는 식이 된다. (*뜻 하는 바가 뭐지?*) <br><br>
![finite horizon](https://whikwon.github.io/images/CS294_finite_horizon.png)
<center> <i> &lt;finite horizon case&gt;</i> </center> <br>

infinite horizon에서는 $$T = \infty$$ 일 때를 다루며, 아래 그림에서와 같이 $$p(s_t, a_t)$$가 stationary한 분포로 도달하는 것을 보여준다.
(*어느 순간 t에 상관없이 state-action 분포가 일정해진다.*) 최종적으로 구한 $$\theta^{\star}$$에 대한 식은 finite와 다름을 알 수 있다. <br><br>
![infinite horizon](https://whikwon.github.io/images/CS294_infinite_horizon.png)
<center> <i> &lt;infinite horizon case&gt;</i> </center> <br>

위 식에서도 나타나 있지만 강화 학습에서는 **expectation** 에 대해서 항상 다룬다.
아래 그림에서 보면 reward 함수는 절벽으로 떨어질 확률 $$\psi$$에 대해서 discrete하다. 따라서 이 둘만 고려하게 되면 학습을 하기가 어렵다.
그래서 reward에 expectation을 해주고 이 값은 $$\psi$$에 *smooth(continuous)* 하게 바뀌어서 학습을 가능하게 해준다. <br><br>
![expectation](https://whikwon.github.io/images/CS294_expectation.png)
<center> <i> &lt;강화 학습에서의 expectation&gt;</i> </center> <br>

## 알고리즘의 흐름
강화 학습의 알고리즘은 아래와 같은 흐름을 가진다. 먼저, policy를 돌려서 sample을 생성하고, 모델을 fit하고 그 결과로
policy를 개선하고 다시 sample을 생성하고 반복되는 구조이다. 학습 방법은 Backpropagation을 사용하며 딥러닝에서의 내용과 같다. <br><br>
![algorithm flow](https://whikwon.github.io/images/CS294_RL_algorithm_flow.png)
근데, 이걸로는 부족한 점들이 있는데(*Only deterministic dynamics, policies, Only continuous states & actions, Very difficult optimization problem*)
나중에 다룰 예정이라고 한다.

## Q, V function
앞으로 많이 볼 것 같은 $$Q, V$$ function에 대해서 정의하자. 아래에서 보듯 $$Q$$ function은 $$s_t$$ 상태에서 $$a_t$$를 했을 때 $$t$$ 이 후 전체 reward를
나타내고 $$V$$ function은 $$s_t$$ 상태일 때 $$t$$ 이 후 전체 reward를 나타낸다. $$s_t$$가 $$s_1$$일 때, value function의 $$p(s_1)$$에 대한 expectation을
구하면 전체 reward를 의미하는 강화 학습의 목적 함수이다. <br><br>
![Q, V function](https://whikwon.github.io/images/CS294_Q_V_function.png)
<center> <i> &lt;Q, V function 정의&gt;</i> </center> <br>

$$Q, V$$ function을 사용해서 어떤 식으로 학습할 수 있을까? 두 가지 방법을 소개한다.
첫번째는 policy와 $$Q$$를 아는 경우 policy를 개선할 수 있다. $$Q$$를 최대로하는 $$a$$를 알 경우 어떤 policy가 되었든 $$s$$가 주어졌을 때,
해당 $$a$$를 내보낼 확률이 1이면 reward가 최대로 갖게 결정을 **잘** 하는 것이므로 현재 policy와 비슷하거나 좀 더 낫다고 볼 수 있다. <br>
두번째는 $$Q$$와 $$V$$의 비교를 통해서 $$Q>V$$인 경우 현재 action이 평균보다 낫다는 의미이니 gradient를 계산해서 policy를 개선할 수 있다.
***이 두 가지 개념이 강화 학습에서 매우 중요하다고 한다. 잘 알아두자.*** <br><br>
![using Q, V function](http://whikwon.github.io/images/CS294_using_qandv_functions.png)
<center> <i> &lt;Q, V function 사용 방법&gt;</i> </center> <br>

## 알고리즘 종류
모든 알고리즘은 $$\theta^{\star} = \underset \theta {argmax} \mathit{E}_{\tau \sim p_{\theta}(\tau)} \bigg[\displaystyle \sum_t r(s_t, a_t) \bigg]$$을 목적함수로 가지며
어떻게 학습을 시키냐에 따라서 나뉘어 진다. 위 알고리즘의 흐름을 나타낸 그림에서 초록색(model fit)과 파란색(policy improvement)부분이 모델마다 다른데 해당 내용은 간략하게 아래와 같다.
- Policy gradients: 직접 목적 함수를 미분($$\theta \leftarrow \theta + \alpha \nabla_{\theta} \mathit{E} \big [\sum_t r(s_t, a_t) \big]$$)해서 학습시킨다.
- Value-based: optimal policy(no explicit)로부터($$\phi(s) = {arg}{max}_a Q(s, a)$$) value 혹은 Q function을 추정한다. ($$fit\ V(s)\ or\ Q(s, a)$$)
- Actor-critic: value 혹은 Q function을 현재 policy로부터 추정하고 이를 이용해서 policy를 개선한다.
- Model-based RL: transition 모델을 추정($$learn\ p(s_{t+1}|s_t, a_t)$$)하고 planning(no explicit policy)에 사용하거나 policy를 개선하거나 기타 등등을 한다.
위 각각의 범주에 해당되는 알고리즘들은 아래와 같이 다양하게 존재한다. <br><br>
![algorithms](https://whikwon.github.io/images/CS294_various_algorithms.png)
<center> <i> &lt;강화 학습 알고리즘 종류&gt;</i> </center> <br>

## Tradeoffs
딥러닝에서 optimzer로 사용하는 방법 중에 SGD, Momentum, Adam 등이 있는데 이들을 조금씩 수식적인 내용이 다르긴 하지만 같은 역할을 한다.
그래서 성능 만을 고려해서 많이 Adam을 쓰는 편이다. 근데 이와 다르게 강화 학습에서는 각각의 알고리즘들을 선택했을 때의 tradeoff가 다양해서
좀 더 신중하게 택할 필요가 있어 보인다. tradeoff는 아래와 같이 다양하게 나타난다. <br><br>
![tradeoff](http://whikwon.github.io/images/CS294_RL_tradeoffs.png)
<center> <i> &lt;강화 학습 알고리즘의 tradeoffs&gt;</i> </center> <br>

- sample efficiency: 좋은 policy를 얻기 위해서 얼마나 sample이 필요한가? 에 대한 내용이다. 여기서 가장 중요한 내용은 *on/off policy 중 어느 것인가?* 이다.
  - Off policy: policy를 개선할 때 새로운 sample의 생성없이 계속 개선할 수 있다.
  - On policy: policy를 개선할 때마다(*조금이라도 변하면*) 새로운 sample을 생성해주어야 한다.
  아래 그림에서 알고리즘 별로 on에 가까운지 off policy에 가까운지 대략적으로 나타내고 각각에 가까운 정도를 efficient라는 단어로 나타내고 있다.
  여기서 헷갈리면 안 되는 건 efficient와 실제 wall clock time과 같지 않다는 점이다. sample을 생성하는 건 학습 단계에서 한 단계만 차지하며 off policy의
  경우 생성이 매우 빠른 편이다. 반대로 off policy인 model-based에서는 policy improve하는데 많은 시간이 걸린다. <br><br>
  ![sample efficiency](https://whikwon.github.io/images/CS294_sample_efficiency.png)
  <center> <i> &lt;강화 학습 알고리즘의 sample efficiency 비교&gt;</i> </center> <br>

- stability and ease of use : 모델이 얼마나 안정적이고 매 학습마다 제대로 수렴하는 지가 중요하다. supervised learning의 경우에는 gradient descent를 통해
거의 항상 학습이 가능하다. 하지만 강화 학습의 경우 종종 학습이 안 되는 경우가 있기 때문에 이런 점을 고려해야 한다. 아래는 알고리즘 별로 겪는 문제인데 뒤에서 배울 때 다시
보도록 하자.
  - Q-learning: fixed point iteration
  - Model-based RL: model is not optimized for expected reward
  - Policy gradient: *is* gradient descent, but also often the least efficient

- Assumptions: 알고리즘 별로 몇 가지 가정하는 사항들이 있다.
  - fully observability: state와 observation이 같다는 것으로 value function fitting method에서 가정한다
  - episodic learning: 끝이 있는 episode를 배운다는 것으로 pure policy gradient method, model-based RL method에서 주로 가정한다.
  - continuity or smoothness: 연속성에 대한 내용으로 continuous value function learning method, model-based RL method에서 가정한다.

Reference:
[Berkeley Univ. CS294 Lecture slides](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_3_rl_intro.pdf)
