---
title:  "CS224n_Lecture13"
date: 2017-09-14 00:00:00
comments: true
---

- CS224n Lecture13에 대한 내용을 정리하였다.

이번 강의에서는 NLP에 이미지 관련 딥러닝 분야에서 가장 많이 쓰이는 CNN 모델을 활용한 사례를 소개한다. <br>
RNN은 각각 단어 별로 vector가 주어지고 정보가 time step에 따라서 넘어간다. 그래서 phrase로 capture하기가 어려운 단점이 있다.
CNN은 receptive field를 가지고 일부 영역만 scan할 수 있으므로 RNN의 위와 같은 단점을 해결할 수 있다. 이 때, 언어학적인
내용을 몰라도 관계없으며 문법적인 내용들은 전부 무시된다. (*딥러닝의 큰 장점..*)

상세한 내용은 슬라이드에도 잘 나와있지만 거의다 [Yoon Kim et al. 2014](https://arxiv.org/pdf/1408.5882)을 기반으로 하고 있어서 바로 논문을 보도록 하자.  <br>

### 논문 - Convolutional Neural Networks for Sentence Classification <br>
논문에선 sentence classification을 위한 CNN 모델 학습을 목적으로 모델을 설계하였고 구조는 아래와 같다.
![structure](https://whikwon.github.io/images/NMT_CNN_structure.png)
$$x_i$$를 문장 내에 $$k$$차원인 $$i$$번째 단어라고하자. 각 문장은 $$n$$개의 단어로 이루어져있고 필요 시 padding으로 맞춰준다.
그럼 문장들을 단어들을 concatenate한 $$x_{1:n} = x_1 \oplus x_2 ... \oplus x_n$$ 으로 나타낼 수 있다.

이제 **filter** 를 정해보자. $$h$$의 window size를 가진다고 했을 때 convolution을 수행해야하니 filter는 $$w \in \mathbb{R}^{hk}$$
인 형태를 가져야 한다. filter를 통과했을 때 생성된 **feature map** 의 값들은 $$c_i = f(w \cdot x_{i:i+h-1} + b)$$로 정의할 수 있고
전체 feature map은 $$ c = [c_1, c_2, ..., c_{n_h+1}],\ c \in \mathbb{R}^{n-h+1}$$ 로 나타낼 수 있다.

위 구조에서 3번째에 pooling이 적용되는데 CNN에서의 pooling과 동일한 방법이며 feature map에 대해서 진행된다.
그렇게 pooling까지 통과시키면 feature $$\hat c = max\{c\}$$ 를 얻게 된다. 1개 channel을 가지고 있다고 생각하면
window당 하나의 feature밖에 얻지 못하므로 여러 개의 channel을 만들어주는 방법이 있고 이 논문에선
**multichannel 구조** 를 사용하였다. 이 외에 regularization을 위해서 **dropout** 을 사용하였다.

CNN의 구조를 거의 그대로 차용해서 복잡한 내용은 없고 생소한 **multichannel 구조** 에 대해서만 조금더 살펴보도록 하자.
학습을 위한 데이터 전처리를 할 때 단어들을 embedding을 해서 vector로 만들어줘야한다. 이 때, 여러가지 방법이 사용된다.
가장 간단하게 vector 길이만 정하고 random initialization 한 뒤에 학습시키는 방법이 있다.
그리고, 이미 pre-trained vector(*word2vec, glove*)로 initialization을 할 수도 있다. 이 경우에는 학습 도중에 vector를
fine-tuning 할 것인지 말 것인지 결정할 수 있다.

위 내용을 정리하면 multichannel 구조를 알 수 있다. 이름에서 알 수 있듯이 2개의 channel을 사용하였으며 pre-trained vector로 initialization
하되 한 channel은 fine-tuning을 하고 다른 channel은 하지 않는 방식이다.

논문에서는 multichannel 외에도 다양한 모델을 학습시켜서 비교했다. <br>
- CNN-rand : 모든 단어들을 random initialization함.
- CNN-static : *word2vec* 에서 가져온 pre-trained vector를 활용함. fine-tuning 하지 않음.
- CNN-non-static : CNN-static과 같은 방법에 fine-tuning 진행.
- CNN-multichannel : channel을 2개로, 양 쪽다 *word2vec* pre-trained vector로 initialized 되지만
한 쪽만 fine-tuning 진행.

학습 결과는 아래와 같다. pre-trained vector를 사용했을 때 성능이 좋게 나오는 것을 확인할 수 있고
fine-tuning 했을 때 성능이 조금 상승하고 multichannel 했을 때 조금 더 상승하는 것을 볼 수 있다.
![result](https://whikwon.github.io/images/NMT_CNN_result.png)

CNN의 특징을 NLP에 적용해서 좋은 성능을 낼 수 있다는 것을 보여주는 논문이며 channel 수를 더 늘려보고 다양한 기법들을
사용해서 더 좋은 성능을 낼 수 있을듯하다. 이 후에 나온 논문들을 살펴보도록 하자.


Reference: <br>
[Stanford CS224n lecture13 slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture13.pdf) <br>
Yoon Kim et al. 2014, [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882)
