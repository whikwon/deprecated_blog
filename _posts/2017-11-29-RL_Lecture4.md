---
title:  "RL_Lecture4: Model-Free Prediction"
date: 2017-11-30 00:00:00
comments: true
---

- David Silver의 Reinforcement Learning 4강을 정리한 내용이다.

이번 장은 MDP가 주어지지 않았을 때(*model-free*) 어떤 방식으로 문제를 해결할 지에 대해 다룬다. 먼저, Prediction과 Control 중 Prediction에 대해서만
다루며 해결 방법으로 Monte-Carlo와 Temporal-Difference Learning을 소개한다. Prediction이 policy evaluation에 관한 내용이다 보니 value function을
구하는 문제를 주로 다루며 그래서 어떤 성능을 평가할 때에도 RMS error를 통해서 평가한다. 이번 강의의 목적은 어떤 방법을 사용해서 value function을 정확하게 구할 수 있을까
에 대한 내용을 배우는 것이다.

## Monte-Carlo Reinforcement Learning
model-free인 MC method는 완전한 episode 경험으로부터 직접적으로 배우는 방식을 취한다. 여기서 완전하다는 것은 DP에서처럼 bootstrapping을 하지 않으며 **끝이 있는**
episode에 대해서만 적용이 가능하다는 뜻이다. Prediction 방법은 간단한데 `value == mean return` 으로 놓고 문제를 풀면 된다.

이를 기호로 나타내면 아래와 같고 value function을 구하기 위해서 *empirical mean* return을 *expected* return 대신에 사용한다는 점만 기억하면 된다.
value function을 구하는 것은 policy evaluation이라고 하는데 아래에서 2가지 policy evaluation 방법을 살펴보도록 하자.

![MC method](https://whikwon.github.io/images/david_silver/MC_method.png)

### 1) First-Visit MC Policy Evaluation
이름에 나타난 것처럼 episode에서 처음 방문하는 state에 대해서만 고려한다. episode를 돌다보면 한 state에 여러 번 방문할 수도 있는데 이는 제외하고
**처음 한 번** 만 고려해서 총 return을 구하고 이에 대한 *empirical mean* 을 구해서 value를 구하면 된다. 계속 반복할 경우 대수의 법칙에 의해서
$$v_{\pi}(s)$$로 수렴하게 된다.

![first-visit MC](https://whikwon.github.io/images/david_silver/first_visit_MC.png)

### 2) Every-Visit MC Policy Evaluation
위 방법과 차이는 매 episode를 돌 때 중복되게 방문하는 state가 있을 수도 있는데 every-visit은 전부 다 고려한다는 점이 차이이다.
episode 처음부터 끝까지 모든 step에 대한 총 return을 구하고 *empirical mean* 을 구해서 value를 구하면 된다. 마찬가지로 대수의 법칙에 의해서
$$v_{\pi}(s)$$로 수렴하게 된다.
![first-visit MC](https://whikwon.github.io/images/david_silver/every_visit_MC.png)

### 3) 예시: Blackjack
blackjack으로 우리가 MDP가 주어지지 않았을 때 MC 방법으로 value function을 구할 수 있을지 확인해본다. blackjack은 dealer와 누가 숫자 21에 가깝게 카드를 갖는지  
대결하는 게임이다. action으로 받거나(twist), 말거나(stick)을 선택할 수 있고 state는 현재 카드 합, dealer의 카드, Ace 사용여부로 구성된다. reward는
숫자가 dealer보다 높을 경우 +1, 같을 경우 0, 낮을 경우 -1이다.

우리의 policy는 20보다 높으면 그대로, 그렇지 않을 경우 계속 받는 전략을 취할 것이고 이 때, MC로 구한 value function은 아래와 같다.
당연히 policy가 20이면 이길 확률이 매우 높으니 player sum 20 이후에 1에 가까워진다. 하지만, 20 보다 작은 경우에는 무조건 받기 때문에 player의 선택으로
이길 확률은 0이다. 이 외에 dealer가 너무 운이 없어서 지거나 하는 경우를 고려하면 약간 울퉁불퉁한 것을 볼 수 있다. 직관적으로 생각했을 때의 결과를 MC를 통해 구한
값이 잘 나타내는 것을 볼 수 있다.

![first-visit MC](https://whikwon.github.io/images/david_silver/blackjack.png)

### 4) Incremental MC update
*empirical mean* 을 구하는 것을 incremental하게 update하는 형식으로 아래 식처럼 바꿔서 쓸 수 있다.

![incremental mean](https://whikwon.github.io/images/david_silver/incremental_mean.png)

MC method에도 이를 적용해서 아래 식과 같이 나타낼 수 있다. 여기서 $$G_t - V(S_t)$$를 error항으로 볼 수 있고 우리는 $$V(S_t)$$를 구하기 위해서
error만큼 계속 update해주는 것으로 생각할 수 있다. 그리고 시간에 따라 환경이 변하는 non-stationary 문제의 경우에는 $$\dfrac 1 {N(S_t)}$$항을 $$\alpha$$
로 사용해도 된다. $$\alpha$$를 사용할 경우에 이동 평균을 계산하는 것이라고 볼 수 있고 이 때, 이전 시간에 관련된 값들은 사라지게 되지만 어차피 현재에 영향을 거의 안
미치므로 상관없다.

![incremental MC](https://whikwon.github.io/images/david_silver/incremental_MC.png)

## Temporal-Difference Learning
model-free인 TD method는 episode의 경험으로부터 직접적으로 배우는 방식을 취한다. 여기서 MC와는 다르게 episode가 완전하지 않아도 되며 DP에서처럼
*bootstrapping* 을 사용한다. episode 전체를 보고 value function을 구하는 것이 아닌 일부만을 보고 일부를 추측하는 형태로 진행되기 때문에 *guess* 를
통해서 계속 update한다고 보면 된다.

### 1) MC vs TD
TD의 특징을 살펴보기 위해서 MC와 비교해보자. 마지막에 incremental update를 살펴봤는데 TD도 비슷하게 나타낼 수 있다. 대신
MC에서 $$G_t$$(*actual return*)의 값을 $$R_{t+1} + \gamma V(S_{t+1})$$(*estimated return*)으로 근사해서 나타낸다.
어떤 부분을 근사해서 나타낸 것인지 근사로 인해서 어떤 특징이 생기는 지는 뒤에서 살펴보도록 하자.

![MC vs TD](https://whikwon.github.io/images/david_silver/MC_vs_TD.png)

### 2) 예시: Driving Home
MC와 TD의 차이를 설명하기 위해 넣은 자료이다. 간단하게 구간 별로 도착까지 예상시간을 예측하는데 구간 별로 변화가 생길 때마다 예상 값들이 조정된다.
state는 도로 상황으로 볼 수 있고 reward는 움직일 때 시간 별로 -1, 도착했을 때 +1로 볼 수 있다. 그리고 action은 그냥 policy evaluation을 위해서
따로 지정해놓고 봤다고 생각하면 된다. 결과를 보면 MC는 전체 episode를 본 결과를 토대로 update가 이루어지지만 TD는 매 step 별로 다음 step에 대해 고려해서
update가 이루어지는 것을 확인할 수 있다.

다른 예시를 들면 길을 가다가 절벽에 떨어질 뻔한 상황이 있다고 가정해보자. 이 경우 *떨어질 뻔* 했으므로 전체 episode를 봤을 때 크게 문제가 없으므로 MC는 크게 고려하지 않는데
TD는 다음 step에 *떨어질 뻔* 한 것이므로 MC보다 더 크게 고려된다.

![driving home](https://whikwon.github.io/images/david_silver/driving_home.png)

### 3) Bias/Variance trade-off
TD의 $$R_{t+1} + \gamma V(S_{t+1})$$항이 어떤 부분을 근사했는지 보고 이로 인해 갖는 특징이 무엇인지 살펴보자. Bellman Equation에 의해서
$$G_t = R_{t+1} + \gamma v_{\pi}(S_{t+1})$$이 성립한다. 즉 모든 policy에 대해서 성립하는 식인데 TD에서 우리가 $$v_{\pi}(S_t)$$를 알 길이 없다.
그래서 모든 policy가 아닌, 특정 policy에서의 value function $$V(S_{t+1})$$로 근사한다. 이 경우에 참 값이 아니므로 *biased* 하게 되지만
모든 policy에 대해 고려해도 되지 않아서 variance는 낮아지게 된다. (*수렴 여부가 확실하다면 variance를 낮추기 위해서 bias를 증가시키는 것도 괜찮은 방법이 되겠다.*)

![bias-variance tradeoff](https://whikwon.github.io/images/david_silver/bias_variance_tradeoff.png)

### 4) MC vs TD(2)
bias, variance 내용을 더해서 종합적으로 MC와 TD를 비교해보자. MC의 장점은 수렴이 언제나 보장된다는 점, bias가 없고 단순하다는 점을 들 수 있고 단점은 variance가 높다는 점이다.
TD의 장점은 variance가 낮고 MC보다 효율적이라는 점이고 단점은 수렴에 대한 보장이 function approximation에 대해서는 되지 않는다는 점과 초기 값에 민감하다는 점이다.

![MC vs TD2](https://whikwon.github.io/images/david_silver/MC_vs_TD2.png)

### 5) 에시: Random Walk
총 5개의 node가 있고 좌/우로 random하게 움직일 수 있다. 이 때의 각 node에서의 value를 MC와 TD로 구해보자.
action은 좌/우 움직임이고 state는 현재 위치, reward는 우측 끝은 +1 좌측 끝은 0이다. 직관적으로 생각해보면 $$E \rightarrow A$$로의 순서로 value 값
(true value: $$\dfrac 5 6, \dfrac 4 6, \dfrac 3 6, \dfrac 2 6, \dfrac 1 6$$)을 가질 것을 생각해볼 수 있다.
MC와 TD로 구한 value와 true value와의 비교를 그래프로 나타내면 아래와 같은데 episode 수 내내 거의 항상 좋은 것을 확인할 수 있다.

![random walk](https://whikwon.github.io/images/david_silver/random_walk.png)
![MC vs TD3](https://whikwon.github.io/images/david_silver/MC_vs_TD3.png)

### 6) Batch MC and TD
MC와 TD는 모두 경험이 무한할 때 value function은 $$V(s) \rightarrow v_{\pi}(s)$$로 수렴이 보장된다. 그럼, 무한하지 않은 상황에서는
MC와 TD가 어떻게 작동할 지에 한 번 아래 예시를 보면서 살펴보도록 하자.

총 2개의 state $$A, B$$가 있고 discounting이 없으며 reward는 0과 1 뿐이며 8개의 episode로 모든 경험이 끝난다. 이 때, MC, TD는 어떻게 학습하며 얻은
$$V(A)$$는 무엇이 될까? 아마 직관적으로 생각해보면 $$A$$가 0밖에 없으니 0이라고 생각할 수도 있고 $$A$$와 $$B$$의 관계를 생각한다면
둘의 값이 같다고 본다면 $$B$$가 1일 때 $$A$$도 1일 것임을 짐작해볼 수도 있다. 이런 두 가지 방식으로 생각하는게 MC와 TD가 다른데 그래서 $$V(A)$$를
MC는 0으로, TD는 0.75로 학습하게 된다. 이에 대한 자세한 설명은 certainty equivalence를 보도록 하자.

![a b example](https://whikwon.github.io/images/david_silver/A_B_example.png)

### 7) Certainty Equivalence
위 예제에 대해서 MC의 경우 전체 episode에 대해서 mean-squared error로 계산하므로 $$V(A) = 0$$의 결과를 얻게 된다.
반면, TD는 현재 state에 다음 state를 고려하는 방식으로 MDP를 푼다고 볼 수 있다. 그래서 Markov model의 max likelihood를 구하는 문제로
풀어서 $$V(A) = 0.75$$의 결과를 얻게 된다.

이 결과를 통해서 MC는 Markov property를 이용하지 않기 때문에 non-Markov environment에서 더 효과적이고
TD는 Markov property를 이용하기 때문에 Markov environment에서 더 효과적이다. (*Markov property는 미래 상태가 현재 상태에 의해서만 조건부로 결정된다는 것을 의미한다.*)

![certainty equivalence](https://whikwon.github.io/images/david_silver/certainty_equivalence.png)

## Unified view

### 1) Backup
backup은 어떤 state의 value를 미래의 state의 value로 update하는 것을 의미한다. MC, TD, DP를 차례로 살펴보자.
먼저 MC backup이다. sampling으로 1개 trajectory의 episode를 끝까지 살펴본 뒤에 return 값과 value의 차이(error)에 비례해서 update하는 방법이다.

![MC backup](https://whikwon.github.io/images/david_silver/MC_backup.png)

다음은 TD backup이다. sampling으로 뽑아서 one-step lookahead로 다음 state에 대해서만 살펴본다.
그리고 TD error($$R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$$)에 비례해서 update하는 방법이다.

![TD backup](https://whikwon.github.io/images/david_silver/TD_backup.png)

마지막으로 DP backup이다. sampling을 하지 않고 one-step lookahead로 다음 state 전체에 대해서 살펴본 뒤
reward와 value function의 expection의 값으로 update한다.

![DP backup](https://whikwon.github.io/images/david_silver/DP_backup.png)

### 2) Bootstrapping and sampling
- Bootstrapping: state의 value의 estimate를 다음 state의 value의 estimate로 update하는 것을 ***bootstrapping*** 이라고 한다.
MC는 bootstrapping을 하지 않고 DP와 TD는 bootstrapping을 한다.
- Sampling: 전체 trajectory을 다 고려하느냐 아니면 sampling에 대해서만 고려하느냐의 차이인데 MC, TD는 sampling을 하며 DP는 전체를 고려한 expectation 값으로 update한다.

### 3) Unified view of RL
위 내용들을 종합해서 2차원 상에 모델들을 나타내면 아래 그림과 같다. x축은 bootstrapping의 정도, y축은 sampling의 정도로 표현된다.

![unified view](https://whikwon.github.io/images/david_silver/unified_view.png)

## TD($\lambda$)

### 1) n-Step TD
TD target이 n-step 미래까지 고려한다고 생각해보자. 1개 step만 고려할 때 지금까지 배운 TD(0)가 될 것이며 끝이 있을 때
전체를 고려한 경우 MC가 될 것이다. n-step TD를 수식으로 나타내면 아래와 같다.
지금까지 배운 것은 TD가 MC보다 성능이 보통 낫다고 배웠는데 그럼 n-step으로 일반화해서 생각했을 때는 n의 값이 성능에 어떤 영향을 미칠까?

![n-step TD](https://whikwon.github.io/images/david_silver/n_step_TD.png)

### 2) 예시: Large Random Walk
아래는 n-step에 따른 성능 RMS error를 비교한 결과이다. on-line, off-line에 대해서 봤을 때 가장 성능이 좋은 n을 고를 수 있다.
어떤 문제이느냐에 따라서 다르고 on/off-policy의 여부에 따라서도 다르게 구해질 것이다. 그리고 이 때, 여러 n에 대해서 한꺼번에 생각해보는
방법도 고려해볼 수 있다. 아래에서 보자.

![large random walk](https://whikwon.github.io/images/david_silver/large_random_walk.png)

### 3) Averaging n-Step Returns
n-step의 return 여러 개를 한꺼번에 고려하면 성능을 더 좋게 만들 수 있을까 라는 생각에서 접근한 방법이다. 실제로 가능하며
전체를 한꺼번에 고려하고 step에 따른 가중치($$\lambda$$)를 주고 모든 return을 더한 뒤 *target* 으로 사용하는 방식이다.
수식적인 내용은 step이 커질 수록 더 작게 만들고 가중치들의 총 합을 1로 둔다.

![TD lambda vs n-step TD](https://whikwon.github.io/images/david_silver/TD_lambda2.png)

시간에 따른 가중치를 2차원 그래프로 나타내면 아래와 같다. 시간이 지남에 따라서 가중치가 점점 줄어들고 총 면적은 1인 것을 알 수 있다.
그리고 $$\lambda$$의 값이 0일 때와 1일 때 어떤 값을 갖는지 확인해보면 끝이 있다고 가정했을 때
$$G_t^{\lambda} = (1 - \lambda) \displaystyle \sum_{n=1}^{T-t-1} \lambda^{n-1}G_{t:t+n} + \lambda^{T-t-1}G_t$$ 로 나타낼 수 있다.
$$\lambda = 0$$일 때, 모든 값이 0이 되고 $$G_{t:t+1}$$ 항만 살아남기 때문에 TD가 되고 $$\lambda = 1$$일 때는 $$G_t$$만 남기 때문에 MC가 되는 것을
알 수 있다.

![TD lambda vs n-step TD](https://whikwon.github.io/images/david_silver/TD_lambda2.png)

이렇게 n-step return을 averaging해서 update하는 것을 TD($$\lambda$$)의 *forward view* 라고도 하는데 현재의 상태에서
미래의 상태에 대해서 본 뒤에 update를 하기 때문에 그렇다. 실제로 성능 면에서 그냥 n-step을 했을 때보다 더 좋은 것을 아래 그림을 통해서 확인할 수 있다.
그렇지만 *forward* 의 한계는 결국 미래에 대한 내용을 전부 봐야 현재를 update할 수 있다는 점인데 이는 MC가 그렇듯 비효율적인 면을 가지고 있다.
그래서 *backward view* 가 나오게 된다.

![TD lambda vs n-step TD](https://whikwon.github.io/images/david_silver/TD_lambda.png)

### 4) Eligibility Traces(Backward View)
Eligibility trace의 직관을 이해하기 위해서 아래 그림을 보자. 벨이 3번 울리고 불이 들어온 뒤에 전기에 감전되었다고 하면 어떤 영향(규칙)에 의한 것일까?
2가지로 크게 생각해 볼 수 있다. 먼저, 벨이 많이 울린 뒤에 전기가 찾아오거나 (*frequency heuristic*) 전구가 들어온 뒤에 바로 전기가 찾아온다고
추측할 수 있다. (*recency heuristic*) 이처럼, 현재 state의 value를 update할 때 영향을 준 과거 state ***빈도의 정도와 그 영향이 얼마나 최근 일인지를 고려*** 하는 것을
**Eligibility trace** 라고 한다. 방법은 각 state에 대한 eligibility trace table을 따로 만들어서 해당 state에 영향을 미쳤던 정보들을 기록하고 update할 때
사용하는 것이다. 여기에 추가적으로 시간에 따라 감소하도록 가중치를 준다. ($$\lambda$$)

![eligibility trace](https://whikwon.github.io/images/david_silver/eligibility_trace.png)

이런 eligibility trace는 *backward view* 라고 볼 수 있는데 과거의 정보를 활용해서 현재를 어떻게 update할 지를 결정하기 때문이다.  
value function을 update할 때 TD-error 방향으로 eligibility trace를 고려해서 update해주면 된다.

![eligibility trace](https://whikwon.github.io/images/david_silver/eligibility_trace2.png)

*forward view* 에서 생각했던 것처럼 $$\lambda$$가 0, 1일 때 어떻게 될 지 살펴보도록 하자. 먼저, $$\lambda = 0$$일 때,
$$E_t(s) = \mathbf{1}(S_t = s)$$ 이므로 현재 상태에 대해서만 고려하게 되므로 정확하게 **TD(0)** 와 같게 된다.

$$\lambda = 1$$일 때를 생각해보자. $$E_t(s) = \gamma \lambda E_{t-1}(s) + \mathbf{1}(S_t = s)$$ 이므로
모든 이전 state의 eligibility trace를 가중치 없이 고려하게 된다. 이 뜻은 state를 update하기 위해서 모든 이전 state를 시간에 관계 없이
다 고려할 것이라는 것으로 볼 수 있다. (*물론 TD-error에 $$\gamma$$가 가중치로 있어서 실질적은 영향력 자체는 점점 줄어든다. 지금은 부가적으로 update하기 위해서 이전 정보를
얼마나 이용해야 하는지에 대해서 검토 중이다.*) 이를 **TD(1)** 이라고 하며 이전에 본 *every-step MC method* 와 아주 유사한 내용으로 볼 수 있다. 하지만 조금 일반화된 버전으로 볼 수 있는데
update가 terminal state에서만 이루어지지 않고 끝나지 않는 episode에서도 사용될 수 있기 때문이다.(*terminal state에서만 update될 경우 every-step MC와 완전히 같다.*)

아래는 forward(우)와 backward(좌)의 성능을 비교해서 나타낸 그래프이다. 보다시피 동일한 $$\lambda$$ 값을 갖는 경우에 최적의 $$\alpha$$값은 약간 차이가 나지만 같은 성능을 내는 것을
확인할 수 있다. 하지만 $$\alpha$$가 조금 변했을 때 backward의 경우에는 성능이 더 많이 변해서 forward에 비해서 unstable하다는 것을 알 수 있다.

forward는 전체 episode를 다 봐야 update가 가능하므로 비효율적인데 backward는 과거의 정보를 활용해서 그런 문제를 없앴다. 근데 성능 면에서는 유사하게 나오므로 backward를 사용하면 되겠다.

![forward backward summary2](https://whikwon.github.io/images/david_silver/forward_backward2.png)

forward와 backward의 비교 내용을 정리하면 아래 표로 나타낼 수 있다. (*episode의 끝에서 update했다고 가정하고 비교한 내용이다.*)

![forward backward summary](https://whikwon.github.io/images/david_silver/forward_backward.png)


Reference:
[David Silver RL Lecture4 slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf)
Sutton & Barto. (2017) Reinforcement Learning: An Introduction (pp. 241, 245)
