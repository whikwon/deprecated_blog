---
title:  "CS224n_Lecture9"
date: 2017-08-25 00:00:00
comments: true
---

- CS224n Lecture9에 대한 내용을 정리하였다.

이번 장에서 주로 소개하는 내용은 크게 두가지이다. 먼저, 기존 Machine Translation(MT)에 대한 내용과 딥러능의 적용을 다루고
후에 NLP에서 가장 많이 사용되는 **GRU와 LSTM** 을 소개한다.

## 기계 번역에의 딥러닝 활용

먼저, 기존 Machine Translation 시스템을 살펴보고 왜 딥러닝이 필요한지 보도록 하자. <br>
![Statistical MT system](https://whikwon.github.io/images/MT_system.PNG)
위 그림을 보면 **Translation Model, Language model, Decoder** 가 각각 나눠져 있는 것을 볼 수 있다.
프랑스어가 들어왔을 때 영어로의 번역이 진행($$p(f|e)$$)되고 그게 문법적으로 잘 맞는지 확인하는 작업($$p(e)$$)을 거친다.
그리고 이 둘을 종합해서 Decode를 하면 적합하다고 판단한 결과물이 나오게 된다.($$argmax\ p(f|e)p(e)$$)

이렇듯, MT를 위해서 세 모델을 학습시켜줘야 하는데 각 언어들의 세세한 부분들을 모두 고려해주어야 하고
주로 사람이 feature engineering해서 매우 복잡한 시스템을 갖추고 있다.

그래서 MT에서도 딥러닝을 활용하게 되었고 뒤에서 다시 볼 예정이지만 매우 훌륭한 결과를 내놓고 있다.
딥러닝의 장점으로는 end-to-end training이 가능해서 위와 같은 복잡한 작업을 할 필요도 없고 단순하다.

그럼 어떤 방식으로 딥러닝에서 MT가 사용되는지 기본적인 구조를 살펴보자.
Encoder : $$h_t = \phi(h_{t-1},x_t)  = f(W^{(hh)}h_{t-1} + W^{(hx)}x_t)$$
Decoder : $$h_t = \phi(h_{t-1}) = f(W^{(hh)}h_{t-1})\\ y_t = softmax(W^{(S)}h_t)$$
기존 MT 시스템과 유사하게 Encoder, Decoder로 이루어지며 아주 단순한 모델의 경우엔 Encoder의 마지막 hidden state에
sentence에 해당되는 모든 정보를 담고 이를 Decoding하는 방식을 취한다. 이는 정보를 전달하는데 한계가 존재해서 여러가지
개선 concept가 등장한다.  <br>
아래 그림은 아주 기본적인 encoder-decoder 구조이다. 모든 정보를 encoder의 마지막 hidden state에서 받아서 decoder의
첫번째 hidden state에
![encoder-decoder_basic](https://whikwon.github.io/images/basic.PNG)

다음으로 소개할 모델은 위 encoder-decoder를 살짝 변형한 모델이다.
기존에 $$h_t= \phi(h_{t-1})= f(W{(hh)}h_{t-1})$$ 식에서는 encoder의 마지막 hidden state값을
decoder의 첫번째 hidden state에만 전달했다면 decoder 전체에 전달하는 형태로 바꾸었다. <br>
또한, decoding되어 나온 결과값($$y$$)이 다음 결과, hidden state에 전달되도록 하였다. <br>
이를 수식으로 나타내면 $$h_{D,t} = \phi_D(h_{t-1}, c, y_{t-1})$$가 된다. (*여기서 c는 encoder의 마지막 hidden vector*)
이렇게 모델을 구상했을 때 decoding시 encoding된 값이 고려되고 decoding된 단어들끼리 서로
상호작용을 하기 때문에 단어가 반복되는 것을 막을 수 있다는 장점이 있다.
![encoder-decoder_advanced](https://whikwon.github.io/images/encoder-decoder.PNG)

이 외에도 Stacked/Deep RNN 구조, 문장의 앞 뒤 영향을 모두 고려해주기 위한 bidirectional encoder, reverse order training이 사용된다.

마지막으로 기본 RNN 구조가 아닌 더 복잡한 unit을 사용하는게 성능 향상에 가장 효과적이라고 언급하고 있다.
기본 RNN의 경우 vanishing/exploding gradient의 문제로 문장이 길어지면 앞의 내용이 뒤에 영향을 끼치지 못하는 큰 단점이 존재해서
이를 보완하고자 Long-term dependency memory를 가진 모델을 고안하게 되었다.
대표적인 모델이 GRU, LSTM 이며 이를 중점적으로 살펴보도록 하겠다.

***
## GRU

GRU는 2014년에 조경현님이 발표하신 모델이다. 어떤 구조로 이루어져 있는지 수식을 통해 이해하도록 하자.
$$Update\ gate : z_t = \sigma(W^{(z)}x_t + U^{(z)}h_{t-1}) \\
Reset\ gate : r_t = \sigma(W^{(r)}x_t + U^{(r)}h_{t-1}) \\
New\ memory\ content : \tilde{h}_t = tanh(Wx_t + r_t \circ Uh_{t-1}) \\
Final\ memory : h_t = z_t \circ h_{t-1} + (1-z_t) \circ \tilde{h}_t$$ <br>

먼저, Update gate, reset gate 에서 각각의 parameter로 현재 input vector와 이전 hidden state값의
연산을 한다. 둘 다 sigmoid를 사용해서 0~1 사이의 값을 얻는다. 그리고 reset gate에서 얻은 값은
New memory content($$\tilde{h}_t$$)를 계산할 때 사용되는데 현재 input과 이전 memory를 어떻게
조합할 지에 대한 결과로 0에 가까우면 이전 memory를 무시하게 되고 새로운 단어의 정보만 갖게 된다. (*그래서 reset인가 보다.*)
이제 Final memory($$h_t$$)값을 구해야 하는데, 수식을 살펴보면 새롭게 구상한 $$\tilde{h}_t$$와
$$h_{t-1}$$의 연산을 통해 구한다. 이는, 다음 state로 넘겨줄 값을 구할 때 reset을 사용해서
새롭게 만든 memory에 다시 한 번 $$h_{t-1}$$을 고려해서 이전 memory를 얼마나 넘겨줄 지에 대해
계산한다.

Final memory를 계산할 때 이전 hidden이 linear하게 고려되기 때문에 Long-term dependency가 생기게 된다.
이전 정보를 전달해주고 싶으면 $$z_t$$을 1로 계속 보내버리면 된다.
근데 의문점이 드는 것은 reset, update gate의 역할이 $$h_{t-1}$$을 반복적으로 고려해주는 것이 맞나에 대한 것과
특정 문장 ABCDE가 있다고 하면 A가 E랑 관련이 깊을 경우 진행 방향을 순차적이게 될 것이므로
B,C에서 모두 A에 대한 정보를 가지고 가야 하는가에 대함이다. 넘겨주는 정보는 한정되어 있으므로 E에 A의 정보가
전달되기 위해서는 B,C가 넘길 정보는 좀 줄어야 되지 않나라는 것이다.

어쨋든 위에서 설명한 내용을 도식화해서 나타내면 아래와 같다.
![GRU](https://whikwon.github.io/images/GRU.PNG)

***
## LSTM
LSTM은 1997년에 S Hochreiter가 발표한 모델이다. 앞서 설명한 GRU의 토대가 되는 모델이며 조금 더 복잡하다.
마찬가지로 수식을 보자. <br>
$$input\ gate: i_t = \sigma(W^{(i)}x_t + U^{(i)}h_{t-1})\\
Forget\ gate : f_t = \sigma(W^{(f)}x_t + U^{(f)}h_{t-1})\\
Output\ gate : o_t = \sigma(W^{(o)}x_t + U^{(o)}h_{t-1})\\
New\ memory\ cell : \tilde{c}_t = tanh(W^{(c)}x_t + U^{(c)}h_{t-1})\\
Final\ memory\ cell : c_t = f_t \circ c_{t-1} + i_t \circ \tilde{c}_t\\
Final\ hidden\ cell : h_t = o_t \circ tanh(c_t)$$

먼저, input, forget, output gate에서 현재 input vector와 이전 hidden state값의 연산을 한다.
앞선 GRU와는 다르게 LSTM에는 hidden말고 cell state도 존재해서 cell을 통해서 계산하고
다음 hidden값을 구할 수 있다.
New memory cell에서는 GRU와 달리 일단 가중치를 주지 않고 값을 구하고, 이 후에 input, forget gate
값을 고려해서 이전 cell을 얼마나 전달할 지, new memory cell값을 얼마나 전달할 지에 대해 연산을 한다.
Final memory cell값을 구할 때 항상 $$c_{t-1}$$이 linear하게 고려되기 때문에 Long-term dependency가 생기게 된다.
이전 정보를 전달하려면 $$f_t$$를 0으로 보내버리면 된다. 그리고 마지막으로 다음 hidden state값은
output gate값과의 연산을 통해 계산하면 된다. (*이 부분은 그렇게 중요하지 않은 듯해서 GRU에서 삭제했다.*)

GRU와 비교하면 cell, output gate의 유무와 연산 방식의 미묘한 차이가 있으나 전체적인 구상이 비슷하다.
(*GRU의 update와 LSTM의 forget이 Long-term dependency를 가져온다. New memory를 구상할 때 GRU는 reset을 고려하지만
  LSTM은 고려하지 않고 이 후에 Final memory를 계산할 때 고려해준다.*)

아래는 위 설명 내용을 도식화새서 정리한 자료이다. [Colah 블로그](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)에 가면 LSTM에 대해 상세하게 볼 수 있다.
![LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)
![LSTM_notation](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png)



***
이렇듯 GRU와 LSTM을 살펴보았고 이를 활용해서 좋은 성능을 내고 있다. 이로부터 파생된 다양한 모델들이 있고
이는 논문을 보면서 더 정리하도록 하겠다.


Reference: <br>
[Stanford CS224n lecture9 slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture9.pdf)
[Stanford CS224n lecture-note](http://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-notes5.pdf)
[Colah blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
