---
title:  "CS294_lecture5"
date: 2017-10-30 00:00:00
comments: true
---

- Berkeley 대학의 2017년도 가을학기 강의인 CS294: Deep Reinforcement Learning 의 5강을 정리한 내용이다.

***
## 살펴볼 내용
1. Critic을 이용해서 policy gradient 개선하기
2. The policy evaluation problem
3. Discount factors
4. The actor-critic algorithm
- 목표:
  - policy evaluation으로 어떻게 policy gradient를 fit 하는지 이해하기
  - Actor-critic 알고리즘 작동 방식 이해하기

***
## Policy gradient 개선
앞서 4장에서 policy gradient 알고리즘을 배운 내용을 되짚어보자. 그림의 가장 위 식을 보면 $$\hat Q_{i,t}$$ ***(reward to go)***
에 해당하는 항이 1개 trajectory에 해당하는 reward의 sum임을 알 수 있다. 이처럼 1개의 trajectory에 대해서만 구해도 학습은 가능하지만
policy gradient의 문제가 계속해서 gradient의 variance가 높은 것이 문제라고 했으므로 많은 trajectory에 대해서 expectation을 구하면
variance 문제가 조금 완화될 수 있다. 그래서 $$Q(s_t, a_t)$$를 *true expected reward-to-go* 라고 명명해준 뒤 식을 전개한다. <br><br>
![policy gradient improvement](https://whikwon.github.io/images/CS294_improve_policy_gradient.png)
<center> <i> &lt;policy gradient 개선&gt;</i> </center> <br>

***
## Baseline은 어떻게 변할까?
이 때, baseline은 어떻게 될까? 앞 장에서 1개 trajectory에 대한 baseline을 average($$b_t = \frac 1 N \sum_i \hat Q(s_{i,t}, a_{i,t})$$)로
둔 것을 기억하자. 그럼 *true expected reward-to-go* ($$Q(s_t, a_t)$$)에 대해서 baseline을 동일하게 policy에 대한 expectation으로
구하면 $$V(s_t) = \mathit{E}_{a_t \sim \pi_{\theta}(a_t|s_t)} \big [Q(s_t, a_t) \big ]$$로 나타낼 수 있다. <br><br>
![baseline?](https://whikwon.github.io/images/CS294_baseline2.png)
<center> <i> &lt;Baseline?&gt;</i> </center> <br>

***
## State & state-action value functions
위 내용을 바탕으로 *true expected reward-to-go* 를 활용해서 gradient를 정리하면 아래 그림과 같이 식을 구할 수 있고
policy gradient와의 차이점은 강화 학습 알고리즘을 풀어갈 떄의 초록색 부분이 $$\hat Q^\pi(x_t, u_t)$$를 구하는 과정에서 $$Q^\pi, V^\pi, A^\pi$$ 중
하나를 fit하면 되는 과정으로 변한 점이다. 아래 식에서는 policy들을 보면서 reward가 좀 더 좋은 방향으로 분포를 결정하기 때문에 $$A^\pi$$를 얼마나 잘 estimate하느냐가 중요하다고 볼 수 있다.

그럼 가장 중요한 핵심인 $$Q^\pi, V^\pi, A^\pi$$ fit을 어떻게 해야할까? 방법은 $$\mathit{E}_{s_{t+1} \sim p(s_{t+1} | s_t, a_t)} \big [V^\pi(s_{t+1}) \big ]$$ 항을
$$V^\pi(s_{t+1})$$과 같다고 둔 뒤 $$V^\pi(s_{t+1})$$을 fit하는 문제로 바꾸는 것이다. 이 말은 원래 $$t$$에서의 $$Q\pi(s_t, a_t)$$를 구할 때 state, action 모두
고려한 reward에 $$t+1$$의 모든 state를 고려했을 때의 Value function 값의 expectation을 더해주어야 하는데, 이를 단순히 $$t+1$$일 때 한 state 점에 대해서의 Value function 값과
같다고 놓고 구하는 것이다. 이럴 경우 식이 간단해져서 $$V^\pi$$만을 fit하는 문제로 변경되어 편리하다.

모델을 fit하기 위해서는 neural network를 만들고 state($$s$$)를 넣어줬을 때의 Value function 값을($$\hat V^\pi(s)$$)를 구해서 supervised 학습을 하면 된다. <br>
*(이런 식으로 전개할 경우에 값이 uncorrect해서 unbiased하게 되지만 그래도 variance를 줄일 수 있어 괜찮다고 한다.)*

![state & state-action value functions](https://whikwon.github.io/images/CS294_state_value_functions.png)
![state & state-action value functions](https://whikwon.github.io/images/CS294_fit_to_what.png)
<center> <i> &lt;state, state-action value functions&gt;</i> </center> <br>

***
## Policy evaluation
Value function을 policy에 fit하는 작업을 policy evaluation이라고도 불리는데 실제 policy 개선을 하지는 않지만 현재 상태에
policy가 어떤지 평가하기 때문이라고 한다. 그럼 이제 어떻게 Value function을 구할 수 있을지 방법을 생각해보자.

$$\begin{align}
&1.\ V^\pi(s_t) \approx \displaystyle \sum_{t' = t}^T r(s_{t'}, a_{t'}) \\
&2.\ V^\pi(s_t) \approx \frac 1 N \displaystyle \sum_{i=1}^N \sum_{t' = t}^T r(s_{t'}, a_{t'})
\end{align}$$ <br><br>
위 두 가지 방법에 대해서 보자. 두 가지 모두 Monte Carlo를 취했고 첫 번째는 policy gradient에서의 방법과 동일하다.
무엇이 나을지에 대해 생각해보면 후자가 많은 trajectory에 대해 average로 본다는 면에서 더 낫다고 얘기할 수 있으나 각각의
trajectory를 볼 때마다 simulator를 reset 해야 하므로 비효율적인 면이 적지 않다. 그래서 한 점에 대한 첫번째 식을 사용하고
, 앞서 supervised learning으로 NN을 학습시킨다고 하였으니 $$s_{i,t}$$에 대한 $$y_{i,t} = \sum_{t' = t}^T r(s_{i, t'}, a_{i, t'})$$를 모아서 training data를 만들고
regression으로 학습시킨다.

![monte carlo evaluation](https://whikwon.github.io/images/CS294_monte_carlo_evaluation.png)
<center> <i> &lt;Monte Carlo evaluation&gt;</i> </center> <br>
Monte Carlo말고 다른 방법으로는 bootstrap이 많이 쓰인다고 한다. 차이점은 우리가 학습(fit)시키려는 Value function을 training_data에 반영해서
$$s_{i,t}$$에 대한 $$y_{i,t} = r(s_{i, t}, a_{i, t}) + V_{\phi}^\pi(s_{i, t + 1})$$을 regression으로 학습시키는 것이다.

전제 조건은 Value function을 fit한 값이 괜찮다고 가정하고 하는 것이며 bootstrap을 하면 bias는 높아지고 variance를 낮출 수 있다고 한다.
강화 학습에서 많이 사용하는 방법이라고 한다.

![bootstrap](https://whikwon.github.io/images/CS294_bootstrap.png)
<center> <i> &lt;Bootstrap&gt;</i> </center> <br>
policy evaluation의 예제로는 TD-Gammon, AlphaGo가 있다고 하며 모두 Value function을 통해서 현재 board 상태에 따른 expected outcome을
에측할 때 사용했다고 한다. *(이 내용은 나중에 논문을 볼 때 다시 확인해보도록 하자.)*

***
## Actor-critic algorithm
지금까지 우리는 batch actor-critic 알고리즘(*online은 뒤에서 볼 예정.*)을 배웠다. 이는 다른 강화 학습 알고리즘 중에서는 policy gradient와 가장 유사하며
아래와 같은 방식으로 학습이 진행된다.

1) system의 policy로부터 state, action을 sampling한다. <br>
2) sampled reward sums로부터 $$\hat V_{\phi}^{\pi}(s)$$를 fit한다. <br>
3) $$\hat A^\pi(s_i, a_i)$$를 구한다. <br>
4) $$\nabla_{\theta} J(\theta) \approx \sum_i \nabla_{\theta} log\ \pi_{\theta}(a_i|s_i) \hat A^\pi(s_i, a_i)$$를 구한다. <br>
5) 4의 결과를 바탕으로 theta를 update해준다. <br>

![actor critic algorithm](https://whikwon.github.io/images/CS294_actor_critic.png)
<center> <i> &lt;Batch actor-critic 알고리즘 정리&gt;</i> </center> <br>

***
## Discount factors
Actor-critic 알고리즘으로 학습시키기 위해서 우리는 $$\hat V^\pi(s_{i, t+1})$$를 구해야 한다. 여기서 만약 한 번의 실행(*episode*)의 시간이
무한대라면 어떨까? 그럼 어떤 상황들에 대해서는 reward가 엄청나게 커질 가능성이 있다. 이는 학습이 오래 걸리거나 안 되게 할 수도 있기 때문에 좋지 않다.
그래서 discount factor($$\gamma$$)를 도입해서 이를 막고자 한다. 매우 단순한 방법으로 동일한 reward라도 빨리 받는 것을 선호하도록 시간이 지날수록 가중치가
계속 곱해져서 reward가 줄어들게끔 만든다.

![discount factors](https://whikwon.github.io/images/CS294_discount_factors.png)
<center> <i> &lt;Discount factors 설명&gt;</i> </center> <br>

***
## Discount factors for policy gradients
policy gradients에 discount factors를 적용하면 특이한 일이 생긴다. 결론적으로 ***causality*** 의 적용 유무에 따라서 식의 내용이 변하게 된다.
기존에 causality 로 영향을 안 미치는 reward 값을 제거해서 variance를 낮추기만 했다. 하지만 discount factors를 적용하고 causality를 적용하지 않을
경우에 아래 식처럼 뒤 step들의 reward의 중요도가 $$\gamma^{t-1}$$에 의해 영향을 받아 떨어지게 된다. *(option 2)*

그럼 둘 중에 어떤 식을 써야할까? 일단 답은 ***option 1*** 을 사용한다. 이유는 humanoid가 무한한 시간동안 달린다고 했을 때 현재에만 빠르게 뛰는게 아니라
계속 빠르게 뛰길 바라기 때문에 나중 step의 gradient도 지금 gradient와 같은 식으로 작용하길 바라기 때문이라고 한다. 또, approximating average
reward와도 관련이 있다고 하는데 이 부분에 대해서 이해를 잘 못하겠다. *(강의에서는 필요하면 논문을 보라고 했다.)*

![discount factors for policy gradient](https://whikwon.github.io/images/CS294_discount_factor_for_pg.png)
<center> <i> &lt;Policy gradient에 discount factors 적용&gt;</i> </center> <br>

***
## Actor-critic algorithm(with discount)
Actor-critic 알고리즘에 discount factors를 적용해보자. 앞서 본 batch 외에 online 학습 방법에 대해서도 소개한다.
batch는 $$\gamma$$가 추가되는 것 외에 앞의 내용과 완전히 동일하므로 넘어가도록 하자. 아래 online 학습을 보면 이름이 의미하듯이
하나의 state($$s$$), action($$a$$) 점으로부터 다음 state($$s'$$)와 reward($$r$$)를 구한다. 그리고 앞에서 살펴본 ***bootstrap*** 을
이용하여 $$\hat V_\phi^\pi$$를 update한다. 이 후 step에 대해서는 batch와 같다.

![actor critic with discount](https://whikwon.github.io/images/CS294_actor_critic_with_discount.png)
<center> <i> &lt;Actor-critic에 discount factors 적용&gt;</i> </center> <br>

***
## Architecture design
Actor-critic 알고리즘 모델의 설계는 두 가지 방식으로 진행될 수 있다. <br>
1. Actor와 Critic을 나누어 각각에 대한 모델(Neural network)을 학습
  - 장점: 단순하고 안정적인 학습이 가능하다.
  - 단점: parameter sharing이 불가능하다.
2. Actor와 Critic을 합쳐서 하나의 모델(Neural network)을 학습
  - 장점: parameter sharing기 가능하고 학습이 잘 될 경우 더 효율적이다.
  - 단점: 서로 다른 2개의 gradient를 반영해주어야 하므로 학습이 안정적이지 않다.

![architecture design](https://whikwon.github.io/images/CS294_architecture_design.png)
<center> <i> &lt;Architecture design&gt;</i> </center> <br>

***
## Online actor-critic in practice
간략하게 어떤 방식으로 Actor-critic 알고리즘 모델을 학습하는지 살펴보고 넘어가자.
크게 두 가지 방식이 많이 사용되며 뒤에서 관련 내용에 대해서는 상세하게 뒤에서 다시 볼 예정이라고 한다.
1. Synchronized parallel actor-critic: thread를 사용해서 sample을 여러 개 뽑아서 parameter($$\theta$$)에 대한 gradient를 각각 구한 뒤 함께 update한다.
2. Asynchronous parallel actor-critic: parameter($$\theta$$)를 관리하는 server를 두고 thread로부터 정보를 받아 update하고 다시 공유해주는 방식으로 진행된다.
1에서 동시적으로 일어나는 부분을 제거해서 빠르고 단순하다고 한다.

![actor-critic in practice](https://whikwon.github.io/images/CS294_actor_critic_in_practice.png)
<center> <i> &lt;Actor-critic 학습 방법&gt;</i> </center> <br>

***
## Critics as state-dependent baseline
이제 조금은 다른 내용을 다뤄보자. Actor-critic과 policy gradient를 비교했을 때 둘 다 baseline이 적용되고 이에 따른 각각의 장단점이 존재한다.
Actor-critic은 variance는 낮으나 critic이 완벽하지 않기에 unbiased하지 않다. 반면 policy gradient는 unbiased하나 variance가 매우 높다.

이러한 점을 고려해서 *둘의 장점이 모두 발휘되도록 알고리즘을 섞어주면 어떨까?* 에 대해 생각해보자. (*모든 접근 방식에 기초가 되는 듯하다. 섞고 섞고 돌리고 섞고*)
그래서 policy gradient의 $$b$$항 위치에 $$\hat V_{\phi}^\pi(s_{i,t})$$를 넣고 결론적으로 unbiased하면서 variance도 낮아진다.
*unbiased* 한 이유는 $$\hat V_{\phi}^\pi(s_{i,t})$$가 action과 관련이 없는 항이기 때문에 integrate할 경우 사라지게 되어 그렇다.
variance는 총 reward의 합과 baseline이 가까우면 낮아지게 될텐데 $$\hat V_{\phi}^\pi(s_{i,t})$$이 reward의 합을 target으로 학습하기 때문에
variance가 낮아질 수 밖에 없다.

![critics as baseline](https://whikwon.github.io/images/CS294_critics_as_baseline.png)
<center> <i> &lt;State-dependent baseline으로의 Critics 사용&gt;</i> </center> <br>

***
## Action-dependent baselines
지금까지 state-dependent한 baseline을 봤다. 이는 state에 dependent해야 unbiased한 상태로 variance를 줄일 수 있기 때문이다.
그럼 action에 dependent한 baseline을 사용하면 어떨까? 아래 식에서 보듯이 $$V_{\phi}^\pi(s_t)$$ 대신 $$Q_{\phi}^\pi(s_t, a_t)$$를 사용하면
$$\hat A^\pi(s_t, a_t)$$의 값이 거의 0에 가까워져서 매우 낮은 variance를 얻을 수 있다. (*critic이 괜찮다면*) 하지만 gradient를 구하는 식($$\nabla_{\theta}J(\theta)$$을 보면
action에 dependent한 값을 사용할 경우 더 이상 unbiased하지 않게 된다.

unbiased한 문제는 몇몇 경우에 해결이 가능하다고 한다. 마지막 식처럼 변형해준 뒤 예를 들면 policy($$\pi$$)가 Gaussian이고 $$Q$$가 Quadratic이어서 analytic하게 expectation을 계산할 수 있다고 한다.  
discrete action이 조금 더 다루기 쉽고 continuous action에 대해서는 조금 더 어렵지만 방법이 있다고 한다. (*마지막 식을 유도하는 과정을 잘 모르겠다.*) <br><br>
![action dependent baselines](https://whikwon.github.io/images/CS294_action_dependent_baselines.png)
<center> <i> &lt;Action-dependent baselines&gt;</i> </center> <br>

***
## Eligibility traces & n-step returns
위 Critics as state-dependent 에서 살펴본 내용에 이어서 조금 더 variance를 낮출 수 있는 방안을 살펴보도록 하자.
방법은 Critic($$\hat A_C^\pi$$)의 lower variance함과 Monte Carlo($$\hat A_{MC}^\pi$$)의 unbiased함을 합쳐주는 과정이다. 여기서 사용한 $$\hat A_{MC}^\pi$$
는 위에서 policy gradient를 변형한 식을 사용한다.

직관적인 접근 방법은 현재로부터 여러 trajectory를 지나서 미래에 도달했을 때 가능성이 너무 많아지므로(*high variance*) 현재에 가까운 부분들(*n step*)만 고려해서 variance를 낮추자는 방안이다.
수식으로 나타내면 기존 $$\hat A_{MC}^\pi$$에서 $$\infty$$에 대한 부분을 n step까지 보는 것으로 바꿔주고, 여기에 n step까지 여러 trajectory에 대해 본 값을 넣어줘야 하므로 $$\hat A_C^\pi$$의
2번째 항을 추가해준다. 최종적으로 n의 조절을 통해서 bias-variance를 조절할 수 있다.

![eligibility traces](https://whikwon.github.io/images/CS294_eligibility_traces.png)
<center> <i> &lt;Eligibility traces & n-step returns&gt;</i> </center> <br>

***
## Generalized advantage estimation
위에서 본 n-step returns 내용을 조금 더 일반화시킨 내용이다. 하나의 n을 구해서 $$\hat A_n$$을 구하는 대신 모든 n-step에 대해서 가중치 평균을 두면 어떨까라는 생각이다.
가중치는 discount factor와 마찬가지로 현재에 가까울수록 더 가중치를 크게 주는 식으로 진행한다. 최종적으로 식을 정리하면 $$\gamma, \lambda$$에 의해서 variance를 조절할 수 있다.

![generalized_advantage_estimation](https://whikwon.github.io/images/CS294_generalized_advantage_estimation.png)
<center> <i> &lt;generalized advantage estimation&gt;</i> </center> <br>

Reference:
[Berkeley Univ. CS294 Lecture slides](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_5_actor_critic_pdf.pdf)
