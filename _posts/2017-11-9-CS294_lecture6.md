---
title: "CS294_lecture6: Value functions introduction"
date: 2017-11-9 00:00:00
layout: post
excerpt: "Berkeley 대학의 2017년도 가을학기 강의인 CS294: Deep Reinforcement Learning 의 6강을 정리한 내용이다."
categories: [Reinforcement Learning, Lecture]
comments: true
---

## 살펴볼 내용
1. Actor 없이 Critic으로만 학습할 수 있는지?
2. Value function에서 policy 빼내기
3. The Q-learning algorithm
4. Extensions: continuous actions, improvements
- 목표:
  - Value function이 어떻게 policy를 내놓는지 이해하기
  - Q-learning algorithm 이해하기
  - Q-learning에서의 practical considerations 이해하기

## Policy gradient를 생략하려면?
4,5장에서 policy gradient의 높은 variance로 인한 문제를 계속 살펴보았다. 그래서 그럼, policy gradient를 생략하면 어떨까? 라는
식의 접근을 한다. $$A^{\pi}(s_t, a_t)$$는 $$\pi$$에 대한 action의 평균보다 $$a_t$$가 얼마나 나은지에 대해 나타내는 값이므로
$$argmax_{a_t} A^\pi(s_t, a_t)$$ 인 $$a_t$$는 $$s_t$$가 주어졌을 때 최고의 action으로 볼 수 있다. 그럼, 우리는 policy를 결정해줄 때
$$
\pi'(a_t \lvert s_t)=\left\{
                        \begin{array}{ll}
                          1\ \text{if}\ a_t = {argmax}_{a_t} A^{\pi}(s_t, a_t)\\
                          0\ \text{otherwise}
                        \end{array}
                     \right.
$$ 와 같이 policy를 학습시킬 수 있다. 이렇게 학습된 policy는 주어진 state에서 최고의 action이므로 괜찮을 것이라고 예상할 수 있다.

***Policy gradient를 빼자*** 라는 아이디어에서 출발해서 $$A^{\pi}(s_t, a_t)$$만 알아내면 policy를 학습할 수 있다는 것을 알았고
이 학습 구조는 아래 그림에 나타나있다. 여기서 $$A^{\pi}(s_t, a_t) = r(s, a) + \gamma \mathit{E}[V^{\pi}(s')] - V^{\pi}(s)$$ 이므로
$$V^{\pi}(s)$$ 만 구하면 policy를 학습시킬 수 있게 된다.

![policy iteration](https://whikwon.github.io/images/CS294_policy_iteration.png)
<center> <i> &lt;policy iteration high-level 아이디어&gt;</i> </center> <br>

## Dynamic programming
앞서 Dynamic programming에 대해 간략하게 소개하면 연속적으로 발생되는 문제를 수학적으로 최적화해서 풀어나가는 것이라고 한다.
두 가지 특성으로 문제를 접근하는데 각각 optimal substructure로서 최적화(*문제를 하위 문제로 쪼개서 최적화하면 원래 문제도 최적화된다.*)와
overlapping subproblem(*subproblem들이 여러 번 반복적으로 나타나기 때문에 하나를 해결하고 이 결과를 저장해서 다시 사용한다.*)이다.
강화 학습에서 MDP에 동일하게 적용되서 Bellman equation과 value function이 이와 같은 특징을 가지고 있다고 한다.
그리고 기본적으로 Dynamic programming은 MDP의 모든 상황에 대해 알고 있다고 가정한다고 한다. 그래서 planning할 때 사용한다고 한다.

여기까지 소개하고 다시 본문으로 돌아가면 state, action가 discrete하고 우리가 Transition($$p(s' \lvert s, a)$$)을 알고 있다고 가정하면
transition table에 Value function 값을 나타낼 수 있다. (*$$V^{\pi}(s)$$를 구하기 위해서는 $$s'$$ 값만 구하면 되는데 transition이 해결해주니 됐다.*)

$$V^{\pi}(s)$$를 구할 수 있으니 아래 과정처럼 iteration 돌려 $$V^{\pi}(s)$$값을 구하고 policy를 update해주면 괜찮은 policy를 얻을 수 있다.

![dynamic programming](https://whikwon.github.io/images/CS294_dynamic_programming.png)
![dynamic programming](https://whikwon.github.io/images/CS294_dynamic_programming2.png)
<center> <i> &lt;Dynamic programming with policy iteration&gt;</i> </center> <br>

## Even simpler dynamic programming
위에서 transition이 주어졌을 때 dynamic programming으로 어떻게 접근할 지 살펴보았다. 더 단순하게 처리할 수 있는 방안을 생각해보자.
핵심은 policy를 skip하고 value를 바로 구하는 것이다. $${argmax}_{a_t}A^{\pi}(s_t, a_t) = {argmax}_{a_t}Q^{\pi}(s_t, a_t)$$인 점을
이용해서 수식을 전개하면 value iteration algorithm을 얻을 수 있다. 미리 알고 있는 transition으로부터 $$Q^{\pi}(s, a)$$를 구하고
해당 값을 maximize하는 action을 선택한 뒤에 (policy는 자동 update되고) $$V^{\pi}(s)$$를 구하는 방식이다.

![simpler dynamic programming](https://whikwon.github.io/images/CS294_simpler_dynamic_programming.png)
<center> <i> &lt;더 단순한 dynamic programming&gt;</i> </center> <br>

## Fitted value iteration
단순한 경우에는 상관이 없으나 실세계는 매우 복잡해서 Value function을 tabular하게 나타내는데 어려움이 있다.
그래서 actor-critic에서 봤던 fitting를 다시 사용한다. neural network로 $$V_{\phi}(s)$$를 fit하며
입력 값 $$s$$를 받아서 출력 값이 $$V(s)$$를 나타내는 $$\underset a {max} Q^{\pi}(s, a)$$를 예측하도록 한 뒤
regression 문제를 풀 듯 $$V_{\phi}(s_i)$$를 학습시킨다.

![fitted value iteration](https://whikwon.github.io/images/CS294_fitted_value_iteration.png)
<center> <i> &lt;fitted value iteration&gt;</i> </center> <br>

## Transition dynamics를 모를 경우
지금까지 우리는 transition이 주어졌을 때를 다뤘는데 만약 모른다면 어떻게 할까? 위에서 소개한 Fitted value iteration으로
구할 방법을 생각해볼 수 있겠다. 근데 여기서 $$y_i$$를 구하기 위해 action에 대한 $$max$$항을 풀어야 하는데 이를 위해서는 한 state에 대한
많은 action을 알아야 한다. 예를 들면 자동차로 치면 현 state에 대한 다양한 action을 알기 위해서는 시간이동을 해야 되는데 이는 불가능하다.
물론 system의 dynamics를 알고 있다면 simulator를 통해 알 수 있지만 그렇지 않다면 구할 수 없다.

그래서 약간의 trick을 써서 state에 대한 항인 $$V^{\pi}(s)$$ 대신 state, action 모두에 대한 항인 $$Q^{\pi}(s, a)$$을
$$max$$로 만드는 action을 찾는 문제로 바꿔준다.

방법은 policy iteration에서의 fitted value iteration algorithm을 만들었을 때의 trick을 사용하면 된다고 한다.
수식이 이해가 명확하게 안 되서 내용만 적으면 $$V^{\pi}(s)$$를 state, action에 대한 항인 $$Q^{\pi}(s, a)$$로 바꿔주면서
$$max$$를 없애고 결과적으로 state에 대한 다양한 action들에 대해 simulation을 할 필요가 없게 만든다.

이렇게 했을 때의 장 단점이 존재하는데 장점은 off-policy에서 작동한다는 점, variance가 높은 policy gradient가 없다는 점이고
단점은 tabular한 case에는 convergence guarantee하지만 neural net의 경우에는 non-guarantee하다는 것이다. 이에 관련된
내용은 뒤에서 더 자세히 살펴본다고 한다.

![without dynamics](https://whikwon.github.io/images/CS294_dont_know_the_dynamics.png)
![without dynamics2](https://whikwon.github.io/images/CS294_dont_know_the_dynamics2.png)
<center> <i> &lt;dynamics를 모르는 경우의 학습 방법&gt;</i> </center> <br>

## Fitted Q-iteration
위에서 제시한 fitted Q-iteration에 대한 수식을 정리해보자. fitted value iteration에서 $$V_{\phi}$$에 관한 항을
$$Q_{\phi}$$로 바꿔주면 된다. sample을 뽑고 state, action에 대한 $$y_i$$ 값을 구하고(*이 때 policy가 update 된다.*)
이를 사용해서 $$Q_{\phi}$$를 학습시키게 된다.

![fitted Q iteration](https://whikwon.github.io/images/CS294_fitted_q_iteration.png)
<center> <i> &lt;fitted Q-iteration&gt;</i> </center> <br>

## Fitted Q-iteration: Off-policy
앞서 언급한 장점 중에 off-policy에서 작동한다고 했는데 왜 그런지 살펴보자. off-policy 하다는 의미는 sample을 뽑은 뒤에 policy가
변할 때마다 다시 sample을 뽑을 필요가 없다는 뜻이다. 아래 1, 2, 3 과정을 살펴보자. 1에서 sample dataset을 뽑고 이를 이용해서
2를 진행한다. $${max}_{a'_i} Q_{\phi}(s'_i, a'_i)$$ 항이 계산될 때 policy가 가장 클 때의 action 값으로 update된다.
그리고 2에서 얻은 값을 이용해서 3에서는 $$Q_{\phi}(s_i, a_i)$$를 학습시켜준다.

결국 2 과정에서 policy가 변하게 되는데 update되더라도 다음 과정에 아무런 영향을 미치지 않게 된다. 그래서 새롭게 sample을 뽑을 필요가
없다. 강의에서는 policy와 independent한 transition의 bucket을 뽑고 여기에서 데이터를 가져다가 Q-iteration을 돌린다고 설명한다.

![fitted Q iteration: off-policy](https://whikwon.github.io/images/CS294_q_iteration_offpolicy.png)
<center> <i> &lt;fitted Q-iteration 특징: off-policy&gt;</i> </center> <br>

## Fitted Q-iteration: Optimization
fitted Q-iteration이 무엇을 optimizing 하는지 살펴보자. 2번 과정에서 policy를 improve 하는 내용은 앞서 살펴봤고
3번 과정에서의 오른쪽 값을 *error(Bellman error)* 라고 하고 $$\mathcal{E} = \dfrac 1 2 \mathit{E}_{(s,a) \sim \beta} \Big [Q_{\phi}(s,a) - \big [r(s,a) + \gamma \underset {a'} {max}\ Q_{\phi}({s'}_i, {a'}_i) \big ] \Big ]$$의 식으로 나타낼 수 있다. 여기서 $$\beta$$는 state와 action이 속한 분포를 나타낸다.
위 식으로부터 $$\mathcal{E} = 0$$ 일 때 모든 $$\beta$$가 주어졌을 때 *optimal Q-function* 을 $$Q_{\phi}(s,a) = r(s,a) + \gamma {max}_{a'}\ Q_{\phi}(s', a')$$ 임을 알 수 있다.
*tabular한 case에만 guarantee하며 neural network에서는 non-guarantee하다.*

![fitted Q iteration: optimization](https://whikwon.github.io/images/CS294_fitting_q_iteration_optimizing.png)
<center> <i> &lt;fitted Q-iteration: optimization&gt;</i> </center> <br>

## Online Q-learning algorithms
지금까지 batch 학습에 대해 살펴봤고 이제 1개의 sample에 대해서 학습이 되는 online 학습에 대해 살펴보자.
1개 action으로부터 state, action, next_state, reward를 observe해서 이로부터 target value를 구하고 gradient update를 1번
하는 과정이 반복되는 형태이다. batch와 다른 점은 하나의 데이터에 대해서 학습이 진행된다는 점이다.

![Online Q-learning algorithms](https://whikwon.github.io/images/CS294_online_q_learning.png)
<center> <i> &lt;Online Q-learning 알고리즘&gt;</i> </center> <br>

## Exploration with Q-learning
지금까지 policy를 결정할 때 greedy 방식을 취했는데 이는 특정한 policy 외에 다른 방향으로의 학습을 아예 고려하지 않기 때문에
나중에 더 좋은 방향이 될 수도 있는 가능성들을 모두 무시하게 되어 좋지 않다. 그래서 어느 정도 exploration을 할 수 있도록
policy를 결정하는 식을 변경해준다. 방법은 *epsilon-greedy*, *Boltzmann exploration* 등이 있다.

![Online Q-learning algorithms](https://whikwon.github.io/images/CS294_exploration_q_learning.png)
<center> <i> &lt;Exploration with Q-learning&gt;</i> </center> <br>

## Value iteration 알고리즘 수렴 여부
value iteration algorithm으로 돌아와서 알고리즘의 수렴 여부에 대해 살펴보도록 하자.
살펴볼 내용은 알고리즘으로 학습했을 때 converge 하는지, 만약 한다면 어디로 하는지, 어떻게 찾을 수 있는지에 대한 내용이다.

operator $$\mathcal{B}: \mathcal{B}V = {max}_a\ r_a + \gamma \mathcal{T}_a V$$을 정의한다.
$$V$$에 대해서 $$\mathcal{B}$$를 operate 해줬을 때 $${max}_a\ Q(s,a)$$를 구하는 내용과 동일하다. 이 때,
$$V^{\star} = r(s,a) + \gamma \mathit{E}[V^{\star}(s')]$$라는 $$\mathcal{B}$$의 *fixed point* 가 있다고 가정하면
이 점에서 operator를 돌려도 값이 그대로이다. $$V^{\star} = \mathcal{B}V^{\star}$$. reward 값에 다음 Value를 더해도
값이 그대로이므로 $$V^{\star}$$에서 *optimal policy* 를 가질 것이라고 볼 수 있다.

$$V^{\star}$$는 항상 unique하게 존재하며 이 때 optimal policy를 가진다고 한다. (*강의에서 수학적인 증명은 생략했다.*)

그럼 이제 value iteration을 통해서 $$V^{\star}$$ 도달할 수 있는지를 보여야 한다. $$\mathcal{B}$$가 [*contraction*](https://en.wikipedia.org/wiki/Contraction_mapping)
임을 증명한 뒤(*강의에서 생략*) $$\bar V$$를 $$V^{\star}$$로 변경해주면 infinity norm에 대해서 $$\mathcal{B}V^{\star} = V^{\star}$$를 증명할 수 있다.
즉, $$\mathcal{B}$$를 operate할 때마다 $$V$$가 $$V^{\star}$$에 점점 가까워져서 수렴한다.

![value function theory](https://whikwon.github.io/images/CS294_value_function_learning_theory.png)
![value function theory](https://whikwon.github.io/images/CS294_value_function_learning_theory2.png)
<center> <i> &lt;Value iteration 알고리즘 수렴 여부&gt;</i> </center> <br>

## Fitted value iteration 알고리즘 수렴 여부
이번에는 Non-tabular한 *fitted* value iteration의 경우에 대해 살펴보자. value iteration과 다르게 *fitted* value iteration은
$$\mathcal{B}$$를 operate한 뒤 Value function을 neural net과 같은 function에 학습시키는 과정이 포함되어 있다.
이를 식으로 나타내면 $$V' \leftarrow {argmin}_{v' \in \Omega} \dfrac 1 2 \sum \lVert V'(s) - (\mathcal{B}V)(s) \rVert^2$$ 이다.
이 과정은 $$\mathcal{B}V$$의 값을 $$Omega$$ 라는 neural net 등으로 나타나는 function에 $$V'$$로 *projection* 하는 의미로 해석할 수 있다.
그래서 새로운 operator $$\Pi$$를 $$\Pi V \leftarrow {argmin}_{v' \in \Omega} \dfrac 1 2 \sum \lVert V'(s) - V(s) \rVert^2$$ 와 같이
정의해준다.

앞서 마찬가지로 fitted value iteration을 통해서 $$V^{\star}$$에 도달할 수 있는지 확인해야 한다. 결론적으로 말하자면 $$\Pi \mathcal{B}$$에 대한 contraction이
성립하지 않아 fitted value iteration은 일반적으로 수렴하지 않는다. 그래서 이를 어떻게 하면 완화해서 practical하게 사용할 수 있을지에 대해 살펴볼 예정이다. (*다음 강의*)

![fitted value function theory](https://whikwon.github.io/images/CS294_non_tabular_value_function_learning.png)
![fitted value function theory](https://whikwon.github.io/images/CS294_non_tabular_value_function_learning2.png)
<center> <i> &lt;Fitted value iteration 알고리즘 수렴 여부&gt;</i> </center> <br>

## Fitted Q-iteration 알고리즘 수렴 여부
마지막으로 fitted Q-iteration의 수렴 여부에 대해 살펴보자. fitted value iteration과 동일한 내용으로 $$\Pi \mathcal{B}$$에 대한 contraction이 성립하지 않아
일반적으로 수렴하지 않는다.

![fitted Q-iteration theory](https://whikwon.github.io/images/CS294_fitted_Q_iteration_theory.png)
<center> <i> &lt;Fitted Q-iteration 알고리즘 수렴 여부&gt;</i> </center> <br>

## 번외: online Q-iteration
다음 Q-learning 강의에서 제시할 해결 방안에 대한 문제를 미리 보도록 하자. online Q-learning에서의 gradient descent항을 살펴보면
gradient descent가 아니고 단순히 regression이라는 것을 알 수 있다. 이 때문에 많은 문제가 생겨서 다음 강의에서 이를 해결하기 위한
방법들을 소개한다고 한다.

![online Q-iteration 문제](https://whikwon.github.io/images/CS294_online_q_iteration_problem.png)
<center> <i> &lt;Online Q-iteration의 gradient descent항?&gt;</i> </center> <br>

## 번외: a sad corollary
아래 내용은 Actor-critic 알고리즘을 살펴볼 때 fitted bootstrapped policy evaluation이 수렴하지 않는다는 설명을 했었는데
그 이유가 위에서 fitted value iteration에서 살펴봤을 때의 내용과 다르지 않다고 생각할 수 있다는 내용이다.

![sad corollary](https://whikwon.github.io/images/CS294_sad_corollary.png)
<center> <i> &lt;Actor-critic 알고리즘과 fitted value iteration의 공통점&gt;</i> </center> <br>

Reference: <br>
[Berkeley Univ. CS294 Lecture slides](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_6_value_functions.pdf) <br>
[dynamic programming 블로그 정리 글](http://daeson.tistory.com/325)
