---
title:  "RL_Lecture2(David Silver)"
date: 2017-11-27 00:00:00
comments: true
---

- David Silver의 Reinforcement Learning 2강을 정리한 내용이다.

## Markov Processes
### 1) Markov Property
state $$S_t$$가 $$\mathcal{P}[S_{t+1} \lvert S_t] = \mathcal{P}[S_{t+1} \lvert S_1, ..., S_t]$$를 만족할 때 *Markov* 하다고 한다.
미래는 현재 상태(state)에 의해서 결정되고 과거(history)와는 독립적이며 현재의 상태가 미래에 필요한 과거 관련 정보들을 가지고 있다. <br>

![markov property](https://whikwon.github.io/images/david_silver/markov_property.png)

### 2) State Transition Matrix
Markov state $$s$$가 그 다음 state인 $$s'$$로 전이될 확률 *state transition probability* 은 $$P_{ss'} = \mathcal{P}[S_{t+1} = s' \lvert S_t = S]$$로 정의한다.

### 3) Markov Process
Markov process는 memoryless random process이다. 즉, 무작위 state에서 출발해서 Markov property를 지키면서 다음 state로 전이된다. <br>

![markov process](https://whikwon.github.io/images/david_silver/markov_process.png)

### 4) 예시
아래 그림은 Markov process에 따라 여러 *episode* 를 sampling한 예시를 나타낸다. 각 node들은 state를 나타내며
다음 state로 전이될 때 과거 state와는 독립적이다. state 전이는 일정 확률로 결정되므로 다양한 episode가 만들어질 수 있다. <br>

![example](https://whikwon.github.io/images/david_silver/example1.png)

## Markov Reward Processes
### 1) Markov Reward Process
Markov reward process는 Markov process에 reward, discount factor를 추가한 형태이다. <br>

![markov reward process](https://whikwon.github.io/images/david_silver/markov_reward_process.png)

### 2) Return
return $$G_t$$는 $$t$$ 이후의 모든 reward들의 합이라고 정의한다. 이 때 discount factor $$\gamma$$는 0과 1 사이의 값으로 정하며
0에 가까울 수록 근시안적, 1에 가까울 수록 원시안적으로 평가한다. <br>

![return](https://whikwon.github.io/images/david_silver/return.png)

### 3) Discount Factor
markov decision process에서 대부분 discount factor를 사용한다. 그 이유를 아래와 같다.
  - 수학적으로 편리하다.
  - 무한히 반복되는 cyclic Markov process를 피할 수 있다.
  - 미래가 불확실한 경우가 많아 이를 보완해줄 수 있다.
  - 사람은 주로 먼 reward보다 가까운 reward에 더 관심이 있다.
  - 모든 episode에 끝이 있는 경우에는 undiscounted($$\gamma = 1$$)로 진행하기도 한다.

### 4) Value Function
value function $$v(s)$$는 state $$s$$에 대한 long-term value를 준다. $$s$$가 주어졌을 때 $$G_t$$에 대한 expectation 형태이므로
가능한 모든 return을 고려했을 때의 값이다. <br>

![value function](https://whikwon.github.io/images/david_silver/value function.png)

### 5) Bellman Equation for MRPs
위에서 정의한 value function를 변형시켜 아래와 같이 나타낼 수 있다. <br>

![bellman eq MRP](https://whikwon.github.io/images/david_silver/bellman_MRP.png) <br>

식이 나타내는 뜻은 $$s$$에서의 value function의 값을 모든 다음 state $$s'$$에 대한 전이할 떄의 reward와 value function 값을 더한 값의
expectation과 같다는 것으로 아래 그림 및 식으로 나타낼 수 있다. <br>

![bellman eq MRP](https://whikwon.github.io/images/david_silver/bellman_MRP2.png) <br>

아래 예시처럼 bellman eq를 통해 value function을 구할 수 있다. <br>

![example](https://whikwon.github.io/images/david_silver/example2.png) <br>

bellman eq는 matrix 형태로 $$v = \mathcal{R} + \gamma \mathcal{P} v$$로 나타낼 수 있다. 이 때 항을 넘겨서 계산하면 $$v = (I - \gamma \mathcal{P})^{-1} \mathcal{R}$$로
해를 구할 수 있는데 직접적으로 계산하기에는 $$O(n^3)$$로 computational complexity가 높아 iterative method를 사용한다.
계속해서 배울 내용들이 여기에 포함되며 Dynamic programming, Monte-Carlo evaluation, Temporal-Difference learning 등이 있다.

## Markov Decision Processes
### 1) Markov Decision Processes
Markov decision process는 Markov reward process에 decision(action)를 추가한 형태이다. <br>

![markov decision process](https://whikwon.github.io/images/david_silver/markov_decision_process.png)

### 2) Policy
Policy는 state가 주어졌을 때 action의 분포를 나타낸다. agent의 행동을 완전히 나타내며 time-independent(*stationary*)한 특징을 갖는다. <br>

![policy](https://whikwon.github.io/images/david_silver/policy.png) <br>

policy $$\pi$$에 대한 state transition probability와 reward는 아래 식으로 나타낼 수 있다.
$$\begin{align}
\mathcal{P}_{s, s'}^{\pi} &= \displaystyle \sum_{a \in A} \pi(a \lvert s) \mathcal{P}_{ss'}^a \\
\mathcal{R}_{s}^{\pi} &= \displaystyle \sum_{a \in A} \pi(a \lvert s) \mathcal{R}_s^a
\end{align}$$

### 3) Value Function
MDP에서는 기존 MRP의 value function인 *state-value function* 와 action에 대한 항이 추가된 *action-value function* 두가지로 정의된다. <br>

![value function](https://whikwon.github.io/images/david_silver/value function2.png)

### 4) Bellman Expectation Equation
MRP에서와 마찬가지로 아래 state-value function을 아래와 같이 나타낼 수 있다.
$$\begin{align}
v_{\pi}(s) &= \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \lvert S_t = s] \\
q_{\pi}(s, a) &= \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) \lvert S_t = s, A_t = a]
\end{align}$$ <br>
backup diagram을 살펴보자. 먼저, state-value function은 모든 action에 대해서 고려했을 때의 값이므로 action-value function에 대한
expectation으로 볼 수 있다. <br>

![backup diagram - v](https://whikwon.github.io/images/david_silver/backup_v.png) <br>

다음은 action-value function인데 다음 state $$s'$$로 전이되었을 때를 생각해보면 전이할 때의 reward가 주어지고 $$s'$$는 여러가지 경우가 나올 수 있으므로
이에 대한 value function의 expectation 값을 더한 형태와 같다고 볼 수 있다. <br>

![backup diagram - q](https://whikwon.github.io/images/david_silver/backup_q.png) <br>

$$v_{\pi}$$, $$q_{\pi}$$에 대한 내용을 합쳐서 다시 나타내면 아래와 같이 $$v_{\pi}$$, $$q_{\pi}$$를 나타낼 수 있다. <br>

![backup diagram - v](https://whikwon.github.io/images/david_silver/backup_v2.png)
![backup diagram - q](https://whikwon.github.io/images/david_silver/backup_q2.png)

위에서 정리한 식으로 value 값을 구하는 예시이다. <br>

![example](https://whikwon.github.io/images/david_silver/example3.png)

### 5) Optimal Value Function
optimal state-value, action-value function을 아래와 같이 정의할 수 있고 우리가 이를 알면 MDP가 해결되었다고 얘기할 수 있다. <br>

![optimal value function](https://whikwon.github.io/images/david_silver/optimal_value_function.png) <br>

policy에 관해서도 아래와 같은 Theorem을 적용할 수 있다. 내용은 optimal policy가 존재하며 그 때 state-value function, action-value function의 값도 optimal하다는 것이다. <br>

![optimal policy](https://whikwon.github.io/images/david_silver/optimal_policy.png) <br>

그럼 optimal action-value function일 때의 policy를 deterministic하게 정하는 방법을 생각해볼 수 있다.

$$\pi_{\ast}(a \lvert s)=\left\{
            \begin{array}{ll}
              1\ \text{if}\ a = \underset {a \in A} {argmax}\ q_{\ast}(s, a)\\
              0\ \text{otherwise}
            \end{array}
        \right.$$ <br>
위 식을 따라 구하면 아래 예시와 같이 최적의 경로를 알 수 있게 된다. 이로써 문제를 풀었다고 할 수 있겠다. <br>

![example](https://whikwon.github.io/images/david_silver/example4.png) <br>

### 6) Bellman Optimality Equation
backup diagram을 살펴보자. state-value function부터 보면 $$v(s) = \displaystyle \sum_{a \in A} \pi(a \lvert s) q_{\pi}(s, a)$$의 식에서
나타나듯 action-value에 대한 expectation 값이다. 그럼 optimal값을 구하려면 가장 좋은 action에서의 값을 고르면 된다. 그래서 $$v_{\ast}(s) = \underset a {max} q_{\ast}(s, a)$$
로 아래 그림과 같이 나타낼 수 있다. <br>

![backup diagram - v](https://whikwon.github.io/images/david_silver/backup_v3.png) <br>

action-value function을 살펴보면 다음 state $$s'$$로 전이되었을 때 reward가 주어지고 $$s'$$는 여러가지 경우가 나올 수 있는데 우린 optimal한 값을 원하므로
optimal state-value의 expectation을 더한 값으로 나타낼 수 있다. <br>

![backup diagram - q](https://whikwon.github.io/images/david_silver/backup_q3.png) <br>

아래 그림은 위 내용을 합쳐서 $$v_{\ast}(s)$$와 $$q_{\ast}(s, a)$$를 나타낸 것이다. <br>

![backup diagram - v](https://whikwon.github.io/images/david_silver/backup_v4.png)
![backup diagram - q](https://whikwon.github.io/images/david_silver/backup_q4.png)

위에서 정의한 Bellman equation은 non-linear하고 closed form solution이 존재하지 않아서 보통 iterative solution method로 푼다.
많이 사용하는 방법이 **Value iteration, Policy iteration, Q-learning, Sarsa** 로 앞으로 배울 내용이다.
