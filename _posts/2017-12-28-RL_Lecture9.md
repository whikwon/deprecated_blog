---
title:  "RL_Lecture9: Exploration and Exploitation"
date: 2017-12-28 00:00:00
layout: post
excerpt: David Silver의 Reinforcement Learning 9강을 정리한 내용이다.
categories: [Reinforcement Learning, Lecture]
comments: true
---

- David Silver의 Reinforcement Learning 9강을 정리한 내용이다.

9강에서는 Exploration과 Exploitation에 대해 다룬다. (요약 필요)
***
### Exploration vs Exploitation
이번 장에서 다룰 Exploration과 Exploitation 사이에는 딜레마가 존재한다. action에 대한 선택에 대해서 현재 정보를 토대로 최선의 선택을 할 수(Exploitation)있지만
때로는 장기적으로 더 큰 보상이 있을 수도 있으므로 현재 일부 손해를 감수하고 새로운 정보를 얻을 필요도 있다.(Exploration)

그럼 어떤 방법과 기준으로 Exploration을 하고 Exploitation을 해야 할 지에 대해 아래 내용들을 다룬다. (*세부 내용은 아래에서*)

![principles](https://whikwon.github.io/images/david_silver/principles.png)

### Multi-Armed Bandit
가장 간단한 예시인 Multi-Armed Bandit을 통해서 Exploration, Exploitation의 개념을 설명한다. Multi-Armed Bandit은 팔이 여러 개 달린
슬롯 머신에서 어떤 팔을 당겨야 가장 돈을 많이 얻을 수 있는지에 대한 문제이다. state가 따로 없으며 action에 대한 reward만 존재한다. 팔을 당겼을 때의 reward를 통해
각각의 팔에 대한 value function을 학습시키면 어떤 팔을 당길 때 가장 큰 누적 reward를 얻을 수 있을지 알 수 있다.

![multi armed bandit](https://whikwon.github.io/images/david_silver/multi_armed_bandit.png)

### Regret
학습을 잘하고 있는지 확인하기 위해서 regret이라는 것을 개념을 만든다. 우리가 optimal한 value function을 알고 있다고 가정하면 학습을 통해 얻은
value function와의 기대 차이 값(loss)을 통해 얼마나 잘 학습이 되었는지 알 수 있다. 이를 1-time step에 대해서 확인하는 경우 *regret*, 전체에 대해서
확인하는 경우 *total regret* 이라고 한다. (*대부분의 경우 optimal value function을 알 수 없으므로 간단한 예제를 통해서 봐야 한다.*)

![regret](https://whikwon.github.io/images/david_silver/regret.png)

위 식을 약간 다르게 표현할 수도 있다. action들에 대해 각각 실행되는 횟수를 $$N_t(a)$$(*count*)로, 해당 action들과 optimal action의
value function 값의 차이를 $$\Delta_a$$(*gap*)로 나타내면 위 식을 아래처럼 변경할 수 있다. 아래 식을 참조하면 기왕이면 gap이 큰 부분을
적게 count하도록 하는 것이 좋은 알고리즘이다.

![counting regret](https://whikwon.github.io/images/david_silver/counting_regret.png)

지금까지 배웠던 greedy, $$\epsilon$$-greedy 알고리즘의 regret을 나타내보면 linear하게 계속 total regret가 증가한다.
greedy의 경우엔 optimal 하지 않은 action을 계속 하기 때문이고 $$\epsilon$$-greedy의 경우엔 optimal 함에도 불구하고 계속 exploration을 하기 때문에
그렇다.

결국 문제 해결을 위해선 전체 state, action space를 최대한 활용해야 하고 특히나 확인되지 않은 space에 대한 uncertainty를 어떻게 다룰 지가 관건이 된다.
최대한 uncertainty를 줄이면서 충분히 확인된 space 내에서 optimal value function을 구하면 된다.

![linear regret](https://whikwon.github.io/images/david_silver/linear_regret.png)

### Random Exploration Algorithm
exploration의 방법에는 여러 가지가 있을 수 있다. uncertainty에 대해서 정의한 후 이를 바탕으로 하는 방법도 있을 수 있고 uncertainty를 배제하고
완전히 random하게 하는 방법도 있을 수 있다. 후자가 간단하니 먼저 살펴보도록 하자.

#### 1) $$\epsilon$$-Greedy Algorithm
앞 강의에서 계속 살펴본 알고리즘이다. action-value를 바탕으로 policy를 결정한다고 했을 때, state에 대한 action의 확률이 존재한다. 이 때, 가장 높은 확률의 action
이 선택될 확률을 $$1-\epsilon$$으로, 나머지 action들이 선택될 확률을 $$\epsilon$$으로 설정한다. 무작위로 action이 선택될 확률이 있기 때문에 전체 state, action space를
확인할 수 있다는 장점이 있다. 하지만, 어느 정도 학습된 뒤에서 계속해서 무작위 action을 하기 때문에 regret은 linear하게 증가한다.

#### 2) Optimistic Initialization
action-value를 학습을 위해 초기화할 때 높은 값으로 주고 Greedy 하게 학습하는 아주 간단한 방법이다. (*교재의 많은 코드 예제에서 주로 0으로 주곤 했다.*)
Greedy 하게 학습하는 경우에 가장 큰 문제는 전체 space를 다 보지 않고 적당히 본 뒤에 해당 policy에 수렴해버리는 것이다. 이 경우 많은 state, action space를 확인도 못하게 된다.
근데 action-value를 초기화 할 때 높은 값으로 줘버리면 아무리 좋은 action이라고 하더라도 action-value 값이 학습 중에 낮아질 수 밖에 없다. 그럼 확인하지 않은 state, action의
경우 action-value 값이 높을 것이므로 자연스럽게 전체 space를 일정한 수준까지는 반복해서 확인하게 된다.

대신, 처음 학습 구간에만 일시적으로 효과가 있으므로 non-stationary한 경우에는 효과를 발휘하기 어렵다.

#### 3) Decay $$\epsilon$$-Greedy Algorithm
학습 시간에 따라서 $$\epsilon$$을 줄여준다. 학습 초반 구간에서는 많은 space를 확인해야 하므로 다양한 action을 할 필요가 있지만
시간이 점차 지나고 전체 space를 충분히 확인한 경우에는 optimal에 가까운 action-value를 얻었다고 할 수 있다. 이 경우 무작위 action으로 인한
regret을 줄일 필요가 있고 점진적으로 $$\epsilon$$을 낮추어 0에 수렴하게 만들어준다. (*greedy*)

이 알고리즘을 통해서 regret을 sub-linear하게 만들 수 있다는 장점이 있다. 하지만, 어느 정도로 천천히(혹은 빨리) decaying 시킬 지에 대한 사전 지식이 필요하다는 단점이 있다.

![decaying epsilon](https://whikwon.github.io/images/david_silver/decaying_epsilon.png)

Reference: <br>
[David Silver RL Lecture8 slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf) <br>
