---
title:  "RL_Lecture9: Exploration and Exploitation"
date: 2017-12-28 00:00:00
layout: post
excerpt: David Silver의 Reinforcement Learning 9강을 정리한 내용이다.
categories: [Reinforcement Learning, Lecture]
comments: true
---

Exploration, Exploitation에 대해 설명하고 Optimal Value function 학습이 되지 않았을 때 계속해서 증가하는 regret을
해결하기 위한 방법을 크게 3가지 소개한다.
- Random Exploration Algorithm: 무작위로 action을 결정해서 state space를 explore한다.
- Optimism in the Face of Uncertainty: Uncertainty를 정의하고 이를 기준으로 action을 결정해서 state space를 explore한다.
- Information State Space

## Exploration vs Exploitation
이번 장에서 다룰 Exploration과 Exploitation 사이에는 딜레마가 존재한다. action에 대한 선택에 대해서 현재 정보를 토대로 최선의 선택을 할 수(Exploitation)있지만
때로는 장기적으로 더 큰 보상이 있을 수도 있으므로 현재 일부 손해를 감수하고 새로운 정보를 얻을 필요도 있다.(Exploration)

그럼 어떤 방법과 기준으로 Exploration을 하고 Exploitation을 해야 할 지에 대해 아래 내용들을 다룬다. (*세부 내용은 아래에서*)

![principles](https://whikwon.github.io/images/david_silver/principles.png)

## Multi-Armed Bandit
가장 간단한 예시인 Multi-Armed Bandit을 통해서 Exploration, Exploitation의 개념을 설명한다. Multi-Armed Bandit은 팔이 여러 개 달린
슬롯 머신에서 어떤 팔을 당겨야 가장 돈을 많이 얻을 수 있는지에 대한 문제이다. state가 따로 없으며 action에 대한 reward만 존재한다. 팔을 당겼을 때의 reward를 통해
각각의 팔에 대한 value function을 학습시키면 어떤 팔을 당길 때 가장 큰 누적 reward를 얻을 수 있을지 알 수 있다.

![multi armed bandit](https://whikwon.github.io/images/david_silver/multi_armed_bandit.png)

## Regret
학습을 잘하고 있는지 확인하기 위해서 regret이라는 것을 개념을 만든다. 우리가 optimal한 value function을 알고 있다고 가정하면 학습을 통해 얻은
value function와의 기대 차이 값(loss)을 통해 얼마나 잘 학습이 되었는지 알 수 있다. 이를 1-time step에 대해서 확인하는 경우 *regret*, 전체에 대해서
확인하는 경우 *total regret* 이라고 한다. (*대부분의 경우 optimal value function을 알 수 없으므로 간단한 예제를 통해서 봐야 한다.*)

![regret](https://whikwon.github.io/images/david_silver/regret.png)

위 식을 약간 다르게 표현할 수도 있다. action들에 대해 각각 실행되는 횟수를 $$N_t(a)$$(*count*)로, 해당 action들과 optimal action의
value function 값의 차이를 $$\Delta_a$$(*gap*)로 나타내면 위 식을 아래처럼 변경할 수 있다. 아래 식을 참조하면 기왕이면 gap이 큰 부분을
적게 count하도록 하는 것이 좋은 알고리즘이다.

![counting regret](https://whikwon.github.io/images/david_silver/counting_regret.png)

지금까지 배웠던 greedy, $$\epsilon$$-greedy 알고리즘의 regret을 나타내보면 linear하게 계속 total regret가 증가한다.
greedy의 경우엔 optimal 하지 않은 action을 계속 하기 때문이고 $$\epsilon$$-greedy의 경우엔 optimal 함에도 불구하고 계속 exploration을 하기 때문에
그렇다.

결국 문제 해결을 위해선 전체 state, action space를 최대한 활용해야 하고 특히나 확인되지 않은 space에 대한 uncertainty를 어떻게 다룰 지가 관건이 된다.
최대한 uncertainty를 줄이면서 충분히 확인된 space 내에서 optimal value function을 구하면 된다.

![linear regret](https://whikwon.github.io/images/david_silver/linear_regret.png)

## Random Exploration Algorithm
exploration의 방법에는 여러 가지가 있을 수 있다. uncertainty에 대해서 정의한 후 이를 바탕으로 하는 방법도 있을 수 있고 uncertainty를 배제하고
완전히 random하게 하는 방법도 있을 수 있다. 후자가 간단하니 먼저 살펴보도록 하자.

### 1) $$\epsilon$$-Greedy Algorithm
앞 강의에서 계속 살펴본 알고리즘이다. action-value를 바탕으로 policy를 결정한다고 했을 때, state에 대한 action의 확률이 존재한다. 이 때, 가장 높은 확률의 action
이 선택될 확률을 $$1-\epsilon$$으로, 나머지 action들이 선택될 확률을 $$\epsilon$$으로 설정한다. 무작위로 action이 선택될 확률이 있기 때문에 전체 state, action space를
확인할 수 있다는 장점이 있다. 하지만, 어느 정도 학습된 뒤에서 계속해서 무작위 action을 하기 때문에 regret은 linear하게 증가한다.

### 2) Optimistic Initialization
action-value를 학습을 위해 초기화할 때 높은 값으로 주고 Greedy 하게 학습하는 아주 간단한 방법이다. (*교재의 많은 코드 예제에서 주로 0으로 주곤 했다.*)
Greedy 하게 학습하는 경우에 가장 큰 문제는 전체 space를 다 보지 않고 적당히 본 뒤에 해당 policy에 수렴해버리는 것이다. 이 경우 많은 state, action space를 확인도 못하게 된다.
근데 action-value를 초기화 할 때 높은 값으로 줘버리면 아무리 좋은 action이라고 하더라도 action-value 값이 학습 중에 낮아질 수 밖에 없다. 그럼 확인하지 않은 state, action의
경우 action-value 값이 높을 것이므로 자연스럽게 전체 space를 일정한 수준까지는 반복해서 확인하게 된다.

대신, 처음 학습 구간에만 일시적으로 효과가 있으므로 non-stationary한 경우에는 효과를 발휘하기 어렵다.

### 3) Decay $$\epsilon$$-Greedy Algorithm
학습 시간에 따라서 $$\epsilon$$을 줄여준다. 학습 초반 구간에서는 많은 space를 확인해야 하므로 다양한 action을 할 필요가 있지만
시간이 점차 지나고 전체 space를 충분히 확인한 경우에는 optimal에 가까운 action-value를 얻었다고 할 수 있다. 이 경우 무작위 action으로 인한
regret을 줄일 필요가 있고 점진적으로 $$\epsilon$$을 낮추어 0에 수렴하게 만들어준다. (*greedy*)

이 알고리즘을 통해서 regret을 sub-linear하게 만들 수 있다는 장점이 있다. 하지만, 어느 정도로 천천히(혹은 빨리) decaying 시킬 지에 대한 사전 지식이 필요하다는 단점이 있다.

![decaying epsilon](https://whikwon.github.io/images/david_silver/decaying_epsilon.png)

### 4) Lower Bound
Decay $$\epsilon$$-greedy 알고리즘에서 sub-linear하게 특정 total regret으로 수렴할 수 있게 만들 수 있다는 것을 알았다. 그렇다면 total regret의 lower bound를 구할 수 있지 않을까?

Lower bound는 알고리즘이 얼마나 효과적으로 학습하느냐와 관련이 있다. (낮을 수록 빠르게 원하는 optimal에 도달할 수 있다.)
그럼, 학습이 잘 되는 경우를 고려해보면 Multi-Armed bandit의 경우 arm들의 reward 발생 분포가 크게 차이나는 경우이다.
분포가 거의 유사한 경우에는 optimal policy를 가리기 위해서 더 많은 시행착오를 겪어야 된다.

분포에 대한 유사성이므로 KL-divergence와 분포의 mean에 관한 식으로 나타낼 수 있고 lower bound는 아래 theorem으로 정의할 수 있다.

![lower bound](https://whikwon.github.io/images/david_silver/lower_bound.png)

## Optimism in the Face of Uncertainty
위에서 random하게 exploration을 하는 방법에 대해 살펴보았고 이제 action의 uncertainty를 다루는 방법에 대해 살펴보자.
아래 그래프는 action들에 대해서 어느 정도 action-value($$Q$$)를 가질지에 대한 확률 분포이다. 파랑은 넓게 분포되어 있으나 평균적인 $$Q$$는 낮고 초록은 평균적인 $$Q$$가 높지만 좁게 분포되어 있다. 빨강은 둘의 중간이다.

![optimism uncertainty](https://whikwon.github.io/images/david_silver/optimism_uncertainty.png)

학습할 때 이런 식으로 모든 action에 대한 분포가 나타날 것이고 계속 변할 것이다. ***이 때, 어떤 action을 선택해야 할까?***
Optimism in the Face of Uncertainty에서 얘기하는 것은 현재 가장 좋은 action(초록)이 아닌 가장 잠재력이 있는 action(파랑)을 선택하라는 것이다. 계속 반복하면 분포가 좁아질 것이고 해당 action에 대한 uncertainty가 줄어들 것이다. 이런 식으로 모든 action에 대해서 반복하면 결국 uncertainty를 확인하고 그 중에 가장 높은 $$Q$$를 반환해주는 action을 선택할 수 있게 된다.

그럼, **uncertainty를 어떻게 나타내고 어떤 기준으로 선택할 건지** 가 매우 중요하겠다. 아래에서 보도록하자.

### 1) Upper Confidence Bounds: uncertainty의 기준
위에서 본 것처럼 각 action마다 방문 횟수($$N_t(a)$$)에 따라서 $$Q_t(a)$$에 대한 확률 분포가 분산이 다르게 나타난다. 각 분포의 잠재력은 결국 분포의 꼬리에 해당하는 부분의 $$Q_t(a)$$의 크기에 달려 있다. 여기에 신뢰 구간(confidence bound)의 개념을 도입해서 예를 들면 95% 신뢰구간의 upper bound 값을 통해 각 action의 잠재력을 정의할 수 있을 것이다.

이렇게 정의한 UCB로부터 greedy하게 action을 선택하면 현재 상황에 uncertainty를 고려했을 때의 최선의 action을 얻을 수 있다.

![UCB](https://whikwon.github.io/images/david_silver/UCB.png)

아직 uncertainty를 나타내는 방법을 살펴보지 않았다. 방문 횟수에 따라 어떻게 분포를 나타내는지 살펴보자.

### 2) Hoeffding's inequality: uncertainty를 나타내는 방법
uncertainty를 나타내기 위해서 Hoeffding's inequality를 활용한다. 내용은 random하게 i.i.d한 변수를 샘플링했을 때 샘플의 mean($$\bar X_t$$)으로부터 $u$만큼 떨어진 값보다 true mean($$\mathbb{E}[X]$$)이 클 확률을 구할 수 있다는 것이다.

즉, 우리가 지정한 신뢰 구간 경계의 mean보다 action의 $$Q$$의 true mean이 클 확률($$\mathbb{P}[Q(a) > \hat Q_t(a) + U_t(a)]$$)을 구할 수 있게 되는 것이다. (*이 경우에 분포를 통해 고려한 잠재력을 확인했다고 볼 수 있다.*)

![hoeffding's inequality](https://whikwon.github.io/images/david_silver/hoeffding_inequality.png)

위에서 구한 확률($$p$$) 값을 바탕으로 $$U_t(a)$를 구해서 action을 결정하는데 사용할 수 있도록 하자. 같은 action에서 확률이 높은 경우에 $$U_t(a)$$가 낮아질 것이고 역도 성립한다. action마다 동일한 신뢰 구간을 사용하기 위해 $$p$$를 고정하면 $$U_t(a) = \sqrt {\dfrac {-\log p} {2N_t(a)}}$$식을 정의할 수 있다. 동일한 신뢰 구간 내에서 방문 횟수가 많아질 수록 UCB가 작아질 것이다.(*분산이 작아진다.*)

여기에 시간에 관한 항을 추가하자. 최종적으로 우리가 알고 싶은 건 95%가 아닌 100% 신뢰 구간에서의 UCB이므로 시간에 따라 $$p$$값을 낮춰준다. 그럼, 학습이 계속됨에 따라 아주 낮은 잠재력의 가능성까지도 다 고려하게 되고 모든 uncertainty를 고려한 optimal policy를 구할 수 있게 된다.

![UCB2](https://whikwon.github.io/images/david_silver/UCB2.png)

### 3) UCB1
위 모든 내용을 종합한 식 $$a_t = \underset {a \in A} {argmax} Q(a) + \sqrt {\dfrac {2\log t} {N_t(a)}}$$
이 **UCB1 알고리즘** 이며 practical하게 잘 작동한다고 한다. UCB1 알고리즘으로부터 total regret이 logarithmic asymptotic 함을 보일 수 있다.

![UCB1](https://whikwon.github.io/images/david_silver/UCB1.png)

UCB와 $$\epsilon$$-greey 알고리즘을 비교한 결과이다. UCB가 더 좋은 성능을 내는 것을 볼 수 있다.

![UCB vs epsilon greedy](https://whikwon.github.io/images/david_silver/UCB_vs_egreedy.png)

## Bayesian Bandits
위에서 본 내용은 reward 분포에 대한 prior가 없다는 가정에서 exploitation, exploration을 진행했다. 그럼, reward 분포에 대한 prior를 가정하면 어떻게 될까?

Bayesian 접근 방법의 경우 prior 분포를 가정하고 여기에 경험이 반영되는 식으로 진행이 되므로 처음에 $$p[R]$$의 prior를 가지고, history $$h_t = a_1, r_1, ..., a_{t-1}, r_{t-1}$$가 더해지면서 posterior 분포인 $$p[R \lvert h_t]$$로 update 된다.

prior 분포가 true reward 분포와 아주 유사하다면 점차적으로 posterior 분포는 true reward에 가까워질 것이다.

$a$


Reference: <br>
[David Silver RL Lecture9 slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/xx.pdf) <br>
