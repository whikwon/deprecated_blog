---
title:  "RL_Lecture7: Policy Gradient"
date: 2017-12-18 00:00:00
comments: true
---

- David Silver의 Reinforcement Learning 7강을 정리한 내용이다.

이번 장에서는 Policy Gradient라는 방법을 통해서 최적의 policy를 찾는 방법을 배운다. 이전 장에서는 experience를 통해 value function을 어떻게 구할 건지에 대해 초점을 맞췄었다면(Bellman equation으로 TD, MC..) 이 장에서는 experience로부터 어떻게 최적의 policy를 구할건지에 대해 살펴본다. 그리고 policy gradient에서 나타나는 variance, 학습 속도 문제를 해결하기 위한 Actor-Critic, Natural policy gradient를 살펴본다. 

***
## Policy-based RL 소개
이전 장에서는 주로 단순한 문제의 경우는 곧바로 $$V^{\pi}(s), Q^{\pi}(s, a)$$를 구했고 복잡한 경우는 $$V_{\theta}(s) \approx V^{\pi}(s), $$Q_{\theta}(s, a) \approx Q^{\pi}(s, a)$$
로 근사해서 풀었다. 그렇게 value function을 구하면 policy는 $\epsilon$-greedy에 의해서 생성할 수 있었다.

이번 장에서는 **policy를 \theta에 대해** 나타낸 뒤 직접 학습시킬 것이다.($$\pi_{\theta}(s, a) = \mathbb{P}[a \lvert s, \theta]$$) 이번 장에서 다룰 내용은 모두 **Model-free** RL에 관한 내용이다.

큰 범주로 나눴을 때 아래 diagram처럼 Value based, Policy based, Actor-Critic으로 나뉘며 **Actor-Critic** 은 나머지 둘을 섞은 개념이라고 보면 된다. (*뒤에서 자세히 볼 예정이다.*)

![value based or policy based RL](https://whikwon.github.io/images/david_silver/value_policy_based.png)

### 1) 장점
Policy-based RL은 기존에 배웠던 학습 방법과 비교했을 때 아래와 같은 장,단점이 있다.

![policy RL adv, disadv](https://whikwon.github.io/images/david_silver/policy_advantage.png)

### 2) 예시1: 가위바위보
위 장점에서 봤던 것처럼 stochastic policy를 배울 수 있다. 가위바위보에 학습시키는 경우에는 Nash equilibrium에 의해서 uniform random policy가 최적의 policy로 구해진다.

### 3) 예시2: Aliased Gridworld
아래 예제는 Markov preperty가 성립하지 않는 partially observed MDP이며 회색 칸에 대해 구별할 수 없는 상황이라고 가정하자.
이 때, 어떤 방식으로 deterministic하게 policy가 학습되는 경우에 회색 칸들끼리 바뀔 가능성이 있기 때문에 한 쪽이 reward로 잘 찾아갈 경우에 다른 한 쪽은 절대 못 찾는 경우가 발생한다.
value based method의 경우에는 value function을 구한 뒤 policy는 greedy하게 가져오기 때문에 deterministic하게 될 수 밖에 없어서 이런 경우 양쪽에서 모두 goal을 찾아 가도록 학습시키는 것이 매우 어렵다.
하지만 policy는 stochastic하게 policy에 대해 학습하기 때문에 아래 그림처럼 양쪽 모두 goal로 어느 정도 확률적으로 도달하는게 가능하도록 학습이 된다.

![aliased gridworld](https://whikwon.github.io/images/david_silver/aliased_gridworld.png)

### 4) Objective function
Policy gradient의 학습은 최적화 문제를 푸는 방법으로 진행된다. 그러므로 학습의 방향에 대한 목적 함수가 필요하고 이를 정의하도록 하자.
optimal policy는 episode의 시작과 끝이 있는 경우엔 시작~끝까지의 reward가 높게 학습되어야 하므로 시작 지점에 대한 value function 값, $$V_{\pi_{\theta}}(s_1)$$으로 목적 함수를 설정할 수 있따.
continuing environment의 경우에는 어디에서도 시작할 수 있기 때문에 처음 state에 대한 확률을 고려해서 average value function을 목적 함수로 둘 수 있다.
또는, time-step 별로 average reward를 목적 함수로 둘 수도 있다. (*value로 두는게 나을거 같은데 왜 reward로 둬도 되는거지?*)

![policy objective funtion](https://whikwon.github.io/images/david_silver/policy_objective_function.png)

### 5) Optimization
최적화 문제를 풀기 위해서 위에서 목적 함수는 정의했으니 이제 어떤 방법으로 학습할지 정해주면 된다. 주로 gradient를 사용하고 몇몇 경우에는 사용하지 않는다.
gradient를 사용했을 때, 그 중에서도 gradient descent가 효율적이므로 이에 대해서만 다룬다고 한다.

![policy optimization](https://whikwon.github.io/images/david_silver/policy_optimization.png)

## Policy Gradient
현재 위치에 대해서 구하고자 하는 parameter $$\theta$$에 대한 목적 함수 $$J$$의 gradient를 구한 뒤 적정한 step size $$\alpha$$만큼 paramter를 update 시켜준다.

![policy gradient](https://whikwon.github.io/images/david_silver/policy_gradient.png)

보통 analytic하게 구하는 방법을 사용한다. numerical하게 구할 수도 있긴 한데 policy가 non-differentiable한 경우를 제외하곤 연산량도 너무 많고 정확하지도 않기 때문에
사용하지 않는다. (*강의에서는 AIBO를 numerical한 방법으로 학습시키는 소개 정도만 한다.*)

### 1) Score function
policy가 differentiable하다고 가정하고 policy에 대한 gradient를 토대로 **likelihood ratio** 라는 trick을 사용해서 **score function** 을 구할 수 있다. score function은
앞으로도 계속 사용될 것인데 supervised learning에서 log likelihood로 parameter를 구할 때 사용된다.

![score function](https://whikwon.github.io/images/david_silver/score_function.png)

policy는 정의하기 나름이다. Softmax, Gaussian으로 policy를 나타냈을 때의 **score function** 는 아래와 같이 나타낼 수 있다.

![softmax policy](https://whikwon.github.io/images/david_silver/softmax_policy.png)

![gaussian policy](https://whikwon.github.io/images/david_silver/gaussian_policy.png)

그럼, score function도 구했으니 이제 가장 간단한 상황에서 어떤 식으로 policy gradient를 사용할 지 생각해보자. start, terminal 2개의 state만으로 구성된
one-step MDP를 생각해보면 움직임이 하나 밖에 없으므로 reward로 하나로 아래와 같이 식을 나타낼 수 있을 것이다. 근데, 위에서 목적 함수를 나타낼 때 Value 값, average reward 등을
고려했었는데 one-step이 아닌 multi-step의 경우에는 당장 다음 reward 보다는 좀 더 장기적인 관점이 필요할 것 같다. 아래에서 일반화된 theorem을 살펴보자.

![onestep MDP](https://whikwon.github.io/images/david_silver/one_step_MDP.png)

### 2) Policy Gradient Theorem
one-step에서 multi-step MDP로 일반화하기 위해서 단기적인 하나의 reward보다 장기적인 관점에서 보는게 필요하다. 그래서 reward($$r$$)를 action-value function($$Q^{\pi}(s,a)$$)으로 바꿔준다.
![PG theorem](https://whikwon.github.io/images/david_silver/PG_theorem.png)

### 3) Monte-Carlo Policy Gradient
policy gradient를 MC 방법으로 풀면 아래 pseudo code와 같이 학습시킬 수 있다. MC이므로 끝이 있는 episode에 대해서만 적용 가능하고 이전에 value-based 방법에서와 같이
전체 episode를 살피면서 time step별 return($$G_t$$)를 구한 뒤 update 시켜주는 방식이다.

![MC_PG](https://whikwon.github.io/images/david_silver/MC_PG.png)

### 4) 예시: Puck World
policy gradient로 학습한 예시이다. puck이 돌아다니면서 도착지점을 찾아야 하며 continuous action으로 puck에 힘이 가해진다. 도착점에 가까울 수록 reward가
주어지고 30초마다 도착점의 위치는 reset 되며 아래는 MC policy gradient 방법으로 학습시킨 결과이다. value-based와 비교했을 때 살펴 볼 내용이 2가지가 있다. 첫번째는 learning curve가 smooth하게
얻어진다는 점이고, 두번째는 학습을 위해서 훨씬 많은 iteration이 필요하다는 점이다. iteration이 많아지면 비효율적인 학습을 의미하므로 이를 해결할 수 있는 방안을 살펴 볼 예정이다.

![puck world](https://whikwon.github.io/images/david_silver/puck_world.png)

### 5) Policy Gradient의 문제점
Policy Gradient 방법은 2가지 문제를 가진다. 먼저, 높은 variance를 가진다. 아래 그림을 policy sample에 대한 reward를 나타낸 그래프라고 생각해보자.
초록색, 노란색을 policy sample이라고 보면 초록색의 경우에는 reward가 음수/양수를 가지므로 음수를 기피하는 쪽으로 variance가 낮게 학습이 될 것이고 노란색의 경우에는 reward가 둘 다 양수를 가지면서 값의 차이만 존재하므로 variance가 크게 학습이 될 것이다. 이 문제는 baseline을 통해서 기준점을 정해주고 음수에 대해서는 기피하고 양수에 대해서 variance를 낮게 학습할 수 있다.

두번째로는 학습 속도의 문제가 있는데 위에서도 본 것처럼 value-based 대비 학습 iteration이 크게 필요했다. 이유는 [Peters & Schaal 2008](https://www.ncbi.nlm.nih.gov/pubmed/18482830
)을 살펴보자. 그래서 이를 해결하기 위해서 natural gradient라는 방법을 도입한다고 한다.

![PG variance](https://whikwon.github.io/images/CS294_problem_of_policy_gradient.png)

## Actor-Critic
Policy Gradient의 문제점들을 봤으니 이를 해결해야겠다. 방법으로 나온 것이 Actor-Critic이며 위 diagram에서 본 것처럼 Policy gradient에 Value-based를 섞어서 문제점들을 해결한다. Actor-Critic은 이름처럼 Actor(배우)와 Critic(비평가)로 나뉜다. Actor는 policy에 관련된 parameter $$\theta$$를 update하는 역할을 하는데 update 방향에 대해서는 Critic이 제시하는 식이다.

그럼 기존 policy gradient에서 어떤 점이 바뀐 것일까? PG는 $$Q, \pi$$ 모두 $$\theta$$에 관한 식으로 update, 방향 모두 하나의 parameter에만 의존하고 있다. 이를 Actor-critic에서 방향을 제시하는 action-value function $$Q^{\pi_{\theta}}$$를 새로운 parameter $$w$$에 관한 식으로 근사해서 update 식과 방향에 대한 식을 분리해버린다.

$$Q_w$$를 구하는 방법은 기존에 policy evaluation 방법(MC, TD.. + function approximation)을 사용하면 된다.

Critic은 근사한 값이므로 bias를 어느 정도 희생하게 되지만 TD와 같은 방법을 사용하게 되면 policy gradient의 문제점인 variance를 낮출 수 있어서 유용하다.

![Actor Critic](https://whikwon.github.io/images/actor_critic.png)

### 1) Action-Value Actor-Critic
위에서 설명한 것처럼 간단한 linear value function approximation을 사용해서 Critic $$Q_w(s,a)$$를 근사할 수 있고  Critic은 TD(0)를 사용해서 parameter를 update하고 Actor는 policy gradient로 update할 수 있다. QAC라는 알고리즘이라고 부르며 아래와 같다.

![QAC](https://whikwon.github.io/images/QAC.png)

### 2) bias, 수렴 가능 여부
Critic는 근사이기 때문에 bias가 생기게 된다. 이 때 Critic이 제시하는 방향이 true policy gradient의 방향과 일치해서 결국 global optimum으로 수렴 가능한지 따지는 것이 매우 중요하다. 결론적으로 적절한 value function approximator(**compatible function approximation**)를 골랐을 때 bias가 없도록 할 수 있다. 그리고 이 때, policy gradient를 완전히 따른다고 한다.(**아래 Theorem**) 그래서 tabular나 단순한 softmax와 같은 approximator의 경우에는 global optimum으로 수렴한다. 하지만 Neural Net과 같은 general function approximator는 대부분 local optimum에 빠진다.

![compatible_function_approx](https://whikwon.github.io/images/compatible_function_approx.png)

### 3) Baseline
policy gradient의 variance 문제를 언급하면서 잠깐 나왔던 내용이다. gradient를 단순히 $$Q^{\pi_{\theta}}(s,a)$$를 놓고 구하게 되면 $$Q^{\pi_{\theta}}(s,a)$$의 값이 제각각이기 때문에 어떤 방향으로 학습해야 할 지 헤매게되어 variance가 높아진다.(학습이 갈 길을 못 찾고 느리게..) 그래서 $$Q^{\pi_{\theta}}(s,a)$의 action에 대한 expectation 값인 $$V^{\pi_{\theta}}(s)$$를 빼준다. 그럼, 값이 양수/음수로 확실하게 나눠지기 때문에 양수인 방향으로 집중적으로 variance가 낮게 학습할 수 있게 된다. 이 때 좋은 점은 gradient를 구하는 과정도 expectation이기 때문에 상수를 빼주더라도 unbiased하므로 쉽게 variance만 낮출 수 있다는 점이다.

![baseline](https://whikwon.github.io/images/baseline.png)

### 4) Advantage Function
위에서 살펴본 baseline의 이점을 활용하기 위해서는 state-value function이 추가적으로 필요하다. 그래서 action-value function을 근사한 것처럼 state-value function도 근사해서 사용하는 방법을 사용할 수 있다. $$V_v(s) \approx V^{\pi_{\theta}}(s), Q_w(s,a) \approx Q^{\pi_{\theta}}(s,a), A(s,a) = Q_w(s,a) - V_v(s)$$

새로운 parameter를 도입하는 것도 좋지만 가능하면 하나로 해결할 순 없을까? $$V^{\pi_{\theta}}(s)$$에 대한 TD error를 구하면 가능하다. 아래와 같이 식을 전개하면 되며 최종적으로는 $$v$$에 대한 parameter만 구하면 Actor-Critic으로 문제를 풀 수 있다. (*가장 흔하게 많이 사용하는 방법이라고 한다.*)

![advantage function](https://whikwon.github.io/images/advantage_function.png)

### 5) Eligibility traces
Actor나 Critic에 여러가지 time scale (MC, TD, ...)에 대해서 적용하는 것도 가능할까? 가능하다. 먼저 Critic에 대해서 살펴보면 아래는 6강에서 Value function을 근사할 때 나타낸 식들로 여러 가지 time-scale에 대해 나타낸 것을 살펴보았다. (여기에서 $$\theta$$는 **value function에 대한 parameter**, $$\phi$$는 table lookup feature이다.)

![AC Eligibility](https://whikwon.github.io/images/AC_eligibility.png)

다음은, Actor에 관해서 살펴보면 Critic에서와 동일한 방법으로 $$\phi(s_t)$$만 $$\nabla_{\theta} \log \pi_{\theta}(s,a)$$로 변경해서 식을 전개한 뒤 **policy에 대한 gradient**를 구할 수 있다.** 바꿔주는 이유는 아마도 gradient의 값이 시간에 따른 state마다 다르게 구해지기 때문에 이를 고려할 필요가 있기 때문이다.

![AC Eligibility](https://whikwon.github.io/images/AC_eligibility2.png)
![AC Eligibility](https://whikwon.github.io/images/AC_eligibility3.png)

### 6) Natural Policy Gradient
SGD를 통해 update할 경우에 gradient descent로 이동할 거리를 측정한 다음에 update를 해주게 된다. 이 때, policy의 parameter가 어떤 분포(*e.g. Gaussian*)를 띄고 있다고
가정하면 제대로 된 거리를 책정하기가 어렵다. 왜냐면 분산이 작은 경우에 조금 이동해도 변화가 크고 반대로 분산이 큰 경우에는 많이 이동해도 변화가 작기 때문이다. 이런 경우에 거리를 측정
하기 위해서는 Statistical 거리가 필요하고 이 때 사용하는 것이 분포가 얼마나 다른지 나타내주는 KL-Divergence가 있다.

이것저것 유도 과정이 있지만 결론적으로 KL-Divergence의 second derivative인 Fisher information matrix을 metric으로 사용해서 natural gradient는 Fisher information matrix를
기존 gradient에 곱한 값으로 사용한다.

강화 학습에서 사용할 때 어떤 이유에서 natural gradient가 필요한지 강의에서 설명하는데 이해가 잘 안 되서 정리되는대로 다시 올리도록 하자.

![natural gradient](https://whikwon.github.io/images/natural_policy_gradient.png)

### 7) Natural Actor-Critic
아래는 policy gradient와 마찬가지로 Actor-critic에 natural gradient를 적용한 것이다. 참고만 하자.

![natural AC](https://whikwon.github.io/images/natural_AC.png)


Reference: <br>
[David Silver RL Lecture7 slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)
Sutton & Barto. (2017) Reinforcement Learning: An Introduction (pp. 271)
[Natural Gradient 설명 Blog](http://kvfrans.com/what-is-the-natural-gradient-and-where-does-it-appear-in-trust-region-policy-optimization/)
