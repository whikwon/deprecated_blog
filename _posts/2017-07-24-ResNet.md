---
title:  "ResNet"
date: 2017-07-24 00:00:00
comments: true
---

- MS에서 발표한 ILSVRC 2015에서 1등한 모델이다. 그 외에 다른 대회들도 휩쓸다시피 했다.
- 이전 구조들에 비해 망의 깊이가 깊어졌다.

***
### 1. Introduction<br><br>
모델을 학습시킬 때 Network depth가 성능에 매우 중요한 역할을 한다는 사실을 몇 년간 알아냈다.
depth를 늘렸을 때 문제가 2가지 발생하며 첫번째는 고질적인 vanishing/exploding gradient 문제이다.
이를 해결하기 위해 initialization, normalization 등의 기법이 사용되고 있다.
두번째는 degradation 문제이다. depth가 어느 정도 이상 증가하면 성능이 saturate되고 빠르게 degrade되는 현상이다.
degradation이 일어난 후에도 높은 성능을 보여 overfitting에 의한 것은 아닌 것으로 보이며 depth와의 연관성을 확인하기 위해
identity function을 추가하여 실험을 했을 때 성능이 안 좋아져서 어느 정도 관련이 있을 것이라고 얘기한다.
아래는 degradation이 나타나는 모습을 보여주는 그림이다. depth가 깊어졌으나 성능이 더 낮아짐을 확인할 수 있다.
![Figure1](http://yanran.li/images/resnet_1.png) <br><br>
degradation 현상을 해결하기 위한 방법으로 ***deep residual learning*** framework를 소개한다.
기존 network에 ***shortcut connection*** 가 추가하는 방법으로 residual learning을 수행하며
연산량의 증가없이 end-to-end BP를 가능하게 할 수 있다는 장점이 있다.  
또한, 위에서 고안한 ***shortcut connection*** 을 적용시키면서 더 깊은 망을 학습시키고 좋은 성능을 낼 수 있도록 하였다.
자세한 내용은 뒤에서 보도록 하겠다.

***
### 2. Deep Residual Learning<br><br>
  - Residual learning<br>
  기존 network는 아래 그림과 같다. <br>
  ![Fig2_1](https://3.bp.blogspot.com/-sSYC7XJ5O0Q/V9t_aUMqzLI/AAAAAAAAB6c/mO4V63zpVHggJpKKQl5HbJHTJNiaZhBpgCEw/s200/%25E1%2584%2591%25E1%2585%25A1%25E1%2584%258B%25E1%2585%25B5%25E1%2586%25AF%2B2016.%2B9.%2B16.%2B%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB%2B9%2B09%2B19.jpeg) <br><br>
  여기서 multiple non-linear layer가 복잡한 function을 근사할 수 있으니 residual function($$\mathcal{F}(x) := \mathcal{H}(x)-x$$)도 근사할 수 있다고
  가정하고 내용을 진행하면 아래 그림과 같이 식이 바뀌게 된다. <br>
  ![Fig2_2](https://2.bp.blogspot.com/-wYlWsLwgR7o/V9t_aas4lII/AAAAAAAAB6c/2B-vn0_Iq64xtiofEe4O0tsDfOai-S8TgCEw/s320/%25E1%2584%2591%25E1%2585%25A1%25E1%2584%258B%25E1%2585%25B5%25E1%2586%25AF%2B2016.%2B9.%2B16.%2B%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB%2B9%2B09%2B50.jpeg) <br><br>
  위에서 말한대로 $$x$$가 입력되어 $$\mathcal{H}(x)$$을 출력하는 구조에서 $$\mathcal{H}(x) - x$$을 출력하도록 식을 변경한다.
  $$\mathcal{F}(x) := \mathcal{H}(x)-x$$로 놓고, shortcut connection을 추가하면 최종적으로 위 그림과 같이
  $$x$$를 입력했을 때 $$\mathcal{F}(x)+x$$를 출력하는 구조의 식을 갖게 된다.
  여기서 identity mapping, 즉 $$x$$을 최적해로 갖는다면 $$\mathcal{F}(x)$$는 0이 되어야 하므로 network의 multiple non-linear layer의 weight는 0에 가까운
  값을 갖게 될 것이고 이런 특성으로 인해서 perturbation에 유리하게 된다.
  (*앞의 실험에서 봤을 때 identity mapping을 넣어줬는데 실제로 망 내에서 진행은 그렇게 진행되지 못해서 degradation 문제가 생긴걸로 보고 있다.
    그래서 구조를 약간 바꿔줘서 identity mapping이 최적일 때 identity로 정말 갈 수 있도록 변경해주자라고 접근해서 위와 같은 구조를 만든게 맞나?*)

  - Identity mapping by shortcuts<br>
  위 그림에서 본 building block의 식을 다시 한번 보면 $$y = \mathcal{F}(x, {W_i}) + x$$이다.
  shortcut connection은 연산량을 거의 증가시키지 않으며 $$\mathcal{F}$$는 flexible해서 layer의 개수가
  여러개가 들어가도 된다. 대신 1개만 가질 경우에는 linear layer가 되어서 이점이 사라진다.

  - Network architectures<br>
    - Plain network <br>
    VGGNet의 기본적인 구조를 가져왔다. Convolution layer는 거의다 3x3 filter로 구성되고 block 내에서는 output이 크기를 동일하게 하기 위해서 filter를 같게 놓았다.
    그리고 feature-map size가 작아질 경우에는 filter수를 2배로 증가시켰다. (*time complextipy per layer를 보존하기 위해서라는데 무슨 의미?*)
    network의 마지막 부분엔 global average pooling, 1000-d FC, softmax를 사용했다.
    depth가 VGGNet보다 훨씬 깊지만 filter수나 complexity 모두 적다는 점을 유념해서 볼만하다.
    ResNet의 구조는 아래와 같다.
    ![Figure3](https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAyaAAAAJGM0ODdmMjY4LTgyNzctNGFmYS04MDYzLTAxNDE5NDM2NzhkMQ.png)<br>
    ![table1](https://raw.githubusercontent.com/raghakot/keras-resnet/master/images/architecture.png?raw=true) <br>

    - Residual network<br>
    shortcut connection을 사용하는데 주의할 점이 있다.
    dimension이 변하는 경우 해당 부분을 identity mapping에 어떻게 처리해줄 것인지가 관건인데 먼저 하나는 zero padding 해주는 방법과
    다른 하나는 projection shortcut($$W_s$$)을 해주는 방법이 있다. 두 방법 모두 pooling을 해주어 input size를 맞춰주었다.

  - Implementation<br>
    - training <br>
    [256, 480]에서 shorter side randomly sampled해서 scale augmentation, 224X224 randomly cropped, horizontal flip, per-pixel mean subtracted, standard color augmentation, batch normalization,
    no pretraining, SGD with mini-batch(256), no dropout. <br>
    - test <br>
    10-crop testing, average the scores at multiple scales

***
### 3. Experiments <br><br>
  - Imagenet classification <br>
  아래 결과에 대해서 얘기하도록 하겠다.
  ![Figure4](http://postfiles15.naver.net/20160718_286/laonple_1468811198847PDYa5_PNG/%C0%CC%B9%CC%C1%F6_81.png?type=w2) <br>
    - Plain networks <br>
    plain network일 때 layer가 18에서 34로 증가했을 때 성능이 오히려 나빠졌다. 하지만 34개 layer일 때도 성능이 좋은 편이고 training시에
    batch normalization도 수행되서 vanishing gradient나 overfitting이 원인으로 보긴 어렵다고 추측하며 degradation에 대한 정확한 원인은 아직 모른다고 한다.

    - Residual networks <br>
    residual network에선 layer가 18에서 34로 증가했을 때 성능이 좋아졌으며 결과로부터 세가지 중요 사항에 대해 알 수 있게 되었다.
    첫째, 깊은 network에 대한 degradation 문제가 어느 정도 해결된 것으로 보인다.
    둘째, 34-plain과 비교해보았을 때, residual learning의 효과가 검증되었다.
    셋째, 18-plain와 18-res를 비교했을 때, 성능에서 차이가 없지만 수렴속도가 더 빠르다. 이는 ResNet이 optimization을 완화시켜서 빠르게 수렴하는 것으로 생각된다.

    - Identity vs Projection shortcuts <br>
    shortcut의 종류에 따른 성능 비교 결과는 아래와 같다.
    ![table3](http://img.blog.csdn.net/20161111194718570) <br> <br>
    A는 zero-padding shortcut, B와 C는 projection shortcut이다. (B는 increasing dimension에 대한, C는 전체에 대함)
    각각의 방법에 따라 연산량의 차이가 있으며 성능에 대해서는 shortcut 별로 차이는 있지만 모두 degradation 발생은 하지 않는다.

    - Deeper bottleneck architecture <br>
    앞서 소개한 building block을 약간 수정해서 *bottle neck architecture* 라고 명명했다.
    구조의 차이는 아래와 같다.
    ![Figure5](https://i.stack.imgur.com/kbiIG.png) <br><br>
    기존 3X3 대신에 1X1 2개 layer를 더해서 depth를 조절할 수 있게 만들었다. 주요 목적은 연산량을 감소시키기 위함이고
    non-linearity는 보너스(?)
    더 깊은 network를(50/101/152) 학습할 때 bottleneck architecture를 사용했고 성능이 depth가 커질수록 좋아지는 것을 확인했다.

    - CIFAR-10 and analysis <br>
    CIFAR-10 dataset으로 진행한 내용을 설명하고 있다. 앞에서 설명한 내용과 거의 동일하다.
    흥미로운 실험 내용이 하나 나오는데, layer를 1000개 이상으로 만들어서 학습 진행했을 때 training error가 0.1% 미만으로
    110-layer와 비슷한 수준으로 나왔다. test error는 7.93%로 높게 나와서 overfitting이 일어난 듯하고 이를
    regularizer로 어떻게 해결할 지에 대한 내용을 더 연구하면 더더 깊은 network의 좋은 성능을 이끌어낼 수 있지 않을까 싶다.



Reference: <br>
He, Kaiming, et al. ["Deep residual learning for image recognition."](https://arxiv.org/pdf/1512.03385.pdf) arXiv preprint arXiv:1512.03385 (2015)
