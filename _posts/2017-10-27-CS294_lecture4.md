---
title:  "CS294_lecture4"
date: 2017-10-27 00:00:00
comments: true
---

- Berkeley 대학의 2017년도 가을학기 강의인 CS294: Deep Reinforcement Learning 의 4강을 정리한 내용이다.

## 살펴볼 내용
1. Policy gradient algorithm에 대해서
2. Basic variance reduction
  - causality
  - baselines
3. Policy gradient 예제
- 목표:
  - 강화 학습의 알고리즘 중 하나인 policy gradient 이해하기
  - policy gradient에 대해 실용적으로 고려해야 할 점들 이해하기

## Evaluating the objective
앞에서 배운 목적 함수를 다시 상기시켜보자. 우리가 원하는 목적은 모델의 parameter $$\theta^{\star}$$가 모든 state, action에 대한
reward의 expectation이 최대를 갖는 것이다. 아래 식에서 $$argmax$$ 뒤 항은 결국 loss인 $$J(\theta)$$를 나타내고,
Monte Carlo를 사용해서 전체가 아닌, sample에 대한 식으로 근사하도록 하자. sample이 많으면 많을수록 가까워진다. <br><br>
![evaluating the objective](https://whikwon.github.io/images/CS294_evaluating_the_objective.png)
<center> <i> &lt;목적 함수 평가&gt;</i> </center> <br>

## Direct policy differentiation
목적 함수인 $$J(\theta)$$ 식을 변형해서 아래와 같이 전개하자. 아래 전개의 목적은 소 제목에도 나와 있듯이 목적 함수의 $$\theta$$에 대한 미분 값을 구하는
것이다. 쭉 전개를 해서 아래 식을 얻고, expectation 안의 첫번째 항은 policy에 log를 취한 값의 $$\theta$$ 대한 미분 값의 합을 나타내며
두번째 항은 reward의 총 합을 나타낸다. 여기서 알 수 있는 내용은 policy의 gradient를 구하기 위해서 policy sampling하면 되며 기타 다른 것들이 필요없다는 점이다. <br><br>
![policy_differentiation](https://whikwon.github.io/images/CS294_direct_policy_differentiation.png)
![policy_differentiation2](https://whikwon.github.io/images/CS294_direct_policy_differentiation2.png)
<center> <i> &lt;direct policy differentiation 수식 전개&gt;</i> </center> <br>

## Evaluating the policy gradient
위에서 수식을 쭉 전개했는데 걸리는 부분이 전체 Policy에 대해서 다 봐야한다는 점이다. 이 부분을 위에서 목적함수 근사할 때 본 것처럼 Monte Carlo를 이용해서
근사한다. 변경되는 부분은 expectation의 전체 policy에 대한 부분($$\pi_{\theta}(\tau)$$)이다. 이렇게, sample을 생성해서 gradient를 구하고
policy를 개선하는 순서대로 반복 진행된다. (*알아야 할 조건들도 많이 없어서 매우 간단하다.*) <br><br>
![evaluating_the_policy_gradient](https://whikwon.github.io/images/CS294_evaluating_the_policy_gradient.png)
<center> <i> &lt;evaluating the policy gradient 수식 전개&gt;</i> </center> <br>

강의에서 위 식과 $$\pi_{\theta}(a_t|s_t)$$에 대한 Maximum likelihood와 비교하는 내용이 나오는데 Maximum log likelihood를 수식으로 표현하면
$$\nabla_{\theta} J_{ML}(\theta) \approx \frac 1 N \displaystyle \sum_{i=1}^N \bigg (\displaystyle \sum_{t=1}^T \nabla_{\theta} log \pi_{\theta} (a_{i, t}|s_{i, t}) \bigg)$$
으로 위 식의 앞의 항과 같은 내용을 나타낸다. 해석하자면 policy에 대해서 확률이 가장 높은 방향(*trajectory*)으로 학습이 될 것이고, reward가 고려될 것이라고 생각할 수 있다.

## 예시: Gaussian policies
아래는 위 policy gradient의 예시로 gaussian policy를 들었다. policy를 정의할 때 무슨 식이 들어가느냐에 따라서 학습이 결정되고
여기에 gaussian을 넣어준 것이 gaussian policy이다. neural net과 gaussian을 합쳐서 나타낸 것 외에 특이한 점은 없고 학습 방법은
이전에 설명한 내용과 동일하다. <br><br>
![Gaussian policies](https://whikwon.github.io/images/CS294_Gaussian policy.png)
<center> <i> &lt;예시: gaussian policy&gt;</i> </center> <br>

## Partial observability
지금까지 본 식은 state($$s_t$$)와 observation($$o_t$$)이 같다고 가정한 상태(*fully observability*) 였고 만약 partial하다고 하면 위에 유도한 식에서 state만 observation으로
변경해주면 된다. Markov property가 사용되지 않아서 변경해줘도 아무 문제가 없다고 한다. (*이해 필요.*)

## Policy gradient의 문제점
이렇게 쉽게 사용 가능한 policy gradient에는 두 가지 큰 문제점이 있다. 첫번째 문제는 variance이다. 아래 그래프를 policy sample에 대한 reward를 나타낸 것이라고 하고 초록색, 노란색의 경우에 대해서
살펴보자. 초록색과 같이 reward가 sample에 대해 나타날 경우에 우측은 양수, 좌측은 음수이므로 음수를 완전 기피하는 방향으로, 우측에 완전 치우쳐져서 variance가 낮게 학습이 될 것이다.
노란색의 경우를 보면 둘 다 양수이고 차이가 나긴 하지만 기피해야 할 음수는 아니므로 적절하게 분배되서 학습이 되고 초록색보다는 넓게 variance가 높게 학습이 된다.
variance가 높을 경우에 우리가 원하는 방향으로 완전히 학습되기 어려우므로 variance를 낮춰줘야 하고, 위 예제를 통해서 reward에 대해 상수를 더해줌으로써 가능하다는 것을 알았다.

두번째 문제는 학습 속도이다. 위 toy model에서 보면 우리가 학습시켜야 할 parameter는 $$\theta = (k, \sigma)$$이다. variance를 높혀주기 위해서 $$\sigma$$를
낮추었다고 가정하면, k를 구하기 위해서 gradient를 구하면 k에 대한 gradient가 작아지는 것을 볼 수 있다.(*아래 그래프*) 그럼 결국 학습 속도가 느려져서 수렴이 안 되는 경우가 생기게 된다.
이런 문제를 해결하기 위해서 natural gradient라는 방법을 도입하는데 이는 뒷 장에서 본다고 한다. <br><br>
![policy gradient의 문제점](https://whikwon.github.io/images/CS294_problem_of_policy_gradient.png)
<center> <i> &lt;policy gradient의 문제점 &gt;</i> </center> <br>

## Policy gradient 문제 해결 : Casuality
앞서 policy gradient의 variance 문제를 소개했고 이를 해결하기 위한 방안을 두 가지 소개한다.
첫째는 ***Casuality*** 로, 현재 policy는 과거 reward에 영향을 줄 수 없다는 점을 활용한다. 이로 인해 변경되는 수식을 나타내면 아래와 같고 $$\hat Q_{i, t}$$를 ***reward to go***
라고 일컫는다. variance가 작아지는 이유는 결국 $$\nabla_{\theta} J(\theta)$$ 값에 의해서 학습이 진행되는데 reward가 음수일 때 학습이 안 되는 것과 마찬가지로,
실제 영향을 받지 않는 reward가 더해져있는 경우면 일정 부분 넓게 퍼진 상태를 가정하는 것과 같다고 볼 수 있다. 그래서 이 값을 빼주었을 때 실제 영향을 미치는 값들만 고려하게 되므로
variance를 낮출 수 있다.<br>
$$\begin{align}
\nabla_{\theta} J(\theta) &\approx \frac 1 N \displaystyle \sum_{i=1}^N \bigg (\displaystyle \sum_{t=1}^T \nabla_{\theta} log \pi_{\theta} (a_{i, t}|s_{i, t}) \bigg) \bigg(\displaystyle \sum_{t=1}^T r(s_{i,t}, a_{i,t}) \bigg) \\
&= \frac 1 N \displaystyle \sum_{i=1}^N \bigg (\displaystyle \sum_{t=1}^T \nabla_{\theta} log \pi_{\theta} (a_{i, t}|s_{i, t}) \bigg) \bigg(\displaystyle \sum_{t'=t}^T r(s_{i,t'}, a_{i,t'}) \bigg) \\
\hat Q_{i,t} &= \sum_{t'=t}^T r(s_{i,t'}, a_{i,t'})
\end{align}$$

## Policy gradient 문제 해결 : Baseline
두번째인 ***baseline*** 을 소개한다. 앞서 문제점을 봤을 때 언급되었던 내용으로 reward에 상수를 더함으로써 gradient의 variance를 낮출 수 있다는 점을 활용한다.
상수 $$b$$를 reward항에서 빼주는데 이 값은 전체 reward의 평균 값을 빼보자. 직관적으로 봤을 때 앞에서 보았던 것처럼 좋은 sample의 확률을 높혀주고 나쁜 sample의 확률을 낮춰줄 것이다.
이를 수학적으로도 사용해도 되는지에 대한 내용을 아래에서 추가 설명한다. 결론적으로 baseline을 빼주더라도 결국 expectation을 취하게 되므로 *unbiased* 해서 문제가 없다. <br><br>
![baseline](https://whikwon.github.io/images/CS294_baseline.png)
<center> <i> &lt;baseline 소개&gt;</i> </center> <br>

baseline을 사용해도 된다는 건 위에서 보였고, 그럼 어떤 baseline이 가장 좋을지 살펴보자.  $$\frac {dVar} {db} = 0$$일 때의 $$b$$ 찾으면 되겠다.
수식을 전개하면 아래 마지막 식처럼 최적의 $$b$$를 찾을 수 있다. (*아래 식에서 $$g(\tau) = \nabla_{\theta} log \pi_{\theta}(\tau)$$ 이다.*) <br><br>
![variance analysis](https://whikwon.github.io/images/CS294_analyzing_variance.png)
<center> <i> &lt;최적의 baseline 구하기&gt;</i> </center> <br>

## On policy to Off policy
policy gradient는 기본적으로 on-policy 알고리즘이다. 즉, policy가 변할 때마다 sample을 뽑아줘야 한다는 단점이 있다.
그래서, 이러한 점을 어떻게 하면 off-policy 문제로 접근할 수 있을 지 생각해본다. ***importance sampling*** 을 활용하는데 이는 아래에 나와있듯
$$p(x)$$에 대한 $$f(x)$$의 expectation을 기존의 $$q(x)$$에 대한 expectation으로 바꿔서 푸는 방법이다. <br><br>
![variance analysis](https://whikwon.github.io/images/CS294_off_policy.png)
<center> <i> &lt;IS를 활용한 on-policy를 off-policy로 변환&gt;</i> </center> <br>
우리의 문제에 적용하면 우리는 현재 policy($$\pi_{\theta}(\tau)$$)에 대한 reward의 expectation을 구하는 것이 목표이다. 그러기 위해서는 sample을 또 생성해야 하므로
위에서 정의한 importance sampling을 활용하면 과거 policy($$\bar {\pi}(\tau)$$ 를 알고 있으므로 이에 대한 expectation을 구하는 문제로 바꿔준다.
**대신 문제를 풀기 위해서는 새롭게 $$\frac {\pi_{\theta}(\tau)} {\bar {\pi}(\tau)}$$를 알아야한다.**
위 식에서 중간에 causality를 고려하고 계속해서 전개하면 아래와 같다. <br><br>
![variance analysis2](https://whikwon.github.io/images/CS294_off_policy2.png)
![variance analysis3](https://whikwon.github.io/images/CS294_off_policy3.png)
위에서 최종적으로 구한 식에서 $$\displaystyle \prod_{t'=1}^t \frac {\pi_{\theta'}(a_{t'}|s_{t'})} {\pi_{\theta}(a_{t'}|s_{t'})}$$에 해당되는 값이
$$T$$ 대해 exponential하기 때문에 문제가 생긴다. 그래서 이 부분을 아래 식처럼 전개해서 해결해준다. 전개의 상세한 내용은 뒷 장에서 다시 볼 예정이라고 하니 일단
이런 방식을 통해서 ***On-policy를 Off-policy가 가능하게 해준다.*** 라는 점만 익히고 가겠다. <br><br>
![variance analysis4](https://whikwon.github.io/images/CS294_off_policy4.png)
<center> <i> &lt;IS를 활용한 on-policy를 off-policy로 변환(수식 전개)&gt;</i> </center> <br>

## Policy gradient with automatic differentiation
$$\nabla_{\theta} J(\theta) \approx \frac 1 N \displaystyle \sum_{i=1}^N \bigg (\displaystyle \sum_{t=1}^T \nabla_{\theta} log \pi_{\theta} (a_{i, t}|s_{i, t}) \bigg) \hat Q_{i,t}$$ 식을 explicit하게 구하는 건 너무 비효율적이니 graph 모델로 나타내서 automatic differentiation 할 수 있도록 해준다. 방법은 그냥 식으로 나타내준 뒤
tensorflow 돌리면 알아서 계산해준다.(*각 식에 대한 Back propagation으로 미분 값을 구하는 과정이 진행된다.*)

## Policy gradient in practice
실제 학습시킬 때는 gradient의 variance가 매우 커서 noisy 하다는 점을 염두해두자. 그러므로 batch 크기를 늘려서 진행하는 것도 한가지 방법이고 optimizer나 learning rate
지정하는 것도(*automatic step size adjustment*) 쉽지 않아서 어떤 방법을 취할지 뒤에서 다시 보도록 하자.

Reference:
[Berkeley Univ. CS294 Lecture slides](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_4_policy_gradient.pdf)
