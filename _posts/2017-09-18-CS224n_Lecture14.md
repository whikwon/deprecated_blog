---
title:  "CS224n_Lecture14"
date: 2017-09-18 00:00:00
comments: true
---

- CS224n Lecture14에 대한 내용을 정리하였다.

14강에서 소개하는 Recursive Neural Network는 13강에서의 CNN과 마찬가지로 NP, VP의 문법이나 의미를
기존 모델들이 잘 다루지 못하는 점을 지적한다.

뒷 장으로 갈수록 수업 내용은 엄청 간략하게 설명하고 논문과 강의노트로 다 때우고 있다. 이번 장도 논문의 전체적인 내용을
살펴보면서 정리해보도록 하자.

### 논문 1) Parsing with Compositional Vector Grammars

논문의 목적은 parsing에 PCFG와 syntactically untied recursive neural network를 결합한
Compositional Vector Grammar(CVG)를 사용해서 성능을 끌어올리는 것이다. <br>
생소한 단어 2개가 나오는데 CVG는 구(phrase)를 벡터로 나타내기 위한 문법, 방법 정도로 해석하면 될 듯하고
CFG는 문법적인 내용인데 봐도 모르겠어서 PCFG가 문법적인 내용을 나타낸다는 정도만 기억해두고 더 살펴보자.

CVG은 아래 그림의 예제를 참조하면 이해하기 쉽다. 먼저, PCFG가 고려된 RNN을 이용해서 tree구조로 연결한다. 연결되는
각각의 node마다 만나는 벡터들을 아래에서 위로 연산하면 최종적으로 하나의 구에 대한 벡터를 구할 수 있게 된다. <br>
![example](https://whikwon.github.io/images/NLP_CVG_example.png) <br>

PCFG와 RNN이 함께 사용되었을 때 얻는 이익은 다음과 같다고 한다. PCFG는 NP, PP 등 구들의 분류를 잘하게 도와주고
RNN은 단어나 구의 세세한 의미나 syntactic한 내용을 더 잘 파악할 수 있도록 해준다고 한다. 이러한 목적으로
두가지 다른 모델을 합쳤다고 한다. PCFG, RNN에 대한 history가 잘 나와있으니 궁금하다면 참고해도 좋을듯하다.

CVG와 기존에 사용되던 RNN과의 가장 큰 차이점은 **syntactically untied weights** 를 사용하는 것이다.
weight는 하위 품사에 의존되도록 하여 다양한 composition function이 구를 합칠 수 있도록 해주었고
parsing accuracy가 상승하였다.

CVG의 objective function을 살펴보자. supervised parsing의 목표는 $$g : \mathcal{X} \rightarrow \mathcal{Y}$$으로,
문장($$\mathcal{X}$$)이 적절한 parse tree($$\mathcal{Y}$$)로 가게끔 하는 것이다. 그럼 tree를 구상했을 때의 loss를
정의해보자. $$y_i$$가 correct tree이고 $$\hat y$$가 예측한 tree일 때 structured margin loss는
$$\Delta (y_i, \hat y) = \sum_{d \in N(\hat y)} \kappa {\bf 1} \{d \notin N(y_i)\}$$로 나타낼 수 있다. <br>
(*수식에서 $$N(y)$$는 tree가 틀린 node 개수이고 $$\kappa$$는 실험적으로 0.1로 둔다.*) <br>
그리고 학습 데이터 ($$x_i, y_i$$)에 대해서 loss를 최소화하는 $$g_{\theta}$$라는 함수를 찾는다고 하면 다음과 같이 정의될 것이다.
$$g_{\theta}(x) = \underset {\hat y \in Y(x)} {arg max}\ s(CVG(\theta, x, \hat y))$$
이 때, $$s$$라는 함수에 의해서 score가 매겨지며 $$g_{\theta}(x_i) = y_i$$일 때 최대 score를 갖게 되며
다른 tree와의 차이만큼 margin을 갖는다. ($$s(CVG(\theta, x_i, y_i)) \geq s(CVG(\theta, x_i, \hat y)) + \Delta(y_i, \hat y)$$)

그래서 최종적으로 objective fuction은 아래 식이다. <br>
$$J(\theta) = {\frac 1 2} \overset m {\underset {i=1} \sum} r_i(\theta) + {\frac \lambda 2} \Vert \theta \Vert_2^2 where
r_i(\theta) = \underset {\hat y \in Y(x_i)} {max} \big ( s(CVG(x_i, \hat y)) + \Delta (y_i, \hat y) \big ) - s(CVG(x_i, y_i)) \tag{1}$$

objective function에서 **score** 를 정의하고 구하는게 빠졌다. CVG가 구의 벡터를 어떤 방식으로 구하는 지 보면서 함께
살펴보자. <br>

아래 그림의 구조를 통해서 Standard RNN에서 무엇이 추가되었는지 보도록 하겠다. <br>
![example2](https://whikwon.github.io/images/NLP_CVG_example2.png) <br>
*(왼쪽은 Standard RNN, 오른쪽은 SU-RNN 이다. 모두 논문 출처.)* <br>
standard RNN에서 tree는 bottom-up beam search로 찾아지게 된다. 그리고 각각의 단어는 (vector, POS) 짝으로 나타낼 수 있다.
tree가 만들어지면서 새로운 parent node들이 생기게 된다. 그림에서 $$\big ( (p^1 \rightarrow bc), (p^2 \rightarrow ap^1) \big )$$로
나타낼수 있으며 parent의 차원은 children과 같다. <br>
이제 children의 벡터 정보를 받아서 parent를 나타내보자. children의 벡터를 concatenate하고 $$W$$라는 parameter를 곱해주면 된다.
이럴 경우 $$p^{(1)} = f \big ( W  + \begin{bmatrix} b \\ c \\ \end{bmatrix} \big ),\ p^{(2)} = f \big ( W  + \begin{bmatrix} a \\ p^1 \\ \end{bmatrix} \big )$$로
나타낼 수 있다. standard RNN에서는 $$W$$를 모두 같은 값으로 사용한다. <br>
score도 드디어 나오는데 $$s(p^{(i)}) = v^{T} p^{(i)}$$로 나타내며 $$v \in \mathbb{R}^n$$은 학습시킬 값이다.
standard RNN은 잠깐 생각해도 single composition function으로 모든 품사들을 다뤄야 하므로 복잡해질 경우 표현력의 한계가 드러날 것이라고
예상해볼 수 있겠다.

위와 같은 문제점을 해결하기 위해서 network를 조금더 deep하게 만들어줄 수도 있겠다. 하지만 그럴 경우 vanishing, exploding gradient 문제가
발생해서 또 다른 어려움이 있다고 한다. 그래서 제시한 구조가 **CVG** 구조로 각각의 node의 composition function를 PCFG로부터
가져와서 사용한다. children의 syntactic category가 composition function을 결정하고 parent의 vector를 계산해서 넘겨주는 방식이다.
그러므로 standard RNN에서 $$W$$가 모두 같았던 반면에 논문에서 소개하는 **SU-RNN** 구조에서는 $$W$$가 children에 따라 다름을 위 그림에서
볼 수 있다. parent vector를 식으로 나타내면 $$p^{(1)} = f \big ( W^{(B,C)} \begin{bmatrix} b \\ c \\ \end{bmatrix} \big )$$와 같고
score는 $$s(p^{(1)}) = (v^{(B,C)})^T p^{(1)} + log P (P_1 \rightarrow B\ C)$$ 로 나타내지며 $$P(P_1 \rightarrow B\ C)$$는 PCFG로부터 결정된다.
$$p^{(2)}$$의 경우도 마찬가지로 진행된다.

CVG의 parsing은 bottom-up beam search로 진행이 된다. 학습 방법에 대해서는 2가지 stage로 나눠서 한다. 먼저, base PCFG를 학습시키고
top tree가 저장이 된다. 그 다음에 PCFG에 조건부로, 위에서 본 (1)식을 objective function으로하여 SU-RNN을 학습시킨다.




Reference: <br>
[Stanford CS224n lecture14 slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture14.pdf) <br>
Richard Socher, John Bauer, Christopher D. Manning, Andrew Y Ng, 2013 [Parsing with Compositional Vector Grammars](https://nlp.stanford.edu/pubs/SocherBauerManningNg_ACL2013.pdf)
