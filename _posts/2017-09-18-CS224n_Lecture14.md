---
title:  "CS224n_Lecture14: Tree Recursive Neural Networks and Constituency Parsing"
date: 2017-09-20 00:00:00
layout: post
excerpt: "Stanford 대학의 2017년도 겨울학기 강의인 CS224n: Natural Language Processing with Deep Learning의 14강을 정리한 내용이다."
categories: [NLP, Lecture]
comments: true
---

14강에서는 13강과 마찬가지로 기존 모델들의 NP, VP의 문법이나 의미를 잘 다루지 못하는 점을 지적하며 이에 대한
해결 방안으로 recursive neural network를 소개한다.

**Recursive Neural Network** 의 목적은 문장 내 구조를 이루는 parse tree를 만들고 문장의 의미를 나타내는 compositional vector representation
를 얻는 것이다. 아래 그림에 잘 나타나있다. 각각의 node(phrase) 대한 의미를 vector로 나타내면서 parsing할 수 있다. <br>
![goal](https://whikwon.github.io/images/NLP_CVG_goal.png) <br>
<center> <i> &lt;Recursive Neural Network 구조&gt; </i> </center> <br>
기존 모델들인 recurrent, convolution과의 차이점을 살펴보자. <br>
아래 그림처럼 recurrent는 한 방향으로 정보를 전달하기 때문에 구(phrase) 단위로 의미를 capture하는게 쉽지 않고 마지막 단어의 정보가 많이 전달되는 경향이 있다.
반면에 recursive는 구 단위로 의미를 capture할 수 있지만 parser가 필요하다는 큰 단점이 있다. <br>
![recursive vs reccurent](https://whikwon.github.io/images/NLP_CVG_rnns.png) <br>
<center> <i> &lt;Recursive, Recurrent Neural Network의 비교&gt; </i> </center> <br>
convolution과는 구 단위로 의미를 capture할 수 있다는 공통점이 있지만 convolution이 문법적으로 말이 되지 않아도 상관없이 다루지만
recursive는 parser를 이용해서 문법적으로 맞는 구만 다룬다는 차이점이 있다. <br>
![recursive vs convolution](https://whikwon.github.io/images/NLP_CVG_cnn_vs_rnn.png) <br>
<center> <i> &lt;Convolution, Recurrent Neural Network의 비교&gt; </i> </center> <br>
그럼 RNN이 어떻게 작동해서 원하는 결과를 얻는지 살펴보자. 왼쪽 그림과 같이 문장 내 단어들이 $$c_1, c_2$$와 같이
벡터로 표현되고 network를 통과하면서 score와 parent의 벡터 연산이 수행된다. score와 tree 구조로 objective function을
만들고 학습이 진행되면서 적절한 $$U^T, W$$를 학습한다고 보면 되겠다. 학습 방법은 backpropagation를 사용하며 standard RNN에서
$$W$$는 모두 같은 값으로 적용된다.
![recursive](https://whikwon.github.io/images/NLP_recursive.png) <br>
<center> <i> &lt;Recursive Neural Network의 구성 요소&gt;</i> </center> <br>
standard RNN에서 $$W$$가 하나의 matrix로 사용되므로 composition function도 하나이다. 그로 인해서
복잡한 composition, 긴 문장들을 해석하기가 힘들어져서 이에 대한 해결 방안으로 SU-RNN 모델(*첫번째 논문*)이 나오게 된다.

그럼 어떤 기준으로 composition matrix를 선정할까? CFG를 기본으로 삼아서 syntactic environment에 따라 나눠주는데 CFG는
속도가 느려서 tree의 subset만 활용하는 PCFG를 사용한다고 한다. 여>기에 기존에 사용되는 TreeRNN을 사용하면 SU-RNN 구조가 완성된다.

SU-RNN의 학습 결과를 시각화해놓은 자료를 보자. 아래 그림은 $$W$$와 children 단어 2개를 곱한 값을 단어에 따라 좌/우로 나눠놓은 것이다.  
그리고 identity initialization한 뒤에 어떤 부분이 더 학습을 잘하는지 보면 NP부분에 집중이 되는 것을 알 수 있다. 이런 식으로
확인했을 때 SU-RNN이 품사에 따라서 어떤 정보들을 더 중요하게 생각하는지 알 수 있다.
![SU-RNN_ex1](https://whikwon.github.io/images/NLP_su_rnn_example1.png) <br>
<center> <i> &lt;SU-RNN 학습 결과 1 : 구의 품사 별 학습 차이&gt;</i> </center> <br>
아래는 문장을 최종적으로 vector representation한 결과가 어떤 분포를 나타내고 있는지 확인한 결과이다.
확인 문장과 주변에 가장 가까운 2개 문장을 비교했을 때 유사한 의미의 내용을 나타내는 것을 볼 수 있다. <br><br>
![SU-RNN_ex2](https://whikwon.github.io/images/NLP_su_rnn_example2.png) <br>
<center> <i> &lt;SU-RNN 학습 결과 2 : vector로 나타냈을 때 가까운 문장 확인 결과&gt;</i> </center> <br>

이렇게 SU-RNN이 standard RNN의 문제를 해결하기 위해서 composition matrix를 다양화하였지만 여전히 단점이 존재한다.
SU-RNN의 단점은 parent node를 형성하기 위해서 단어들 간의 연산을 할 때 담당하는 matrix의 부분이 정해져 있어서 단어 간 상호작용이 없다는 점이다.

이를 해결하기 위해서 MV-RNN이 나오게 된다. 핵심은 단어 간에 섞이지 않던 부분을 섞어주자는 것이다.
상세한 알고리즘은 논문을 참고하자. (*아래 vector와 matrix를 둘 다 구해서 어떻게 쓰는지만 알면 될 듯하다.*)
![MV-RNN](https://whikwon.github.io/images/NLP_mv_rnn.png) <br>
<center> <i>&lt;MV-RNN의 단어 간 연산 방법&gt;</i> </center> <br>
이렇게 했을 때의 결과는 아래와 같은데 가장 중요한 점은 기존 RNN이 ***not~*** 구문의 해석이 어려웠는데 이를 잘 해냈다는 점이다.
![MV-RNN_result](https://whikwon.github.io/images/NLP_mv_rnn_result.png) <br>
<center> <i>&lt;MV-RNN 학습 결과 : 구문 별 Sentiment distribution 예측&gt;</i> </center> <br>
MV-RNN을 semantic relationship 분류에 사용했을 때 좋은 성능을 얻었는데 다른 NLP에서의 모델들과 마찬가지로
Feature를 거의 갖지 않고 기존 모델보다 좋은 성능을 내서 역시 neural network가 최고인 듯하다.

## 논문 1) Parsing with Compositional Vector Grammars

논문의 목적은 parsing에 PCFG와 syntactically untied recursive neural network를 결합한
Compositional Vector Grammar(CVG)를 사용해서 성능을 끌어올리는 것이다. <br>
생소한 단어 2개가 나오는데 CVG는 구(phrase)를 벡터로 나타내기 위한 방법 정도로 해석하면 될 듯하고
CFG는 문법적인 내용인데 봐도 모르겠어서 PCFG가 문법적인 내용을 나타내기 위한 방법이라는 정도만 기억해두고 더 살펴보자.
(*강의에서는 PCFG를 속도 때문에 사용한다고 한다. tree의 subset만 활용하여 빠르게 score를 계산할 수 있다고 한다.*)

CVG은 아래 그림의 예제를 참조하면 이해하기 쉽다. 먼저, PCFG가 고려된 RNN을 이용해서 tree구조로 연결한다. 연결되는
각각의 node마다 만나는 벡터들을 아래에서 위로 연산하면 최종적으로 하나의 구에 대한 벡터를 구할 수 있게 된다. <br>
![example](https://whikwon.github.io/images/NLP_CVG_example.png) <br>

PCFG와 RNN이 함께 사용되었을 때 얻는 이익은 다음과 같다고 한다. PCFG는 NP, PP 등 구들의 분류를 잘하게 도와주고
RNN은 단어나 구의 세세한 의미나 syntactic한 내용을 더 잘 파악할 수 있도록 해준다고 한다. 이러한 목적으로
두가지 다른 모델을 합쳤다고 한다. PCFG, RNN에 대한 history가 잘 나와있으니 궁금하다면 참고해도 좋을듯하다.

CVG와 기존에 사용되던 RNN과의 가장 큰 차이점은 **syntactically untied weights** 를 사용하는 것이다.
weight는 하위 품사에 의존되도록 하여 다양한 composition function이 구를 합칠 수 있도록 해주었고
parsing accuracy가 상승하였다.

CVG의 objective function을 살펴보자. supervised parsing의 목표는 $$g : \mathcal{X} \rightarrow \mathcal{Y}$$으로,
문장($$\mathcal{X}$$)이 적절한 parse tree($$\mathcal{Y}$$)로 가게끔 하는 것이다. 그럼 tree를 구상했을 때의 loss를
정의해보자. $$y_i$$가 correct tree이고 $$\hat y$$가 예측한 tree일 때 structured margin loss는
$$\Delta (y_i, \hat y) = \sum_{d \in N(\hat y)} \kappa {\bf 1} \{d \notin N(y_i)\}$$로 나타낼 수 있다. <br>
(*수식에서 $$N(y)$$는 tree가 틀린 node 개수이고 $$\kappa$$는 실험적으로 0.1로 둔다.*) <br>
그리고 학습 데이터 ($$x_i, y_i$$)에 대해서 loss를 최소화하는 $$g_{\theta}$$라는 함수를 찾는다고 하면 다음과 같이 정의될 것이다.
$$g_{\theta}(x) = \underset {\hat y \in Y(x)} {arg max}\ s(CVG(\theta, x, \hat y))$$
이 때, $$s$$라는 함수에 의해서 score가 매겨지며 $$g_{\theta}(x_i) = y_i$$일 때 최대 score를 갖게 되며
target tree와의 차이만큼 margin을 갖는다. 식으로 나타내면 $$s(CVG(\theta, x_i, y_i)) \geq s(CVG(\theta, x_i, \hat y)) + \Delta(y_i, \hat y)$$

그래서 최종적으로 구한 objective fuction은 아래 식으로 나타내진다. <br> <br>
$$J(\theta) = {\frac 1 2} \overset m {\underset {i=1} \sum} r_i(\theta) + {\frac \lambda 2} \Vert \theta \Vert_2^2 \tag{1}$$ <br>
$$r_i(\theta) = \underset {\hat y \in Y(x_i)} {max} \big ( s(CVG(x_i, \hat y)) + \Delta (y_i, \hat y) \big ) - s(CVG(x_i, y_i)) \tag{2}$$

objective function에서 **score** 를 정의하고 구하는게 빠졌다. CVG가 구의 벡터를 어떤 방식으로 구하는 지 보면서 함께
살펴보자. <br>

아래 그림의 구조를 통해서 Standard RNN에서 무엇이 추가되었는지 보도록 하겠다. <br>
![example2](https://whikwon.github.io/images/NLP_CVG_example2.png) <br>
<center> <i> (왼쪽은 Standard RNN, 오른쪽은 SU-RNN 이다. 모두 논문 출처.) </i> </center> <br>

standard RNN에서 tree는 bottom-up beam search로 찾아지게 된다. 그리고 각각의 단어는 (vector, POS) 짝으로 나타낼 수 있다.
tree가 만들어지면서 새로운 parent node들이 생기게 된다. 그림에서 $$\big ( (p^1 \rightarrow bc), (p^2 \rightarrow ap^1) \big )$$로
나타낼수 있으며 parent의 차원은 children과 같다. <br>
이제 children의 벡터 정보를 받아서 parent를 나타내보자. children의 벡터를 concatenate하고 $$W$$라는 parameter를 곱해주면 된다.
이럴 경우 $$p^{(1)} = f \big ( W  + \begin{bmatrix} b \\ c \\ \end{bmatrix} \big ),\ p^{(2)} = f \big ( W  + \begin{bmatrix} a \\ p^1 \\ \end{bmatrix} \big )$$로
나타낼 수 있다. standard RNN에서는 $$W$$를 모두 같은 값으로 사용한다. <br>
score도 드디어 나오는데 $$s(p^{(i)}) = v^{T} p^{(i)}$$로 나타내며 $$v \in \mathbb{R}^n$$은 학습시킬 값이다.
standard RNN은 잠깐 생각해도 single composition function으로 모든 품사들을 다뤄야 하므로 복잡해질 경우 표현력의 한계가 드러날 것이라고
예상해볼 수 있겠다.

위와 같은 문제점을 해결하기 위해서 network를 조금더 deep하게 만들어줄 수도 있겠다. 하지만 그럴 경우 vanishing, exploding gradient 문제가
발생해서 또 다른 어려움이 있다고 한다. 그래서 제시한 구조가 **CVG** 구조로 각각의 node의 composition function를 PCFG로부터
가져와서 사용한다. children의 syntactic category가 composition function을 결정하고 parent의 vector를 계산해서 넘겨주는 방식이다.
그러므로 standard RNN에서 $$W$$가 모두 같았던 반면에 논문에서 소개하는 **SU-RNN** 구조에서는 $$W$$가 children에 따라 다름을 위 그림에서
볼 수 있다. <br>
parent vector를 식으로 나타내면 $$p^{(1)} = f \big ( W^{(B,C)} \begin{bmatrix} b \\ c \\ \end{bmatrix} \big )$$와 같고
score는 $$s(p^{(1)}) = (v^{(B,C)})^T p^{(1)} + log P (P_1 \rightarrow B\ C)$$ 로 나타내지며 $$P(P_1 \rightarrow B\ C)$$는 PCFG로부터 결정된다.
$$p^{(2)}$$의 경우도 마찬가지로 진행된다.

CVG의 parsing은 bottom-up beam search로 진행이 된다. 학습 방법에 대해서는 2가지 stage로 나눠서 한다. 먼저, base PCFG를 학습시키고
top tree가 저장이 된다. 그 다음에 PCFG에 조건부로, 위에서 본 (1), (2) 식을 objective function으로하여 SU-RNN을 학습시킨다.

paramter를 업데이트하기 위해서 다른 모델들과 동일하게 backpropagation을 사용하며 한가지 다른 점은 hinge loss로 인해 objective function이
미분 불가능하기 때문에 [**subgradient method**](https://en.wikipedia.org/wiki/Subgradient_method) 를 사용해준다. subgradient method는
간략하게 미분 불가능한 함수에 대한 convex minimization를 풀 수 있는 방법이다. steepest descent(gradient descent)와 마찬가지로 iterative method이다.
자세한 유도 과정은 생략하고 AdaGrad를 사용했을 때 가장 좋은 성능을 얻을 수 있었고 이 때 update 식은
$$\theta_{t,i} = \theta_{t-1, i} - \frac \alpha {\sqrt{\sum_{\tau = 1}^t g_{\tau, i}^2}}g_{t,i}$$ 이다.


Reference: <br>
[Stanford CS224n lecture14 slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture14.pdf) <br>
Richard Socher, John Bauer, Christopher D. Manning, Andrew Y Ng, 2013 [Parsing with Compositional Vector Grammars](https://nlp.stanford.edu/pubs/SocherBauerManningNg_ACL2013.pdf) <br>
Richard Socher Brody Huval Christopher D. Manning Andrew Y. Ng, 2012[Semantic Compositionality through Recursive Matrix-Vector Spaces](http://www.aclweb.org/anthology/D12-1110) <br>
